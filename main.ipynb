{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"95R4RqiOuA7A","colab_type":"code","outputId":"75790772-e1b3-4305-e36b-eefe35f23dae","executionInfo":{"status":"ok","timestamp":1588658579289,"user_tz":-540,"elapsed":1024,"user":{"displayName":"염기웅","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX7VJGycIiSE_ScyzrUP21BjN23qYkmpaLsooQow=s64","userId":"17474065399754057575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IPruP1vbulVa","colab_type":"text"},"source":["# 필요한 필수 새팅 작업"]},{"cell_type":"code","metadata":{"id":"1OwOX8fsYS5D","colab_type":"code","outputId":"267fb78e-b1b5-46b3-826e-31ecec7ea485","executionInfo":{"status":"ok","timestamp":1588658586045,"user_tz":-540,"elapsed":5781,"user":{"displayName":"염기웅","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX7VJGycIiSE_ScyzrUP21BjN23qYkmpaLsooQow=s64","userId":"17474065399754057575"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["!ls drive/'My Drive'/'KoGPT2-FineTuning'/"],"execution_count":9,"outputs":[{"output_type":"stream","text":["checkpoint\t img\t\t       LICENSE\t    __pycache__       samples\n","dataset\t\t jupyter_generator.py  local.ipynb  README.md\t      samples1\n","Generator.ipynb  jupyter_main.py       main.ipynb   requirements.txt  util\n","generator.py\t kogpt2\t\t       main.py\t    runs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rHALfG-nWlRV","colab_type":"code","outputId":"eeadb6f2-350b-4ebb-a369-78ae2947e9c1","executionInfo":{"status":"ok","timestamp":1588658588127,"user_tz":-540,"elapsed":7851,"user":{"displayName":"염기웅","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX7VJGycIiSE_ScyzrUP21BjN23qYkmpaLsooQow=s64","userId":"17474065399754057575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!ls"],"execution_count":10,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-m6L6j_nYTTl","colab_type":"code","outputId":"a27fa533-ba43-4a2b-a97d-ad46e6cf74f1","executionInfo":{"status":"ok","timestamp":1588658591777,"user_tz":-540,"elapsed":11490,"user":{"displayName":"염기웅","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX7VJGycIiSE_ScyzrUP21BjN23qYkmpaLsooQow=s64","userId":"17474065399754057575"}},"colab":{"base_uri":"https://localhost:8080/","height":649}},"source":["!pip install -r drive/'My Drive'/'KoGPT2-FineTuning'/requirements.txt"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gluonnlp>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (0.9.1)\n","Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (1.6.0)\n","Requirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 3)) (0.1.86)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 4)) (1.5.0+cu101)\n","Requirement already satisfied: transformers>=2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (2.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 6)) (4.38.0)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 7)) (2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (20.3)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (0.29.17)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (1.18.3)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (2.23.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (0.8.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 4)) (0.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (3.0.12)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (1.12.47)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.5.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.7)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.0.43)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 7)) (1.12.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 7)) (3.10.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (3.0.4)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (1.15.47)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.9.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (7.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 7)) (46.1.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d8M3DCwcYlMv","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","sys.path.append('drive/My Drive/KoGPT2-FineTuning')\n","logs_base_dir = \"runs\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-qz4OLnYlSY","colab_type":"code","colab":{}},"source":["from jupyter_main import main"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_Jjj58pd1Rq","colab_type":"code","colab":{}},"source":["ctx= 'cuda'\n","cachedir='~/kogpt2/'\n","load_path = './gdrive/My Drive/KoGPT2-FineTuning/checkpoint/KoGPT2_checkpoint_296000.tar'\n","save_path = './gdrive/My Drive/KoGPT2-FineTuning/checkpoint/'\n","data_file_path = './gdrive/My Drive/KoGPT2-FineTuning/dataset/All_make_lyrics_dataset.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"umcGNCCYktXo","colab_type":"text"},"source":["# 모델 학습 시작"]},{"cell_type":"code","metadata":{"id":"is4QYvO_Q2Jl","colab_type":"code","outputId":"edb45cc6-d5ea-47fb-e37c-f14f1a597d0d","executionInfo":{"status":"ok","timestamp":1588658593857,"user_tz":-540,"elapsed":8459,"user":{"displayName":"염기웅","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX7VJGycIiSE_ScyzrUP21BjN23qYkmpaLsooQow=s64","userId":"17474065399754057575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 저장 잘 되는지 테스트\n","drive.mount('/content/gdrive')\n","\n","f = open(save_path+ 'KoGPT2_checkpoint_' + str(142) + '.tar', 'w')\n","f.write(\"가자\")\n","f.close()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9srHbQR92gGW","colab_type":"code","outputId":"b07e7084-53cd-4dcf-db06-f6b2553ce33f","executionInfo":{"status":"ok","timestamp":1588658597342,"user_tz":-540,"elapsed":11639,"user":{"displayName":"염기웅","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX7VJGycIiSE_ScyzrUP21BjN23qYkmpaLsooQow=s64","userId":"17474065399754057575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!curl ipecho.net/plain"],"execution_count":16,"outputs":[{"output_type":"stream","text":["104.198.106.55"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VH_JXair0ja3","colab_type":"code","colab":{}},"source":["#%load_ext tensorboard\n","#%tensorboard --logdir='./gdrive/My Drive/KoGPT2-FineTuning/runs'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH2FvBjrhLx6","colab_type":"code","colab":{}},"source":["# LOG_DIR = './gdrive/My Drive/KoGPT2-FineTuning/runs/'\n","\n","# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","# !unzip ngrok-stable-linux-amd64.zip\n","\n","# import os\n","# if not os.path.exists(LOG_DIR):\n","#   os.makedirs(LOG_DIR)\n","  \n","# get_ipython().system_raw(\n","#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","#     .format('./gdrive/My Drive/KoGPT2-FineTuning/runs'))\n","\n","# get_ipython().system_raw('./ngrok http 6006 &')\n","\n","# !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84_yuOvKx9DN","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"_A6oaFXJZEim","colab_type":"code","outputId":"ea8fe5dd-3418-4df1-eccd-699d9a5d60d8","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["main(load_path = load_path, data_file_path = data_file_path, save_path = './gdrive/My Drive/KoGPT2-FineTuning/checkpoint/', summary_url = './gdrive/My Drive/KoGPT2-FineTuning/runs/2020-05-01-new/', text_size = 1000, new = 1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n","epoch no.0 train no.1550  loss = 2.06106 avg_loss = 1.48503\n","epoch no.0 train no.1560  loss = 1.16489 avg_loss = 1.47086\n","epoch no.0 train no.1560  loss = 1.16489 avg_loss = 1.47086\n","epoch no.0 train no.1570  loss = 2.08006 avg_loss = 1.48229\n","epoch no.0 train no.1570  loss = 2.08006 avg_loss = 1.48229\n","epoch no.0 train no.1580  loss = 0.97080 avg_loss = 1.48816\n","epoch no.0 train no.1580  loss = 0.97080 avg_loss = 1.48816\n","epoch no.0 train no.1590  loss = 1.75699 avg_loss = 1.49484\n","epoch no.0 train no.1590  loss = 1.75699 avg_loss = 1.49484\n","epoch no.0 train no.1600  loss = 1.82692 avg_loss = 1.49499\n","epoch no.0 train no.1600  loss = 1.82692 avg_loss = 1.49499\n","epoch no.0 train no.1610  loss = 1.41251 avg_loss = 1.48121\n","epoch no.0 train no.1610  loss = 1.41251 avg_loss = 1.48121\n","epoch no.0 train no.1620  loss = 1.16120 avg_loss = 1.49384\n","epoch no.0 train no.1620  loss = 1.16120 avg_loss = 1.49384\n","epoch no.0 train no.1630  loss = 1.74985 avg_loss = 1.51334\n","epoch no.0 train no.1630  loss = 1.74985 avg_loss = 1.51334\n","epoch no.0 train no.1640  loss = 1.53809 avg_loss = 1.50674\n","epoch no.0 train no.1640  loss = 1.53809 avg_loss = 1.50674\n","epoch no.0 train no.1650  loss = 1.47040 avg_loss = 1.50373\n","epoch no.0 train no.1650  loss = 1.47040 avg_loss = 1.50373\n","epoch no.0 train no.1660  loss = 1.44142 avg_loss = 1.48364\n","epoch no.0 train no.1660  loss = 1.44142 avg_loss = 1.48364\n","epoch no.0 train no.1670  loss = 1.20217 avg_loss = 1.48730\n","epoch no.0 train no.1670  loss = 1.20217 avg_loss = 1.48730\n","epoch no.0 train no.1680  loss = 1.55384 avg_loss = 1.49132\n","epoch no.0 train no.1680  loss = 1.55384 avg_loss = 1.49132\n","epoch no.0 train no.1690  loss = 1.03698 avg_loss = 1.49801\n","epoch no.0 train no.1690  loss = 1.03698 avg_loss = 1.49801\n","epoch no.0 train no.1700  loss = 1.85974 avg_loss = 1.51426\n","epoch no.0 train no.1700  loss = 1.85974 avg_loss = 1.51426\n","epoch no.0 train no.1710  loss = 1.19611 avg_loss = 1.51554\n","epoch no.0 train no.1710  loss = 1.19611 avg_loss = 1.51554\n","epoch no.0 train no.1720  loss = 1.35775 avg_loss = 1.50216\n","epoch no.0 train no.1720  loss = 1.35775 avg_loss = 1.50216\n","epoch no.0 train no.1730  loss = 1.39587 avg_loss = 1.50964\n","epoch no.0 train no.1730  loss = 1.39587 avg_loss = 1.50964\n","epoch no.0 train no.1740  loss = 1.97390 avg_loss = 1.52318\n","epoch no.0 train no.1740  loss = 1.97390 avg_loss = 1.52318\n","epoch no.0 train no.1750  loss = 1.93112 avg_loss = 1.51634\n","epoch no.0 train no.1750  loss = 1.93112 avg_loss = 1.51634\n","epoch no.0 train no.1760  loss = 1.60205 avg_loss = 1.51367\n","epoch no.0 train no.1760  loss = 1.60205 avg_loss = 1.51367\n","epoch no.0 train no.1770  loss = 1.49651 avg_loss = 1.51042\n","epoch no.0 train no.1770  loss = 1.49651 avg_loss = 1.51042\n","epoch no.0 train no.1780  loss = 1.52791 avg_loss = 1.50562\n","epoch no.0 train no.1780  loss = 1.52791 avg_loss = 1.50562\n","epoch no.0 train no.1790  loss = 1.74062 avg_loss = 1.50426\n","epoch no.0 train no.1790  loss = 1.74062 avg_loss = 1.50426\n","epoch no.0 train no.1800  loss = 1.47906 avg_loss = 1.49024\n","epoch no.0 train no.1800  loss = 1.47906 avg_loss = 1.49024\n","epoch no.0 train no.1810  loss = 1.25053 avg_loss = 1.50047\n","epoch no.0 train no.1810  loss = 1.25053 avg_loss = 1.50047\n","epoch no.0 train no.1820  loss = 2.24230 avg_loss = 1.52823\n","epoch no.0 train no.1820  loss = 2.24230 avg_loss = 1.52823\n","epoch no.0 train no.1830  loss = 1.37589 avg_loss = 1.52544\n","epoch no.0 train no.1830  loss = 1.37589 avg_loss = 1.52544\n","epoch no.0 train no.1840  loss = 1.57193 avg_loss = 1.51828\n","epoch no.0 train no.1840  loss = 1.57193 avg_loss = 1.51828\n","epoch no.0 train no.1850  loss = 1.02996 avg_loss = 1.50743\n","epoch no.0 train no.1850  loss = 1.02996 avg_loss = 1.50743\n","epoch no.0 train no.1860  loss = 1.47152 avg_loss = 1.51129\n","epoch no.0 train no.1860  loss = 1.47152 avg_loss = 1.51129\n","epoch no.0 train no.1870  loss = 1.28527 avg_loss = 1.50759\n","epoch no.0 train no.1870  loss = 1.28527 avg_loss = 1.50759\n","epoch no.0 train no.1880  loss = 1.22042 avg_loss = 1.49468\n","epoch no.0 train no.1880  loss = 1.22042 avg_loss = 1.49468\n","epoch no.0 train no.1890  loss = 1.08416 avg_loss = 1.47786\n","epoch no.0 train no.1890  loss = 1.08416 avg_loss = 1.47786\n","epoch no.0 train no.1900  loss = 1.23039 avg_loss = 1.47249\n","epoch no.0 train no.1900  loss = 1.23039 avg_loss = 1.47249\n","epoch no.0 train no.1910  loss = 1.35764 avg_loss = 1.46967\n","epoch no.0 train no.1910  loss = 1.35764 avg_loss = 1.46967\n","epoch no.0 train no.1920  loss = 1.06124 avg_loss = 1.45070\n","epoch no.0 train no.1920  loss = 1.06124 avg_loss = 1.45070\n","epoch no.0 train no.1930  loss = 2.27374 avg_loss = 1.45206\n","epoch no.0 train no.1930  loss = 2.27374 avg_loss = 1.45206\n","epoch no.0 train no.1940  loss = 1.32002 avg_loss = 1.42862\n","epoch no.0 train no.1940  loss = 1.32002 avg_loss = 1.42862\n","epoch no.0 train no.1950  loss = 1.05415 avg_loss = 1.41842\n","epoch no.0 train no.1950  loss = 1.05415 avg_loss = 1.41842\n","epoch no.0 train no.1960  loss = 1.64932 avg_loss = 1.42583\n","epoch no.0 train no.1960  loss = 1.64932 avg_loss = 1.42583\n","epoch no.0 train no.1970  loss = 1.44658 avg_loss = 1.42674\n","epoch no.0 train no.1970  loss = 1.44658 avg_loss = 1.42674\n","epoch no.0 train no.1980  loss = 1.68079 avg_loss = 1.42497\n","epoch no.0 train no.1980  loss = 1.68079 avg_loss = 1.42497\n","epoch no.0 train no.1990  loss = 1.14380 avg_loss = 1.44042\n","epoch no.0 train no.1990  loss = 1.14380 avg_loss = 1.44042\n","epoch no.0 train no.2000  loss = 1.78099 avg_loss = 1.44124\n","epoch no.0 train no.2000  loss = 1.78099 avg_loss = 1.44124\n","to_tokens: ['▁', '▁노', '▁너', '▁모두', '▁', '▁안고', '▁묻어', '둘', '▁', '게', '▁', '▁손을', '▁사랑', '으로', '▁', '를', '▁지', '▁간직', '할', '게', '▁', '▁lo', 'll', '▁g', '▁s', 'or', 'ry', ',', '▁I', 'ea', 'h', '▁', '’', 'm', '▁so', '▁s', 'or', 'ry', ',', 'ab', 'y', '▁', '▁b', '’', 'm', '▁so', '▁s', 'or', 'ry', '▁b', 'ab', 'e', '▁', '를', '▁난', '▁', '<unused0>', '▁말', ',', '▁솔직히', '▁no', '▁', '▁', 'I', '’', 'm', '▁so', '▁s', 'or', 'ry', ',', '▁']\n","사랑했던 기억들 전부 가슴에 묻어두고\n","\n","갈게\n","\n","내 모든 사랑도\n","\n","너를 위해 간직할게\n","\n","I’m so sorry, yeah  I’m so sorry  babe\n","\n",", I’m so sorry babe\n","\n","너에게 하고 픈 말 , no no\n","\n","no (I’m so sorry)\n","to_tokens: ['▁', '▁노', '▁너', '▁모두', '▁', '▁안고', '▁묻어', '둘', '▁', '게', '▁', '▁손을', '▁사랑', '으로', '▁', '를', '▁지', '▁간직', '할', '게', '▁', '▁lo', 'll', '▁g', '▁s', 'or', 'ry', ',', '▁I', 'ea', 'h', '▁', '’', 'm', '▁so', '▁s', 'or', 'ry', ',', 'ab', 'y', '▁', '▁b', '’', 'm', '▁so', '▁s', 'or', 'ry', '▁b', 'ab', 'e', '▁', '를', '▁난', '▁', '<unused0>', '▁말', ',', '▁솔직히', '▁no', '▁', '▁', 'I', '’', 'm', '▁so', '▁s', 'or', 'ry', ',', '▁']\n","사랑했던 기억들 전부 가슴에 묻어두고\n","\n","갈게\n","\n","내 모든 사랑도\n","\n","너를 위해 간직할게\n","\n","I’m so sorry, yeah  I’m so sorry  babe\n","\n",", I’m so sorry babe\n","\n","너에게 하고 픈 말 , no no\n","\n","no (I’m so sorry)\n","epoch no.0 train no.2010  loss = 1.51770 avg_loss = 1.44035\n","epoch no.0 train no.2010  loss = 1.51770 avg_loss = 1.44035\n","epoch no.0 train no.2020  loss = 1.14999 avg_loss = 1.44383\n","epoch no.0 train no.2020  loss = 1.14999 avg_loss = 1.44383\n","epoch no.0 train no.2030  loss = 1.15670 avg_loss = 1.41817\n","epoch no.0 train no.2030  loss = 1.15670 avg_loss = 1.41817\n","epoch no.0 train no.2040  loss = 1.48208 avg_loss = 1.42377\n","epoch no.0 train no.2040  loss = 1.48208 avg_loss = 1.42377\n","epoch no.0 train no.2050  loss = 1.50100 avg_loss = 1.42616\n","epoch no.0 train no.2050  loss = 1.50100 avg_loss = 1.42616\n","epoch no.0 train no.2060  loss = 1.20265 avg_loss = 1.41683\n","epoch no.0 train no.2060  loss = 1.20265 avg_loss = 1.41683\n","epoch no.0 train no.2070  loss = 2.07636 avg_loss = 1.43395\n","epoch no.0 train no.2070  loss = 2.07636 avg_loss = 1.43395\n","epoch no.0 train no.2080  loss = 1.69883 avg_loss = 1.43735\n","epoch no.0 train no.2080  loss = 1.69883 avg_loss = 1.43735\n","epoch no.0 train no.2090  loss = 1.45705 avg_loss = 1.43146\n","epoch no.0 train no.2090  loss = 1.45705 avg_loss = 1.43146\n","epoch no.0 train no.2100  loss = 1.50721 avg_loss = 1.42900\n","epoch no.0 train no.2100  loss = 1.50721 avg_loss = 1.42900\n","epoch no.0 train no.2110  loss = 1.13540 avg_loss = 1.42604\n","epoch no.0 train no.2110  loss = 1.13540 avg_loss = 1.42604\n","epoch no.0 train no.2120  loss = 1.40923 avg_loss = 1.43654\n","epoch no.0 train no.2120  loss = 1.40923 avg_loss = 1.43654\n","epoch no.0 train no.2130  loss = 1.61564 avg_loss = 1.43611\n","epoch no.0 train no.2130  loss = 1.61564 avg_loss = 1.43611\n","epoch no.0 train no.2140  loss = 1.35093 avg_loss = 1.43537\n","epoch no.0 train no.2140  loss = 1.35093 avg_loss = 1.43537\n","epoch no.0 train no.2150  loss = 1.01124 avg_loss = 1.43818\n","epoch no.0 train no.2150  loss = 1.01124 avg_loss = 1.43818\n","epoch no.0 train no.2160  loss = 1.46234 avg_loss = 1.42188\n","epoch no.0 train no.2160  loss = 1.46234 avg_loss = 1.42188\n","epoch no.0 train no.2170  loss = 1.42720 avg_loss = 1.43186\n","epoch no.0 train no.2170  loss = 1.42720 avg_loss = 1.43186\n","epoch no.0 train no.2180  loss = 1.27255 avg_loss = 1.43989\n","epoch no.0 train no.2180  loss = 1.27255 avg_loss = 1.43989\n","epoch no.0 train no.2190  loss = 1.12124 avg_loss = 1.43662\n","epoch no.0 train no.2190  loss = 1.12124 avg_loss = 1.43662\n","epoch no.0 train no.2200  loss = 1.63949 avg_loss = 1.44723\n","epoch no.0 train no.2200  loss = 1.63949 avg_loss = 1.44723\n","epoch no.0 train no.2210  loss = 1.29885 avg_loss = 1.45347\n","epoch no.0 train no.2210  loss = 1.29885 avg_loss = 1.45347\n","epoch no.0 train no.2220  loss = 1.26378 avg_loss = 1.44819\n","epoch no.0 train no.2220  loss = 1.26378 avg_loss = 1.44819\n","epoch no.0 train no.2230  loss = 1.62930 avg_loss = 1.45037\n","epoch no.0 train no.2230  loss = 1.62930 avg_loss = 1.45037\n","epoch no.0 train no.2240  loss = 1.46465 avg_loss = 1.44281\n","epoch no.0 train no.2240  loss = 1.46465 avg_loss = 1.44281\n","epoch no.0 train no.2250  loss = 1.16775 avg_loss = 1.44328\n","epoch no.0 train no.2250  loss = 1.16775 avg_loss = 1.44328\n","epoch no.0 train no.2260  loss = 1.48123 avg_loss = 1.44888\n","epoch no.0 train no.2260  loss = 1.48123 avg_loss = 1.44888\n","epoch no.0 train no.2270  loss = 0.86932 avg_loss = 1.43250\n","epoch no.0 train no.2270  loss = 0.86932 avg_loss = 1.43250\n","epoch no.0 train no.2280  loss = 1.33551 avg_loss = 1.43358\n","epoch no.0 train no.2280  loss = 1.33551 avg_loss = 1.43358\n","epoch no.0 train no.2290  loss = 1.39427 avg_loss = 1.43400\n","epoch no.0 train no.2290  loss = 1.39427 avg_loss = 1.43400\n","epoch no.0 train no.2300  loss = 1.04822 avg_loss = 1.44216\n","epoch no.0 train no.2300  loss = 1.04822 avg_loss = 1.44216\n","epoch no.0 train no.2310  loss = 1.42779 avg_loss = 1.43501\n","epoch no.0 train no.2310  loss = 1.42779 avg_loss = 1.43501\n","epoch no.0 train no.2320  loss = 1.04369 avg_loss = 1.42550\n","epoch no.0 train no.2320  loss = 1.04369 avg_loss = 1.42550\n","epoch no.0 train no.2330  loss = 1.27944 avg_loss = 1.42912\n","epoch no.0 train no.2330  loss = 1.27944 avg_loss = 1.42912\n","epoch no.0 train no.2340  loss = 1.56309 avg_loss = 1.43624\n","epoch no.0 train no.2340  loss = 1.56309 avg_loss = 1.43624\n","epoch no.0 train no.2350  loss = 1.61233 avg_loss = 1.44601\n","epoch no.0 train no.2350  loss = 1.61233 avg_loss = 1.44601\n","epoch no.0 train no.2360  loss = 1.44655 avg_loss = 1.45565\n","epoch no.0 train no.2360  loss = 1.44655 avg_loss = 1.45565\n","epoch no.0 train no.2370  loss = 1.34895 avg_loss = 1.45811\n","epoch no.0 train no.2370  loss = 1.34895 avg_loss = 1.45811\n","epoch no.0 train no.2380  loss = 1.42374 avg_loss = 1.45485\n","epoch no.0 train no.2380  loss = 1.42374 avg_loss = 1.45485\n","epoch no.0 train no.2390  loss = 1.03810 avg_loss = 1.44428\n","epoch no.0 train no.2390  loss = 1.03810 avg_loss = 1.44428\n","epoch no.0 train no.2400  loss = 1.47068 avg_loss = 1.44352\n","epoch no.0 train no.2400  loss = 1.47068 avg_loss = 1.44352\n","epoch no.0 train no.2410  loss = 2.11009 avg_loss = 1.44122\n","epoch no.0 train no.2410  loss = 2.11009 avg_loss = 1.44122\n","epoch no.0 train no.2420  loss = 1.00364 avg_loss = 1.43714\n","epoch no.0 train no.2420  loss = 1.00364 avg_loss = 1.43714\n","epoch no.0 train no.2430  loss = 0.82658 avg_loss = 1.41986\n","epoch no.0 train no.2430  loss = 0.82658 avg_loss = 1.41986\n","epoch no.0 train no.2440  loss = 1.77532 avg_loss = 1.43940\n","epoch no.0 train no.2440  loss = 1.77532 avg_loss = 1.43940\n","epoch no.0 train no.2450  loss = 1.18388 avg_loss = 1.43181\n","epoch no.0 train no.2450  loss = 1.18388 avg_loss = 1.43181\n","epoch no.0 train no.2460  loss = 1.55572 avg_loss = 1.42406\n","epoch no.0 train no.2460  loss = 1.55572 avg_loss = 1.42406\n","epoch no.0 train no.2470  loss = 1.62532 avg_loss = 1.41241\n","epoch no.0 train no.2470  loss = 1.62532 avg_loss = 1.41241\n","epoch no.0 train no.2480  loss = 1.21686 avg_loss = 1.40902\n","epoch no.0 train no.2480  loss = 1.21686 avg_loss = 1.40902\n","epoch no.0 train no.2490  loss = 1.85312 avg_loss = 1.39776\n","epoch no.0 train no.2490  loss = 1.85312 avg_loss = 1.39776\n","epoch no.0 train no.2500  loss = 2.28848 avg_loss = 1.40558\n","epoch no.0 train no.2500  loss = 2.28848 avg_loss = 1.40558\n","epoch no.0 train no.2510  loss = 1.59524 avg_loss = 1.39582\n","epoch no.0 train no.2510  loss = 1.59524 avg_loss = 1.39582\n","epoch no.0 train no.2520  loss = 1.46019 avg_loss = 1.40277\n","epoch no.0 train no.2520  loss = 1.46019 avg_loss = 1.40277\n","epoch no.0 train no.2530  loss = 1.17925 avg_loss = 1.38890\n","epoch no.0 train no.2530  loss = 1.17925 avg_loss = 1.38890\n","epoch no.0 train no.2540  loss = 1.54625 avg_loss = 1.38989\n","epoch no.0 train no.2540  loss = 1.54625 avg_loss = 1.38989\n","epoch no.0 train no.2550  loss = 1.99576 avg_loss = 1.38835\n","epoch no.0 train no.2550  loss = 1.99576 avg_loss = 1.38835\n","epoch no.0 train no.2560  loss = 1.26044 avg_loss = 1.39308\n","epoch no.0 train no.2560  loss = 1.26044 avg_loss = 1.39308\n","epoch no.0 train no.2570  loss = 1.17865 avg_loss = 1.39954\n","epoch no.0 train no.2570  loss = 1.17865 avg_loss = 1.39954\n","epoch no.0 train no.2580  loss = 1.08357 avg_loss = 1.40553\n","epoch no.0 train no.2580  loss = 1.08357 avg_loss = 1.40553\n","epoch no.0 train no.2590  loss = 1.39771 avg_loss = 1.39977\n","epoch no.0 train no.2590  loss = 1.39771 avg_loss = 1.39977\n","epoch no.0 train no.2600  loss = 2.36642 avg_loss = 1.41299\n","epoch no.0 train no.2600  loss = 2.36642 avg_loss = 1.41299\n","epoch no.0 train no.2610  loss = 1.25961 avg_loss = 1.40282\n","epoch no.0 train no.2610  loss = 1.25961 avg_loss = 1.40282\n","epoch no.0 train no.2620  loss = 1.07821 avg_loss = 1.41543\n","epoch no.0 train no.2620  loss = 1.07821 avg_loss = 1.41543\n","epoch no.0 train no.2630  loss = 0.89498 avg_loss = 1.41902\n","epoch no.0 train no.2630  loss = 0.89498 avg_loss = 1.41902\n","epoch no.0 train no.2640  loss = 2.44648 avg_loss = 1.42686\n","epoch no.0 train no.2640  loss = 2.44648 avg_loss = 1.42686\n","epoch no.0 train no.2650  loss = 1.28267 avg_loss = 1.41924\n","epoch no.0 train no.2650  loss = 1.28267 avg_loss = 1.41924\n","epoch no.0 train no.2660  loss = 1.30429 avg_loss = 1.42693\n","epoch no.0 train no.2660  loss = 1.30429 avg_loss = 1.42693\n","epoch no.0 train no.2670  loss = 1.20443 avg_loss = 1.43285\n","epoch no.0 train no.2670  loss = 1.20443 avg_loss = 1.43285\n","epoch no.0 train no.2680  loss = 1.62748 avg_loss = 1.41718\n","epoch no.0 train no.2680  loss = 1.62748 avg_loss = 1.41718\n","epoch no.0 train no.2690  loss = 1.13432 avg_loss = 1.41806\n","epoch no.0 train no.2690  loss = 1.13432 avg_loss = 1.41806\n","epoch no.0 train no.2700  loss = 1.74422 avg_loss = 1.39647\n","epoch no.0 train no.2700  loss = 1.74422 avg_loss = 1.39647\n","epoch no.0 train no.2710  loss = 0.99064 avg_loss = 1.37804\n","epoch no.0 train no.2710  loss = 0.99064 avg_loss = 1.37804\n","epoch no.0 train no.2720  loss = 2.14213 avg_loss = 1.39440\n","epoch no.0 train no.2720  loss = 2.14213 avg_loss = 1.39440\n","epoch no.0 train no.2730  loss = 1.28019 avg_loss = 1.38636\n","epoch no.0 train no.2730  loss = 1.28019 avg_loss = 1.38636\n","epoch no.0 train no.2740  loss = 1.37957 avg_loss = 1.38737\n","epoch no.0 train no.2740  loss = 1.37957 avg_loss = 1.38737\n","epoch no.0 train no.2750  loss = 1.12830 avg_loss = 1.37372\n","epoch no.0 train no.2750  loss = 1.12830 avg_loss = 1.37372\n","epoch no.0 train no.2760  loss = 1.58674 avg_loss = 1.37871\n","epoch no.0 train no.2760  loss = 1.58674 avg_loss = 1.37871\n","epoch no.0 train no.2770  loss = 1.65045 avg_loss = 1.38951\n","epoch no.0 train no.2770  loss = 1.65045 avg_loss = 1.38951\n","epoch no.0 train no.2780  loss = 1.25078 avg_loss = 1.37954\n","epoch no.0 train no.2780  loss = 1.25078 avg_loss = 1.37954\n","epoch no.0 train no.2790  loss = 1.62709 avg_loss = 1.39006\n","epoch no.0 train no.2790  loss = 1.62709 avg_loss = 1.39006\n","epoch no.0 train no.2800  loss = 1.26296 avg_loss = 1.38596\n","epoch no.0 train no.2800  loss = 1.26296 avg_loss = 1.38596\n","epoch no.0 train no.2810  loss = 1.26872 avg_loss = 1.39279\n","epoch no.0 train no.2810  loss = 1.26872 avg_loss = 1.39279\n","epoch no.0 train no.2820  loss = 1.07292 avg_loss = 1.38263\n","epoch no.0 train no.2820  loss = 1.07292 avg_loss = 1.38263\n","epoch no.0 train no.2830  loss = 1.18063 avg_loss = 1.38867\n","epoch no.0 train no.2830  loss = 1.18063 avg_loss = 1.38867\n","epoch no.0 train no.2840  loss = 1.07698 avg_loss = 1.38097\n","epoch no.0 train no.2840  loss = 1.07698 avg_loss = 1.38097\n","epoch no.0 train no.2850  loss = 1.72545 avg_loss = 1.38134\n","epoch no.0 train no.2850  loss = 1.72545 avg_loss = 1.38134\n","epoch no.0 train no.2860  loss = 1.72615 avg_loss = 1.38992\n","epoch no.0 train no.2860  loss = 1.72615 avg_loss = 1.38992\n","epoch no.0 train no.2870  loss = 1.18709 avg_loss = 1.40146\n","epoch no.0 train no.2870  loss = 1.18709 avg_loss = 1.40146\n","epoch no.0 train no.2880  loss = 1.12549 avg_loss = 1.39648\n","epoch no.0 train no.2880  loss = 1.12549 avg_loss = 1.39648\n","epoch no.0 train no.2890  loss = 1.84028 avg_loss = 1.42568\n","epoch no.0 train no.2890  loss = 1.84028 avg_loss = 1.42568\n","epoch no.0 train no.2900  loss = 1.33245 avg_loss = 1.41146\n","epoch no.0 train no.2900  loss = 1.33245 avg_loss = 1.41146\n","epoch no.0 train no.2910  loss = 0.96005 avg_loss = 1.40622\n","epoch no.0 train no.2910  loss = 0.96005 avg_loss = 1.40622\n","epoch no.0 train no.2920  loss = 0.80149 avg_loss = 1.40790\n","epoch no.0 train no.2920  loss = 0.80149 avg_loss = 1.40790\n","epoch no.0 train no.2930  loss = 1.41791 avg_loss = 1.41594\n","epoch no.0 train no.2930  loss = 1.41791 avg_loss = 1.41594\n","epoch no.0 train no.2940  loss = 1.59188 avg_loss = 1.42406\n","epoch no.0 train no.2940  loss = 1.59188 avg_loss = 1.42406\n","epoch no.0 train no.2950  loss = 1.80338 avg_loss = 1.41658\n","epoch no.0 train no.2950  loss = 1.80338 avg_loss = 1.41658\n","epoch no.0 train no.2960  loss = 1.54518 avg_loss = 1.42250\n","epoch no.0 train no.2960  loss = 1.54518 avg_loss = 1.42250\n","epoch no.0 train no.2970  loss = 1.17369 avg_loss = 1.41754\n","epoch no.0 train no.2970  loss = 1.17369 avg_loss = 1.41754\n","epoch no.0 train no.2980  loss = 1.35310 avg_loss = 1.43112\n","epoch no.0 train no.2980  loss = 1.35310 avg_loss = 1.43112\n","epoch no.0 train no.2990  loss = 1.46105 avg_loss = 1.42585\n","epoch no.0 train no.2990  loss = 1.46105 avg_loss = 1.42585\n","epoch no.0 train no.3000  loss = 1.19850 avg_loss = 1.42701\n","epoch no.0 train no.3000  loss = 1.19850 avg_loss = 1.42701\n","to_tokens: ['▁', '▁노', '▁', '해요', '▁', '▁my', '▁', '해요', '▁', '만을', 'ab', 'y', '▁', 'o', 'y', '▁', 'ab', 'y', '▁', 'ab', 'y', '▁', 'o', 'y', '▁', 'ab', 'y', '▁', 'ab', 'y', '▁', 'ab', 'e', '▁', 'o', 'y', '▁']\n","사랑해요\n","\n","사랑해요 그대만을\n","\n","사랑해요\n","\n","그대 baby\n","\n","baby\n","\n","baby baby\n","\n","baby\n","\n","baby\n","\n","baby  baby\n","\n","babe\n","\n","\n","to_tokens: ['▁', '▁노', '▁', '해요', '▁', '▁my', '▁', '해요', '▁', '만을', 'ab', 'y', '▁', 'o', 'y', '▁', 'ab', 'y', '▁', 'ab', 'y', '▁', 'o', 'y', '▁', 'ab', 'y', '▁', 'ab', 'y', '▁', 'ab', 'e', '▁', 'o', 'y', '▁']\n","사랑해요\n","\n","사랑해요 그대만을\n","\n","사랑해요\n","\n","그대 baby\n","\n","baby\n","\n","baby baby\n","\n","baby\n","\n","baby\n","\n","baby  baby\n","\n","babe\n","\n","\n","epoch no.0 train no.3010  loss = 1.60519 avg_loss = 1.42640\n","epoch no.0 train no.3010  loss = 1.60519 avg_loss = 1.42640\n","epoch no.0 train no.3020  loss = 1.38004 avg_loss = 1.42337\n","epoch no.0 train no.3020  loss = 1.38004 avg_loss = 1.42337\n","epoch no.0 train no.3030  loss = 1.04631 avg_loss = 1.42485\n","epoch no.0 train no.3030  loss = 1.04631 avg_loss = 1.42485\n","epoch no.0 train no.3040  loss = 0.86913 avg_loss = 1.41797\n","epoch no.0 train no.3040  loss = 0.86913 avg_loss = 1.41797\n","epoch no.0 train no.3050  loss = 1.99436 avg_loss = 1.41682\n","epoch no.0 train no.3050  loss = 1.99436 avg_loss = 1.41682\n","epoch no.0 train no.3060  loss = 0.82088 avg_loss = 1.41276\n","epoch no.0 train no.3060  loss = 0.82088 avg_loss = 1.41276\n","epoch no.0 train no.3070  loss = 1.10366 avg_loss = 1.41635\n","epoch no.0 train no.3070  loss = 1.10366 avg_loss = 1.41635\n","epoch no.0 train no.3080  loss = 1.42821 avg_loss = 1.41280\n","epoch no.0 train no.3080  loss = 1.42821 avg_loss = 1.41280\n","epoch no.0 train no.3090  loss = 1.15554 avg_loss = 1.40329\n","epoch no.0 train no.3090  loss = 1.15554 avg_loss = 1.40329\n","epoch no.0 train no.3100  loss = 1.58898 avg_loss = 1.40131\n","epoch no.0 train no.3100  loss = 1.58898 avg_loss = 1.40131\n","epoch no.0 train no.3110  loss = 0.86460 avg_loss = 1.41241\n","epoch no.0 train no.3110  loss = 0.86460 avg_loss = 1.41241\n","epoch no.0 train no.3120  loss = 1.36224 avg_loss = 1.41699\n","epoch no.0 train no.3120  loss = 1.36224 avg_loss = 1.41699\n","epoch no.0 train no.3130  loss = 1.44662 avg_loss = 1.42453\n","epoch no.0 train no.3130  loss = 1.44662 avg_loss = 1.42453\n","epoch no.0 train no.3140  loss = 1.14939 avg_loss = 1.41140\n","epoch no.0 train no.3140  loss = 1.14939 avg_loss = 1.41140\n","epoch no.0 train no.3150  loss = 1.68387 avg_loss = 1.42094\n","epoch no.0 train no.3150  loss = 1.68387 avg_loss = 1.42094\n","epoch no.0 train no.3160  loss = 1.10703 avg_loss = 1.39897\n","epoch no.0 train no.3160  loss = 1.10703 avg_loss = 1.39897\n","epoch no.0 train no.3170  loss = 1.30215 avg_loss = 1.39124\n","epoch no.0 train no.3170  loss = 1.30215 avg_loss = 1.39124\n","epoch no.0 train no.3180  loss = 1.49835 avg_loss = 1.37905\n","epoch no.0 train no.3180  loss = 1.49835 avg_loss = 1.37905\n","epoch no.0 train no.3190  loss = 1.46283 avg_loss = 1.38638\n","epoch no.0 train no.3190  loss = 1.46283 avg_loss = 1.38638\n","epoch no.0 train no.3200  loss = 1.82690 avg_loss = 1.40136\n","epoch no.0 train no.3200  loss = 1.82690 avg_loss = 1.40136\n","epoch no.0 train no.3210  loss = 1.41038 avg_loss = 1.39161\n","epoch no.0 train no.3210  loss = 1.41038 avg_loss = 1.39161\n","epoch no.0 train no.3220  loss = 1.29693 avg_loss = 1.38467\n","epoch no.0 train no.3220  loss = 1.29693 avg_loss = 1.38467\n","epoch no.0 train no.3230  loss = 1.68374 avg_loss = 1.39119\n","epoch no.0 train no.3230  loss = 1.68374 avg_loss = 1.39119\n","epoch no.0 train no.3240  loss = 1.82912 avg_loss = 1.40101\n","epoch no.0 train no.3240  loss = 1.82912 avg_loss = 1.40101\n","epoch no.0 train no.3250  loss = 1.12521 avg_loss = 1.40474\n","epoch no.0 train no.3250  loss = 1.12521 avg_loss = 1.40474\n","epoch no.0 train no.3260  loss = 1.04709 avg_loss = 1.39893\n","epoch no.0 train no.3260  loss = 1.04709 avg_loss = 1.39893\n","epoch no.0 train no.3270  loss = 0.49289 avg_loss = 1.37068\n","epoch no.0 train no.3270  loss = 0.49289 avg_loss = 1.37068\n","epoch no.0 train no.3280  loss = 2.24447 avg_loss = 1.35482\n","epoch no.0 train no.3280  loss = 2.24447 avg_loss = 1.35482\n","epoch no.0 train no.3290  loss = 1.58347 avg_loss = 1.36744\n","epoch no.0 train no.3290  loss = 1.58347 avg_loss = 1.36744\n","epoch no.0 train no.3300  loss = 1.43432 avg_loss = 1.36250\n","epoch no.0 train no.3300  loss = 1.43432 avg_loss = 1.36250\n","epoch no.0 train no.3310  loss = 1.44554 avg_loss = 1.36228\n","epoch no.0 train no.3310  loss = 1.44554 avg_loss = 1.36228\n","epoch no.0 train no.3320  loss = 1.83809 avg_loss = 1.37725\n","epoch no.0 train no.3320  loss = 1.83809 avg_loss = 1.37725\n","epoch no.0 train no.3330  loss = 1.02689 avg_loss = 1.36672\n","epoch no.0 train no.3330  loss = 1.02689 avg_loss = 1.36672\n","epoch no.0 train no.3340  loss = 1.90664 avg_loss = 1.37749\n","epoch no.0 train no.3340  loss = 1.90664 avg_loss = 1.37749\n","epoch no.0 train no.3350  loss = 1.31771 avg_loss = 1.37866\n","epoch no.0 train no.3350  loss = 1.31771 avg_loss = 1.37866\n","epoch no.0 train no.3360  loss = 1.00690 avg_loss = 1.37394\n","epoch no.0 train no.3360  loss = 1.00690 avg_loss = 1.37394\n","epoch no.0 train no.3370  loss = 2.03815 avg_loss = 1.36713\n","epoch no.0 train no.3370  loss = 2.03815 avg_loss = 1.36713\n","epoch no.0 train no.3380  loss = 1.40981 avg_loss = 1.35873\n","epoch no.0 train no.3380  loss = 1.40981 avg_loss = 1.35873\n","epoch no.0 train no.3390  loss = 0.84783 avg_loss = 1.34251\n","epoch no.0 train no.3390  loss = 0.84783 avg_loss = 1.34251\n","epoch no.0 train no.3400  loss = 1.64543 avg_loss = 1.35054\n","epoch no.0 train no.3400  loss = 1.64543 avg_loss = 1.35054\n","epoch no.0 train no.3410  loss = 1.42642 avg_loss = 1.35132\n","epoch no.0 train no.3410  loss = 1.42642 avg_loss = 1.35132\n","epoch no.0 train no.3420  loss = 1.69366 avg_loss = 1.36478\n","epoch no.0 train no.3420  loss = 1.69366 avg_loss = 1.36478\n","epoch no.0 train no.3430  loss = 1.17278 avg_loss = 1.36444\n","epoch no.0 train no.3430  loss = 1.17278 avg_loss = 1.36444\n","epoch no.0 train no.3440  loss = 1.73793 avg_loss = 1.36918\n","epoch no.0 train no.3440  loss = 1.73793 avg_loss = 1.36918\n","epoch no.0 train no.3450  loss = 0.87347 avg_loss = 1.35922\n","epoch no.0 train no.3450  loss = 0.87347 avg_loss = 1.35922\n","epoch no.0 train no.3460  loss = 0.93487 avg_loss = 1.34377\n","epoch no.0 train no.3460  loss = 0.93487 avg_loss = 1.34377\n","epoch no.0 train no.3470  loss = 1.30589 avg_loss = 1.33601\n","epoch no.0 train no.3470  loss = 1.30589 avg_loss = 1.33601\n","epoch no.0 train no.3480  loss = 1.50312 avg_loss = 1.34378\n","epoch no.0 train no.3480  loss = 1.50312 avg_loss = 1.34378\n","epoch no.0 train no.3490  loss = 1.43247 avg_loss = 1.33379\n","epoch no.0 train no.3490  loss = 1.43247 avg_loss = 1.33379\n","epoch no.0 train no.3500  loss = 0.99050 avg_loss = 1.31924\n","epoch no.0 train no.3500  loss = 0.99050 avg_loss = 1.31924\n","epoch no.0 train no.3510  loss = 0.84760 avg_loss = 1.31175\n","epoch no.0 train no.3510  loss = 0.84760 avg_loss = 1.31175\n","epoch no.0 train no.3520  loss = 1.58499 avg_loss = 1.31154\n","epoch no.0 train no.3520  loss = 1.58499 avg_loss = 1.31154\n","epoch no.0 train no.3530  loss = 1.82581 avg_loss = 1.31576\n","epoch no.0 train no.3530  loss = 1.82581 avg_loss = 1.31576\n","epoch no.0 train no.3540  loss = 1.70030 avg_loss = 1.31607\n","epoch no.0 train no.3540  loss = 1.70030 avg_loss = 1.31607\n","epoch no.0 train no.3550  loss = 1.06759 avg_loss = 1.32335\n","epoch no.0 train no.3550  loss = 1.06759 avg_loss = 1.32335\n","epoch no.0 train no.3560  loss = 1.40000 avg_loss = 1.33058\n","epoch no.0 train no.3560  loss = 1.40000 avg_loss = 1.33058\n","epoch no.0 train no.3570  loss = 1.54345 avg_loss = 1.34225\n","epoch no.0 train no.3570  loss = 1.54345 avg_loss = 1.34225\n","epoch no.0 train no.3580  loss = 1.43550 avg_loss = 1.35065\n","epoch no.0 train no.3580  loss = 1.43550 avg_loss = 1.35065\n","epoch no.0 train no.3590  loss = 1.19658 avg_loss = 1.34802\n","epoch no.0 train no.3590  loss = 1.19658 avg_loss = 1.34802\n","epoch no.0 train no.3600  loss = 1.02352 avg_loss = 1.34490\n","epoch no.0 train no.3600  loss = 1.02352 avg_loss = 1.34490\n","epoch no.0 train no.3610  loss = 1.12322 avg_loss = 1.36030\n","epoch no.0 train no.3610  loss = 1.12322 avg_loss = 1.36030\n","epoch no.0 train no.3620  loss = 1.38946 avg_loss = 1.36399\n","epoch no.0 train no.3620  loss = 1.38946 avg_loss = 1.36399\n","epoch no.0 train no.3630  loss = 1.55248 avg_loss = 1.36550\n","epoch no.0 train no.3630  loss = 1.55248 avg_loss = 1.36550\n","epoch no.0 train no.3640  loss = 0.77667 avg_loss = 1.35343\n","epoch no.0 train no.3640  loss = 0.77667 avg_loss = 1.35343\n","epoch no.0 train no.3650  loss = 1.65566 avg_loss = 1.34902\n","epoch no.0 train no.3650  loss = 1.65566 avg_loss = 1.34902\n","epoch no.0 train no.3660  loss = 1.34339 avg_loss = 1.33339\n","epoch no.0 train no.3660  loss = 1.34339 avg_loss = 1.33339\n","epoch no.0 train no.3670  loss = 1.50899 avg_loss = 1.34350\n","epoch no.0 train no.3670  loss = 1.50899 avg_loss = 1.34350\n","epoch no.0 train no.3680  loss = 1.29626 avg_loss = 1.34648\n","epoch no.0 train no.3680  loss = 1.29626 avg_loss = 1.34648\n","epoch no.0 train no.3690  loss = 1.32437 avg_loss = 1.34250\n","epoch no.0 train no.3690  loss = 1.32437 avg_loss = 1.34250\n","epoch no.0 train no.3700  loss = 1.70671 avg_loss = 1.35772\n","epoch no.0 train no.3700  loss = 1.70671 avg_loss = 1.35772\n","epoch no.0 train no.3710  loss = 1.06699 avg_loss = 1.36394\n","epoch no.0 train no.3710  loss = 1.06699 avg_loss = 1.36394\n","epoch no.0 train no.3720  loss = 1.31241 avg_loss = 1.35238\n","epoch no.0 train no.3720  loss = 1.31241 avg_loss = 1.35238\n","epoch no.0 train no.3730  loss = 1.42736 avg_loss = 1.34410\n","epoch no.0 train no.3730  loss = 1.42736 avg_loss = 1.34410\n","epoch no.0 train no.3740  loss = 1.38202 avg_loss = 1.35624\n","epoch no.0 train no.3740  loss = 1.38202 avg_loss = 1.35624\n","epoch no.0 train no.3750  loss = 1.81833 avg_loss = 1.37001\n","epoch no.0 train no.3750  loss = 1.81833 avg_loss = 1.37001\n","epoch no.0 train no.3760  loss = 1.47920 avg_loss = 1.38656\n","epoch no.0 train no.3760  loss = 1.47920 avg_loss = 1.38656\n","epoch no.0 train no.3770  loss = 1.59701 avg_loss = 1.37726\n","epoch no.0 train no.3770  loss = 1.59701 avg_loss = 1.37726\n","epoch no.0 train no.3780  loss = 1.13606 avg_loss = 1.36171\n","epoch no.0 train no.3780  loss = 1.13606 avg_loss = 1.36171\n","epoch no.0 train no.3790  loss = 1.14816 avg_loss = 1.35739\n","epoch no.0 train no.3790  loss = 1.14816 avg_loss = 1.35739\n","epoch no.0 train no.3800  loss = 0.88883 avg_loss = 1.35026\n","epoch no.0 train no.3800  loss = 0.88883 avg_loss = 1.35026\n","epoch no.0 train no.3810  loss = 0.73267 avg_loss = 1.33930\n","epoch no.0 train no.3810  loss = 0.73267 avg_loss = 1.33930\n","epoch no.0 train no.3820  loss = 1.63731 avg_loss = 1.33764\n","epoch no.0 train no.3820  loss = 1.63731 avg_loss = 1.33764\n","epoch no.0 train no.3830  loss = 1.86722 avg_loss = 1.35058\n","epoch no.0 train no.3830  loss = 1.86722 avg_loss = 1.35058\n","epoch no.0 train no.3840  loss = 1.08476 avg_loss = 1.34928\n","epoch no.0 train no.3840  loss = 1.08476 avg_loss = 1.34928\n","epoch no.0 train no.3850  loss = 1.10561 avg_loss = 1.33064\n","epoch no.0 train no.3850  loss = 1.10561 avg_loss = 1.33064\n","epoch no.0 train no.3860  loss = 1.32382 avg_loss = 1.33658\n","epoch no.0 train no.3860  loss = 1.32382 avg_loss = 1.33658\n","epoch no.0 train no.3870  loss = 1.42599 avg_loss = 1.32777\n","epoch no.0 train no.3870  loss = 1.42599 avg_loss = 1.32777\n","epoch no.0 train no.3880  loss = 0.98081 avg_loss = 1.34945\n","epoch no.0 train no.3880  loss = 0.98081 avg_loss = 1.34945\n","epoch no.0 train no.3890  loss = 1.25295 avg_loss = 1.33134\n","epoch no.0 train no.3890  loss = 1.25295 avg_loss = 1.33134\n","epoch no.0 train no.3900  loss = 1.30777 avg_loss = 1.33396\n","epoch no.0 train no.3900  loss = 1.30777 avg_loss = 1.33396\n","epoch no.0 train no.3910  loss = 1.45428 avg_loss = 1.32532\n","epoch no.0 train no.3910  loss = 1.45428 avg_loss = 1.32532\n","epoch no.0 train no.3920  loss = 0.99202 avg_loss = 1.31033\n","epoch no.0 train no.3920  loss = 0.99202 avg_loss = 1.31033\n","epoch no.0 train no.3930  loss = 1.01146 avg_loss = 1.31027\n","epoch no.0 train no.3930  loss = 1.01146 avg_loss = 1.31027\n","epoch no.0 train no.3940  loss = 1.46067 avg_loss = 1.31541\n","epoch no.0 train no.3940  loss = 1.46067 avg_loss = 1.31541\n","epoch no.0 train no.3950  loss = 1.83114 avg_loss = 1.31543\n","epoch no.0 train no.3950  loss = 1.83114 avg_loss = 1.31543\n","epoch no.0 train no.3960  loss = 1.28874 avg_loss = 1.31625\n","epoch no.0 train no.3960  loss = 1.28874 avg_loss = 1.31625\n","epoch no.0 train no.3970  loss = 1.47378 avg_loss = 1.31904\n","epoch no.0 train no.3970  loss = 1.47378 avg_loss = 1.31904\n","epoch no.0 train no.3980  loss = 1.02625 avg_loss = 1.29733\n","epoch no.0 train no.3980  loss = 1.02625 avg_loss = 1.29733\n","epoch no.0 train no.3990  loss = 1.84398 avg_loss = 1.30560\n","epoch no.0 train no.3990  loss = 1.84398 avg_loss = 1.30560\n","epoch no.0 train no.4000  loss = 0.77935 avg_loss = 1.30593\n","epoch no.0 train no.4000  loss = 0.77935 avg_loss = 1.30593\n","to_tokens: ['▁', '해요', '▁사랑', '▁사랑', ',', '▁여자', '▁', '▁사람도', '▁', '▁사람도', ',', '▁사람도', ',', '다', 'les', '린', '▁바람', '▁덧', '▁', '네', '▁', '란', ',', '▁li', ',', '▁사람도', ',', '▁사람도', ',', '▁사람도', '▁', '며', '시', '며', '▁', '며', '▁', '지', '▁살', '<unused0>', 'k', 'ate', '<unused0>', '다가', '..', 'k', '▁', '<unused0>', '다', 'seo', 'k', '랬', '▁말도', '도', '남', '▁', '래', 'seo', 'k', '▁', '<unused0>', '다', 'seo', 'k', '▁', '파', '도', '▁처럼', '▁만들', '다', 'seo', 'k', '▁', '<unused0>', '다', 'seo', 'k', '▁', '파', '도', '▁처럼', '▁만들', '다', 'seo', 'k', '▁']\n","사랑도, 사람도 , 사람도, 사람도, 사람도\n","\n",", 사람도 살며 시린 상처를 남기고\n","\n","가네\n","\n",".려, 사람도  ,도, 사람도, 사람도\n","\n","살며 살며 살며 살며\n","\n","seok\n","\n","랬다seok\n","\n","랬다seok\n","\n",",파도 처럼  만들다seok\n","\n","랬다seok\n","\n",",파도처럼 만들다seok\n","\n","랬다seok\n","\n",",파도 처럼 만들다seok\n","to_tokens: ['▁', '해요', '▁사랑', '▁사랑', ',', '▁여자', '▁', '▁사람도', '▁', '▁사람도', ',', '▁사람도', ',', '다', 'les', '린', '▁바람', '▁덧', '▁', '네', '▁', '란', ',', '▁li', ',', '▁사람도', ',', '▁사람도', ',', '▁사람도', '▁', '며', '시', '며', '▁', '며', '▁', '지', '▁살', '<unused0>', 'k', 'ate', '<unused0>', '다가', '..', 'k', '▁', '<unused0>', '다', 'seo', 'k', '랬', '▁말도', '도', '남', '▁', '래', 'seo', 'k', '▁', '<unused0>', '다', 'seo', 'k', '▁', '파', '도', '▁처럼', '▁만들', '다', 'seo', 'k', '▁', '<unused0>', '다', 'seo', 'k', '▁', '파', '도', '▁처럼', '▁만들', '다', 'seo', 'k', '▁']\n","사랑도, 사람도 , 사람도, 사람도, 사람도\n","\n",", 사람도 살며 시린 상처를 남기고\n","\n","가네\n","\n",".려, 사람도  ,도, 사람도, 사람도\n","\n","살며 살며 살며 살며\n","\n","seok\n","\n","랬다seok\n","\n","랬다seok\n","\n",",파도 처럼  만들다seok\n","\n","랬다seok\n","\n",",파도처럼 만들다seok\n","\n","랬다seok\n","\n",",파도 처럼 만들다seok\n","epoch no.0 train no.4010  loss = 0.69762 avg_loss = 1.30651\n","epoch no.0 train no.4010  loss = 0.69762 avg_loss = 1.30651\n","epoch no.0 train no.4020  loss = 1.20755 avg_loss = 1.31324\n","epoch no.0 train no.4020  loss = 1.20755 avg_loss = 1.31324\n","epoch no.0 train no.4030  loss = 1.16849 avg_loss = 1.31391\n","epoch no.0 train no.4030  loss = 1.16849 avg_loss = 1.31391\n","epoch no.0 train no.4040  loss = 1.41326 avg_loss = 1.31320\n","epoch no.0 train no.4040  loss = 1.41326 avg_loss = 1.31320\n","epoch no.0 train no.4050  loss = 1.18281 avg_loss = 1.31212\n","epoch no.0 train no.4050  loss = 1.18281 avg_loss = 1.31212\n","epoch no.0 train no.4060  loss = 0.75692 avg_loss = 1.30227\n","epoch no.0 train no.4060  loss = 0.75692 avg_loss = 1.30227\n","epoch no.0 train no.4070  loss = 1.35638 avg_loss = 1.30348\n","epoch no.0 train no.4070  loss = 1.35638 avg_loss = 1.30348\n","epoch no.0 train no.4080  loss = 1.22477 avg_loss = 1.30076\n","epoch no.0 train no.4080  loss = 1.22477 avg_loss = 1.30076\n","epoch no.0 train no.4090  loss = 1.14154 avg_loss = 1.29268\n","epoch no.0 train no.4090  loss = 1.14154 avg_loss = 1.29268\n","epoch no.0 train no.4100  loss = 1.97503 avg_loss = 1.28518\n","epoch no.0 train no.4100  loss = 1.97503 avg_loss = 1.28518\n","epoch no.0 train no.4110  loss = 1.47303 avg_loss = 1.29588\n","epoch no.0 train no.4110  loss = 1.47303 avg_loss = 1.29588\n","epoch no.0 train no.4120  loss = 1.36920 avg_loss = 1.29366\n","epoch no.0 train no.4120  loss = 1.36920 avg_loss = 1.29366\n","epoch no.0 train no.4130  loss = 0.88723 avg_loss = 1.27801\n","epoch no.0 train no.4130  loss = 0.88723 avg_loss = 1.27801\n","epoch no.0 train no.4140  loss = 1.25775 avg_loss = 1.28824\n","epoch no.0 train no.4140  loss = 1.25775 avg_loss = 1.28824\n","epoch no.0 train no.4150  loss = 0.77276 avg_loss = 1.30576\n","epoch no.0 train no.4150  loss = 0.77276 avg_loss = 1.30576\n","epoch no.0 train no.4160  loss = 1.26485 avg_loss = 1.30985\n","epoch no.0 train no.4160  loss = 1.26485 avg_loss = 1.30985\n","epoch no.0 train no.4170  loss = 1.51324 avg_loss = 1.32046\n","epoch no.0 train no.4170  loss = 1.51324 avg_loss = 1.32046\n","epoch no.0 train no.4180  loss = 1.38688 avg_loss = 1.31940\n","epoch no.0 train no.4180  loss = 1.38688 avg_loss = 1.31940\n","epoch no.0 train no.4190  loss = 1.39855 avg_loss = 1.32818\n","epoch no.0 train no.4190  loss = 1.39855 avg_loss = 1.32818\n","epoch no.0 train no.4200  loss = 2.10000 avg_loss = 1.34461\n","epoch no.0 train no.4200  loss = 2.10000 avg_loss = 1.34461\n","epoch no.0 train no.4210  loss = 1.75004 avg_loss = 1.34546\n","epoch no.0 train no.4210  loss = 1.75004 avg_loss = 1.34546\n","epoch no.0 train no.4220  loss = 1.39678 avg_loss = 1.34572\n","epoch no.0 train no.4220  loss = 1.39678 avg_loss = 1.34572\n","epoch no.0 train no.4230  loss = 1.65840 avg_loss = 1.35276\n","epoch no.0 train no.4230  loss = 1.65840 avg_loss = 1.35276\n","epoch no.0 train no.4240  loss = 1.05606 avg_loss = 1.36466\n","epoch no.0 train no.4240  loss = 1.05606 avg_loss = 1.36466\n","epoch no.0 train no.4250  loss = 0.73230 avg_loss = 1.35006\n","epoch no.0 train no.4250  loss = 0.73230 avg_loss = 1.35006\n","epoch no.0 train no.4260  loss = 0.96249 avg_loss = 1.34007\n","epoch no.0 train no.4260  loss = 0.96249 avg_loss = 1.34007\n","epoch no.0 train no.4270  loss = 1.63150 avg_loss = 1.34687\n","epoch no.0 train no.4270  loss = 1.63150 avg_loss = 1.34687\n","epoch no.0 train no.4280  loss = 1.23159 avg_loss = 1.35794\n","epoch no.0 train no.4280  loss = 1.23159 avg_loss = 1.35794\n","epoch no.0 train no.4290  loss = 1.23690 avg_loss = 1.35932\n","epoch no.0 train no.4290  loss = 1.23690 avg_loss = 1.35932\n","epoch no.0 train no.4300  loss = 1.51699 avg_loss = 1.34728\n","epoch no.0 train no.4300  loss = 1.51699 avg_loss = 1.34728\n","epoch no.0 train no.4310  loss = 1.96332 avg_loss = 1.33453\n","epoch no.0 train no.4310  loss = 1.96332 avg_loss = 1.33453\n","epoch no.0 train no.4320  loss = 1.93798 avg_loss = 1.34790\n","epoch no.0 train no.4320  loss = 1.93798 avg_loss = 1.34790\n","epoch no.0 train no.4330  loss = 1.42201 avg_loss = 1.35057\n","epoch no.0 train no.4330  loss = 1.42201 avg_loss = 1.35057\n","epoch no.0 train no.4340  loss = 1.04656 avg_loss = 1.33048\n","epoch no.0 train no.4340  loss = 1.04656 avg_loss = 1.33048\n","epoch no.0 train no.4350  loss = 1.45558 avg_loss = 1.34784\n","epoch no.0 train no.4350  loss = 1.45558 avg_loss = 1.34784\n","epoch no.0 train no.4360  loss = 1.43134 avg_loss = 1.35219\n","epoch no.0 train no.4360  loss = 1.43134 avg_loss = 1.35219\n","epoch no.0 train no.4370  loss = 1.21786 avg_loss = 1.34765\n","epoch no.0 train no.4370  loss = 1.21786 avg_loss = 1.34765\n","epoch no.0 train no.4380  loss = 1.71351 avg_loss = 1.34325\n","epoch no.0 train no.4380  loss = 1.71351 avg_loss = 1.34325\n","epoch no.0 train no.4390  loss = 1.03557 avg_loss = 1.34623\n","epoch no.0 train no.4390  loss = 1.03557 avg_loss = 1.34623\n","epoch no.0 train no.4400  loss = 0.81094 avg_loss = 1.34502\n","epoch no.0 train no.4400  loss = 0.81094 avg_loss = 1.34502\n","epoch no.0 train no.4410  loss = 1.39553 avg_loss = 1.34569\n","epoch no.0 train no.4410  loss = 1.39553 avg_loss = 1.34569\n","epoch no.0 train no.4420  loss = 1.91816 avg_loss = 1.33385\n","epoch no.0 train no.4420  loss = 1.91816 avg_loss = 1.33385\n","epoch no.0 train no.4430  loss = 1.08322 avg_loss = 1.31218\n","epoch no.0 train no.4430  loss = 1.08322 avg_loss = 1.31218\n","epoch no.0 train no.4440  loss = 1.31760 avg_loss = 1.31183\n","epoch no.0 train no.4440  loss = 1.31760 avg_loss = 1.31183\n","epoch no.0 train no.4450  loss = 1.55592 avg_loss = 1.31282\n","epoch no.0 train no.4450  loss = 1.55592 avg_loss = 1.31282\n","epoch no.0 train no.4460  loss = 1.28034 avg_loss = 1.31122\n","epoch no.0 train no.4460  loss = 1.28034 avg_loss = 1.31122\n","epoch no.0 train no.4470  loss = 1.68453 avg_loss = 1.32339\n","epoch no.0 train no.4470  loss = 1.68453 avg_loss = 1.32339\n","epoch no.0 train no.4480  loss = 1.31491 avg_loss = 1.31722\n","epoch no.0 train no.4480  loss = 1.31491 avg_loss = 1.31722\n","epoch no.0 train no.4490  loss = 1.22454 avg_loss = 1.31253\n","epoch no.0 train no.4490  loss = 1.22454 avg_loss = 1.31253\n","epoch no.0 train no.4500  loss = 1.12621 avg_loss = 1.30966\n","epoch no.0 train no.4500  loss = 1.12621 avg_loss = 1.30966\n","epoch no.0 train no.4510  loss = 0.25890 avg_loss = 1.30435\n","epoch no.0 train no.4510  loss = 0.25890 avg_loss = 1.30435\n","epoch no.0 train no.4520  loss = 0.84762 avg_loss = 1.29388\n","epoch no.0 train no.4520  loss = 0.84762 avg_loss = 1.29388\n","epoch no.0 train no.4530  loss = 0.80125 avg_loss = 1.28223\n","epoch no.0 train no.4530  loss = 0.80125 avg_loss = 1.28223\n","epoch no.0 train no.4540  loss = 1.65419 avg_loss = 1.27770\n","epoch no.0 train no.4540  loss = 1.65419 avg_loss = 1.27770\n","epoch no.0 train no.4550  loss = 1.32173 avg_loss = 1.27221\n","epoch no.0 train no.4550  loss = 1.32173 avg_loss = 1.27221\n","epoch no.0 train no.4560  loss = 0.90039 avg_loss = 1.25421\n","epoch no.0 train no.4560  loss = 0.90039 avg_loss = 1.25421\n","epoch no.0 train no.4570  loss = 0.84821 avg_loss = 1.25397\n","epoch no.0 train no.4570  loss = 0.84821 avg_loss = 1.25397\n","epoch no.0 train no.4580  loss = 0.90125 avg_loss = 1.25799\n","epoch no.0 train no.4580  loss = 0.90125 avg_loss = 1.25799\n","epoch no.0 train no.4590  loss = 1.15485 avg_loss = 1.24116\n","epoch no.0 train no.4590  loss = 1.15485 avg_loss = 1.24116\n","epoch no.0 train no.4600  loss = 1.82528 avg_loss = 1.25057\n","epoch no.0 train no.4600  loss = 1.82528 avg_loss = 1.25057\n","epoch no.0 train no.4610  loss = 1.03562 avg_loss = 1.24615\n","epoch no.0 train no.4610  loss = 1.03562 avg_loss = 1.24615\n","epoch no.0 train no.4620  loss = 1.77262 avg_loss = 1.25444\n","epoch no.0 train no.4620  loss = 1.77262 avg_loss = 1.25444\n","epoch no.0 train no.4630  loss = 1.51233 avg_loss = 1.27862\n","epoch no.0 train no.4630  loss = 1.51233 avg_loss = 1.27862\n","epoch no.0 train no.4640  loss = 1.38873 avg_loss = 1.27350\n","epoch no.0 train no.4640  loss = 1.38873 avg_loss = 1.27350\n","epoch no.0 train no.4650  loss = 1.18207 avg_loss = 1.26409\n","epoch no.0 train no.4650  loss = 1.18207 avg_loss = 1.26409\n","epoch no.0 train no.4660  loss = 1.61988 avg_loss = 1.26852\n","epoch no.0 train no.4660  loss = 1.61988 avg_loss = 1.26852\n","epoch no.0 train no.4670  loss = 1.60041 avg_loss = 1.27119\n","epoch no.0 train no.4670  loss = 1.60041 avg_loss = 1.27119\n","epoch no.0 train no.4680  loss = 0.97862 avg_loss = 1.27913\n","epoch no.0 train no.4680  loss = 0.97862 avg_loss = 1.27913\n","epoch no.0 train no.4690  loss = 1.25519 avg_loss = 1.27225\n","epoch no.0 train no.4690  loss = 1.25519 avg_loss = 1.27225\n","epoch no.0 train no.4700  loss = 1.57555 avg_loss = 1.27799\n","epoch no.0 train no.4700  loss = 1.57555 avg_loss = 1.27799\n","epoch no.0 train no.4710  loss = 1.21992 avg_loss = 1.27363\n","epoch no.0 train no.4710  loss = 1.21992 avg_loss = 1.27363\n","epoch no.0 train no.4720  loss = 1.64647 avg_loss = 1.29604\n","epoch no.0 train no.4720  loss = 1.64647 avg_loss = 1.29604\n","epoch no.0 train no.4730  loss = 1.21645 avg_loss = 1.28388\n","epoch no.0 train no.4730  loss = 1.21645 avg_loss = 1.28388\n","epoch no.0 train no.4740  loss = 0.65995 avg_loss = 1.27872\n","epoch no.0 train no.4740  loss = 0.65995 avg_loss = 1.27872\n","epoch no.0 train no.4750  loss = 1.44476 avg_loss = 1.28643\n","epoch no.0 train no.4750  loss = 1.44476 avg_loss = 1.28643\n","epoch no.0 train no.4760  loss = 1.48517 avg_loss = 1.28830\n","epoch no.0 train no.4760  loss = 1.48517 avg_loss = 1.28830\n","epoch no.0 train no.4770  loss = 1.20075 avg_loss = 1.28865\n","epoch no.0 train no.4770  loss = 1.20075 avg_loss = 1.28865\n","epoch no.0 train no.4780  loss = 1.47132 avg_loss = 1.29759\n","epoch no.0 train no.4780  loss = 1.47132 avg_loss = 1.29759\n","epoch no.0 train no.4790  loss = 1.09527 avg_loss = 1.29007\n","epoch no.0 train no.4790  loss = 1.09527 avg_loss = 1.29007\n","epoch no.0 train no.4800  loss = 0.91324 avg_loss = 1.29459\n","epoch no.0 train no.4800  loss = 0.91324 avg_loss = 1.29459\n","epoch no.0 train no.4810  loss = 0.94081 avg_loss = 1.27831\n","epoch no.0 train no.4810  loss = 0.94081 avg_loss = 1.27831\n","epoch no.0 train no.4820  loss = 0.99698 avg_loss = 1.26991\n","epoch no.0 train no.4820  loss = 0.99698 avg_loss = 1.26991\n","epoch no.0 train no.4830  loss = 1.15074 avg_loss = 1.26935\n","epoch no.0 train no.4830  loss = 1.15074 avg_loss = 1.26935\n","epoch no.0 train no.4840  loss = 1.39695 avg_loss = 1.25758\n","epoch no.0 train no.4840  loss = 1.39695 avg_loss = 1.25758\n","epoch no.0 train no.4850  loss = 1.02147 avg_loss = 1.26973\n","epoch no.0 train no.4850  loss = 1.02147 avg_loss = 1.26973\n","epoch no.0 train no.4860  loss = 1.38750 avg_loss = 1.27156\n","epoch no.0 train no.4860  loss = 1.38750 avg_loss = 1.27156\n","epoch no.0 train no.4870  loss = 1.10538 avg_loss = 1.25663\n","epoch no.0 train no.4870  loss = 1.10538 avg_loss = 1.25663\n","epoch no.0 train no.4880  loss = 0.97028 avg_loss = 1.25481\n","epoch no.0 train no.4880  loss = 0.97028 avg_loss = 1.25481\n","epoch no.0 train no.4890  loss = 1.23980 avg_loss = 1.23916\n","epoch no.0 train no.4890  loss = 1.23980 avg_loss = 1.23916\n","epoch no.0 train no.4900  loss = 1.74655 avg_loss = 1.26649\n","epoch no.0 train no.4900  loss = 1.74655 avg_loss = 1.26649\n","epoch no.0 train no.4910  loss = 1.90434 avg_loss = 1.27915\n","epoch no.0 train no.4910  loss = 1.90434 avg_loss = 1.27915\n","epoch no.0 train no.4920  loss = 1.14649 avg_loss = 1.27438\n","epoch no.0 train no.4920  loss = 1.14649 avg_loss = 1.27438\n","epoch no.0 train no.4930  loss = 1.08831 avg_loss = 1.27775\n","epoch no.0 train no.4930  loss = 1.08831 avg_loss = 1.27775\n","epoch no.0 train no.4940  loss = 2.24280 avg_loss = 1.28878\n","epoch no.0 train no.4940  loss = 2.24280 avg_loss = 1.28878\n","epoch no.0 train no.4950  loss = 1.34938 avg_loss = 1.29612\n","epoch no.0 train no.4950  loss = 1.34938 avg_loss = 1.29612\n","epoch no.0 train no.4960  loss = 1.35302 avg_loss = 1.29548\n","epoch no.0 train no.4960  loss = 1.35302 avg_loss = 1.29548\n","epoch no.0 train no.4970  loss = 1.58333 avg_loss = 1.30663\n","epoch no.0 train no.4970  loss = 1.58333 avg_loss = 1.30663\n","epoch no.0 train no.4980  loss = 1.11282 avg_loss = 1.29542\n","epoch no.0 train no.4980  loss = 1.11282 avg_loss = 1.29542\n","epoch no.0 train no.4990  loss = 1.67021 avg_loss = 1.29253\n","epoch no.0 train no.4990  loss = 1.67021 avg_loss = 1.29253\n","epoch no.0 train no.5000  loss = 0.90301 avg_loss = 1.29869\n","epoch no.0 train no.5000  loss = 0.90301 avg_loss = 1.29869\n","to_tokens: ['▁', '해요', '▁걸', '▁글', '▁', '▁', '둬', '▁', '▁해', '▁', 'H', 'h', ')', '▁', '지마', '▁잊', '▁번', '▁', '▁부탁해', '둬', '▁(', 'H', 'ey', ')', '▁', 'O', 'h', '-', '▁', '도', '의', '▁그', '▁(', '▁두', '▁한', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁잊', '지마', '▁한', '▁글자', '만', '▁알아', '둬', '▁(', 'H', 'ey', ')', '▁(', 'O', 'h', ')', '▁지워', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'H', 'h', ')', '▁잊', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁지워', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'ey', ')', '▁(', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'ey', ')', '▁', 'V', 'er', 'se', '▁1', ':', '▁']\n","사랑이란 두 글자만 알아둬야 해\n","\n","(Oh)\n","\n","잊지마 한 글자만\n","\n","알아둬 (Hey)\n","\n","(Oh)\n","\n","지워 너의 사랑이란 단 글자만 알아둬 (Oh) 잊지마 한 글자만 알아둬 (Hey) (Oh)  지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Hey)  지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Oh)  지워 너의 사랑이란 단 글자만 알아둬 (Oh)\n","\n","(Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Hey)\n","\n","[Verse 1]\n","\n","\n","to_tokens: ['▁', '해요', '▁걸', '▁글', '▁', '▁', '둬', '▁', '▁해', '▁', 'H', 'h', ')', '▁', '지마', '▁잊', '▁번', '▁', '▁부탁해', '둬', '▁(', 'H', 'ey', ')', '▁', 'O', 'h', '-', '▁', '도', '의', '▁그', '▁(', '▁두', '▁한', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁잊', '지마', '▁한', '▁글자', '만', '▁알아', '둬', '▁(', 'H', 'ey', ')', '▁(', 'O', 'h', ')', '▁지워', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'H', 'h', ')', '▁잊', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁지워', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'ey', ')', '▁(', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'h', ')', '▁', 'O', 'h', ')', '▁', '▁너', '의', '▁사랑', '이란', '▁단', '▁글자', '만', '▁알아', '둬', '▁(', 'O', 'ey', ')', '▁', 'V', 'er', 'se', '▁1', ':', '▁']\n","사랑이란 두 글자만 알아둬야 해\n","\n","(Oh)\n","\n","잊지마 한 글자만\n","\n","알아둬 (Hey)\n","\n","(Oh)\n","\n","지워 너의 사랑이란 단 글자만 알아둬 (Oh) 잊지마 한 글자만 알아둬 (Hey) (Oh)  지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Hey)  지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Oh)  지워 너의 사랑이란 단 글자만 알아둬 (Oh)\n","\n","(Oh) 지워 너의 사랑이란 단 글자만 알아둬 (Hey)\n","\n","[Verse 1]\n","\n","\n","epoch no.0 train no.5010  loss = 1.64443 avg_loss = 1.27844\n","epoch no.0 train no.5010  loss = 1.64443 avg_loss = 1.27844\n","epoch no.0 train no.5020  loss = 1.31485 avg_loss = 1.28652\n","epoch no.0 train no.5020  loss = 1.31485 avg_loss = 1.28652\n","epoch no.0 train no.5030  loss = 1.18191 avg_loss = 1.29328\n","epoch no.0 train no.5030  loss = 1.18191 avg_loss = 1.29328\n","epoch no.0 train no.5040  loss = 1.94524 avg_loss = 1.30530\n","epoch no.0 train no.5040  loss = 1.94524 avg_loss = 1.30530\n","epoch no.0 train no.5050  loss = 0.66358 avg_loss = 1.30599\n","epoch no.0 train no.5050  loss = 0.66358 avg_loss = 1.30599\n","epoch no.0 train no.5060  loss = 1.87481 avg_loss = 1.31206\n","epoch no.0 train no.5060  loss = 1.87481 avg_loss = 1.31206\n","epoch no.0 train no.5070  loss = 0.65214 avg_loss = 1.28422\n","epoch no.0 train no.5070  loss = 0.65214 avg_loss = 1.28422\n","epoch no.0 train no.5080  loss = 0.71971 avg_loss = 1.28987\n","epoch no.0 train no.5080  loss = 0.71971 avg_loss = 1.28987\n","epoch no.0 train no.5090  loss = 0.97883 avg_loss = 1.29071\n","epoch no.0 train no.5090  loss = 0.97883 avg_loss = 1.29071\n","epoch no.0 train no.5100  loss = 1.21571 avg_loss = 1.30062\n","epoch no.0 train no.5100  loss = 1.21571 avg_loss = 1.30062\n","epoch no.0 train no.5110  loss = 1.16166 avg_loss = 1.29804\n","epoch no.0 train no.5110  loss = 1.16166 avg_loss = 1.29804\n","epoch no.0 train no.5120  loss = 1.46354 avg_loss = 1.32064\n","epoch no.0 train no.5120  loss = 1.46354 avg_loss = 1.32064\n","epoch no.0 train no.5130  loss = 1.48548 avg_loss = 1.31335\n","epoch no.0 train no.5130  loss = 1.48548 avg_loss = 1.31335\n","epoch no.0 train no.5140  loss = 1.68787 avg_loss = 1.31931\n","epoch no.0 train no.5140  loss = 1.68787 avg_loss = 1.31931\n","epoch no.0 train no.5150  loss = 1.05003 avg_loss = 1.29892\n","epoch no.0 train no.5150  loss = 1.05003 avg_loss = 1.29892\n","epoch no.0 train no.5160  loss = 1.62550 avg_loss = 1.29459\n","epoch no.0 train no.5160  loss = 1.62550 avg_loss = 1.29459\n","epoch no.0 train no.5170  loss = 1.54221 avg_loss = 1.28063\n","epoch no.0 train no.5170  loss = 1.54221 avg_loss = 1.28063\n","epoch no.0 train no.5180  loss = 0.40305 avg_loss = 1.26849\n","epoch no.0 train no.5180  loss = 0.40305 avg_loss = 1.26849\n","epoch no.0 train no.5190  loss = 1.11537 avg_loss = 1.28291\n","epoch no.0 train no.5190  loss = 1.11537 avg_loss = 1.28291\n","epoch no.0 train no.5200  loss = 1.58484 avg_loss = 1.29390\n","epoch no.0 train no.5200  loss = 1.58484 avg_loss = 1.29390\n","epoch no.0 train no.5210  loss = 1.13474 avg_loss = 1.28885\n","epoch no.0 train no.5210  loss = 1.13474 avg_loss = 1.28885\n","epoch no.0 train no.5220  loss = 1.26926 avg_loss = 1.26777\n","epoch no.0 train no.5220  loss = 1.26926 avg_loss = 1.26777\n","epoch no.0 train no.5230  loss = 1.64367 avg_loss = 1.26525\n","epoch no.0 train no.5230  loss = 1.64367 avg_loss = 1.26525\n","epoch no.0 train no.5240  loss = 1.67372 avg_loss = 1.27093\n","epoch no.0 train no.5240  loss = 1.67372 avg_loss = 1.27093\n","epoch no.0 train no.5250  loss = 1.68125 avg_loss = 1.24731\n","epoch no.0 train no.5250  loss = 1.68125 avg_loss = 1.24731\n","epoch no.0 train no.5260  loss = 1.05734 avg_loss = 1.24461\n","epoch no.0 train no.5260  loss = 1.05734 avg_loss = 1.24461\n","epoch no.0 train no.5270  loss = 1.17053 avg_loss = 1.22627\n","epoch no.0 train no.5270  loss = 1.17053 avg_loss = 1.22627\n","epoch no.0 train no.5280  loss = 1.51436 avg_loss = 1.22707\n","epoch no.0 train no.5280  loss = 1.51436 avg_loss = 1.22707\n","epoch no.0 train no.5290  loss = 1.31725 avg_loss = 1.21776\n","epoch no.0 train no.5290  loss = 1.31725 avg_loss = 1.21776\n","epoch no.0 train no.5300  loss = 1.89299 avg_loss = 1.23209\n","epoch no.0 train no.5300  loss = 1.89299 avg_loss = 1.23209\n","epoch no.0 train no.5310  loss = 1.71099 avg_loss = 1.25443\n","epoch no.0 train no.5310  loss = 1.71099 avg_loss = 1.25443\n","epoch no.0 train no.5320  loss = 1.27904 avg_loss = 1.25140\n","epoch no.0 train no.5320  loss = 1.27904 avg_loss = 1.25140\n","epoch no.0 train no.5330  loss = 1.40102 avg_loss = 1.24145\n","epoch no.0 train no.5330  loss = 1.40102 avg_loss = 1.24145\n","epoch no.0 train no.5340  loss = 1.30535 avg_loss = 1.25551\n","epoch no.0 train no.5340  loss = 1.30535 avg_loss = 1.25551\n","epoch no.0 train no.5350  loss = 0.59151 avg_loss = 1.24364\n","epoch no.0 train no.5350  loss = 0.59151 avg_loss = 1.24364\n","epoch no.0 train no.5360  loss = 1.06355 avg_loss = 1.23649\n","epoch no.0 train no.5360  loss = 1.06355 avg_loss = 1.23649\n","epoch no.0 train no.5370  loss = 1.61086 avg_loss = 1.24696\n","epoch no.0 train no.5370  loss = 1.61086 avg_loss = 1.24696\n","epoch no.0 train no.5380  loss = 1.19921 avg_loss = 1.24534\n","epoch no.0 train no.5380  loss = 1.19921 avg_loss = 1.24534\n","epoch no.0 train no.5390  loss = 0.73712 avg_loss = 1.24500\n","epoch no.0 train no.5390  loss = 0.73712 avg_loss = 1.24500\n","epoch no.0 train no.5400  loss = 1.16037 avg_loss = 1.24798\n","epoch no.0 train no.5400  loss = 1.16037 avg_loss = 1.24798\n","epoch no.0 train no.5410  loss = 1.23468 avg_loss = 1.25285\n","epoch no.0 train no.5410  loss = 1.23468 avg_loss = 1.25285\n","epoch no.0 train no.5420  loss = 0.69561 avg_loss = 1.26185\n","epoch no.0 train no.5420  loss = 0.69561 avg_loss = 1.26185\n","epoch no.0 train no.5430  loss = 1.12522 avg_loss = 1.24648\n","epoch no.0 train no.5430  loss = 1.12522 avg_loss = 1.24648\n","epoch no.0 train no.5440  loss = 1.56681 avg_loss = 1.23862\n","epoch no.0 train no.5440  loss = 1.56681 avg_loss = 1.23862\n","epoch no.0 train no.5450  loss = 0.66763 avg_loss = 1.25159\n","epoch no.0 train no.5450  loss = 0.66763 avg_loss = 1.25159\n","epoch no.0 train no.5460  loss = 1.38205 avg_loss = 1.25032\n","epoch no.0 train no.5460  loss = 1.38205 avg_loss = 1.25032\n","epoch no.0 train no.5470  loss = 1.03163 avg_loss = 1.24397\n","epoch no.0 train no.5470  loss = 1.03163 avg_loss = 1.24397\n","epoch no.0 train no.5480  loss = 0.64403 avg_loss = 1.24676\n","epoch no.0 train no.5480  loss = 0.64403 avg_loss = 1.24676\n","epoch no.0 train no.5490  loss = 1.18138 avg_loss = 1.25635\n","epoch no.0 train no.5490  loss = 1.18138 avg_loss = 1.25635\n","epoch no.0 train no.5500  loss = 1.31118 avg_loss = 1.24048\n","epoch no.0 train no.5500  loss = 1.31118 avg_loss = 1.24048\n","epoch no.0 train no.5510  loss = 1.83428 avg_loss = 1.26845\n","epoch no.0 train no.5510  loss = 1.83428 avg_loss = 1.26845\n","epoch no.0 train no.5520  loss = 1.30525 avg_loss = 1.27493\n","epoch no.0 train no.5520  loss = 1.30525 avg_loss = 1.27493\n","epoch no.0 train no.5530  loss = 0.88437 avg_loss = 1.25990\n","epoch no.0 train no.5530  loss = 0.88437 avg_loss = 1.25990\n","epoch no.0 train no.5540  loss = 0.97294 avg_loss = 1.25410\n","epoch no.0 train no.5540  loss = 0.97294 avg_loss = 1.25410\n","epoch no.0 train no.5550  loss = 1.01876 avg_loss = 1.24490\n","epoch no.0 train no.5550  loss = 1.01876 avg_loss = 1.24490\n","epoch no.0 train no.5560  loss = 0.61304 avg_loss = 1.25375\n","epoch no.0 train no.5560  loss = 0.61304 avg_loss = 1.25375\n","epoch no.0 train no.5570  loss = 1.81008 avg_loss = 1.26550\n","epoch no.0 train no.5570  loss = 1.81008 avg_loss = 1.26550\n","epoch no.0 train no.5580  loss = 0.92859 avg_loss = 1.26801\n","epoch no.0 train no.5580  loss = 0.92859 avg_loss = 1.26801\n","epoch no.0 train no.5590  loss = 0.82527 avg_loss = 1.25513\n","epoch no.0 train no.5590  loss = 0.82527 avg_loss = 1.25513\n","epoch no.0 train no.5600  loss = 1.42020 avg_loss = 1.26065\n","epoch no.0 train no.5600  loss = 1.42020 avg_loss = 1.26065\n","epoch no.0 train no.5610  loss = 1.15340 avg_loss = 1.24492\n","epoch no.0 train no.5610  loss = 1.15340 avg_loss = 1.24492\n","epoch no.0 train no.5620  loss = 1.34200 avg_loss = 1.26773\n","epoch no.0 train no.5620  loss = 1.34200 avg_loss = 1.26773\n","epoch no.0 train no.5630  loss = 1.26648 avg_loss = 1.26482\n","epoch no.0 train no.5630  loss = 1.26648 avg_loss = 1.26482\n","epoch no.0 train no.5640  loss = 0.50991 avg_loss = 1.25186\n","epoch no.0 train no.5640  loss = 0.50991 avg_loss = 1.25186\n","epoch no.0 train no.5650  loss = 1.70186 avg_loss = 1.25046\n","epoch no.0 train no.5650  loss = 1.70186 avg_loss = 1.25046\n","epoch no.0 train no.5660  loss = 1.07481 avg_loss = 1.25595\n","epoch no.0 train no.5660  loss = 1.07481 avg_loss = 1.25595\n","epoch no.0 train no.5670  loss = 1.12122 avg_loss = 1.25707\n","epoch no.0 train no.5670  loss = 1.12122 avg_loss = 1.25707\n","epoch no.0 train no.5680  loss = 1.07831 avg_loss = 1.24154\n","epoch no.0 train no.5680  loss = 1.07831 avg_loss = 1.24154\n","epoch no.0 train no.5690  loss = 1.41518 avg_loss = 1.24335\n","epoch no.0 train no.5690  loss = 1.41518 avg_loss = 1.24335\n","epoch no.0 train no.5700  loss = 1.17354 avg_loss = 1.25140\n","epoch no.0 train no.5700  loss = 1.17354 avg_loss = 1.25140\n","epoch no.0 train no.5710  loss = 1.39879 avg_loss = 1.24449\n","epoch no.0 train no.5710  loss = 1.39879 avg_loss = 1.24449\n","epoch no.0 train no.5720  loss = 1.11075 avg_loss = 1.23484\n","epoch no.0 train no.5720  loss = 1.11075 avg_loss = 1.23484\n","epoch no.0 train no.5730  loss = 1.09172 avg_loss = 1.23690\n","epoch no.0 train no.5730  loss = 1.09172 avg_loss = 1.23690\n","epoch no.0 train no.5740  loss = 1.73811 avg_loss = 1.22717\n","epoch no.0 train no.5740  loss = 1.73811 avg_loss = 1.22717\n","epoch no.0 train no.5750  loss = 2.01029 avg_loss = 1.24071\n","epoch no.0 train no.5750  loss = 2.01029 avg_loss = 1.24071\n","epoch no.0 train no.5760  loss = 1.20996 avg_loss = 1.24127\n","epoch no.0 train no.5760  loss = 1.20996 avg_loss = 1.24127\n","epoch no.0 train no.5770  loss = 0.74632 avg_loss = 1.24108\n","epoch no.0 train no.5770  loss = 0.74632 avg_loss = 1.24108\n","epoch no.0 train no.5780  loss = 1.43554 avg_loss = 1.24210\n","epoch no.0 train no.5780  loss = 1.43554 avg_loss = 1.24210\n","epoch no.0 train no.5790  loss = 1.14510 avg_loss = 1.24570\n","epoch no.0 train no.5790  loss = 1.14510 avg_loss = 1.24570\n","epoch no.0 train no.5800  loss = 0.60704 avg_loss = 1.23877\n","epoch no.0 train no.5800  loss = 0.60704 avg_loss = 1.23877\n","epoch no.0 train no.5810  loss = 1.00917 avg_loss = 1.23685\n","epoch no.0 train no.5810  loss = 1.00917 avg_loss = 1.23685\n","epoch no.0 train no.5820  loss = 1.00449 avg_loss = 1.24863\n","epoch no.0 train no.5820  loss = 1.00449 avg_loss = 1.24863\n","epoch no.0 train no.5830  loss = 0.83691 avg_loss = 1.25308\n","epoch no.0 train no.5830  loss = 0.83691 avg_loss = 1.25308\n","epoch no.0 train no.5840  loss = 1.04465 avg_loss = 1.24372\n","epoch no.0 train no.5840  loss = 1.04465 avg_loss = 1.24372\n","epoch no.0 train no.5850  loss = 1.53979 avg_loss = 1.24848\n","epoch no.0 train no.5850  loss = 1.53979 avg_loss = 1.24848\n","epoch no.0 train no.5860  loss = 1.11139 avg_loss = 1.24919\n","epoch no.0 train no.5860  loss = 1.11139 avg_loss = 1.24919\n","epoch no.0 train no.5870  loss = 0.86859 avg_loss = 1.24846\n","epoch no.0 train no.5870  loss = 0.86859 avg_loss = 1.24846\n","epoch no.0 train no.5880  loss = 1.84864 avg_loss = 1.25557\n","epoch no.0 train no.5880  loss = 1.84864 avg_loss = 1.25557\n","epoch no.0 train no.5890  loss = 1.33728 avg_loss = 1.25855\n","epoch no.0 train no.5890  loss = 1.33728 avg_loss = 1.25855\n","epoch no.0 train no.5900  loss = 1.75876 avg_loss = 1.27168\n","epoch no.0 train no.5900  loss = 1.75876 avg_loss = 1.27168\n","epoch no.0 train no.5910  loss = 1.26645 avg_loss = 1.27824\n","epoch no.0 train no.5910  loss = 1.26645 avg_loss = 1.27824\n","epoch no.0 train no.5920  loss = 1.29683 avg_loss = 1.26950\n","epoch no.0 train no.5920  loss = 1.29683 avg_loss = 1.26950\n","epoch no.0 train no.5930  loss = 1.40497 avg_loss = 1.26114\n","epoch no.0 train no.5930  loss = 1.40497 avg_loss = 1.26114\n","epoch no.0 train no.5940  loss = 0.94944 avg_loss = 1.27476\n","epoch no.0 train no.5940  loss = 0.94944 avg_loss = 1.27476\n","epoch no.0 train no.5950  loss = 1.01539 avg_loss = 1.28238\n","epoch no.0 train no.5950  loss = 1.01539 avg_loss = 1.28238\n","epoch no.0 train no.5960  loss = 0.85923 avg_loss = 1.27686\n","epoch no.0 train no.5960  loss = 0.85923 avg_loss = 1.27686\n","epoch no.0 train no.5970  loss = 1.61422 avg_loss = 1.27910\n","epoch no.0 train no.5970  loss = 1.61422 avg_loss = 1.27910\n","epoch no.0 train no.5980  loss = 1.63166 avg_loss = 1.27642\n","epoch no.0 train no.5980  loss = 1.63166 avg_loss = 1.27642\n","epoch no.0 train no.5990  loss = 1.35827 avg_loss = 1.27411\n","epoch no.0 train no.5990  loss = 1.35827 avg_loss = 1.27411\n","epoch no.0 train no.6000  loss = 1.36974 avg_loss = 1.28059\n","epoch no.0 train no.6000  loss = 1.36974 avg_loss = 1.28059\n","to_tokens: ['▁', '해요', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁사랑', 'om', 'an', 'ization', '▁', 'ul', 't', 'ara', 'a', '▁', 'o', 'a', '▁na', 'an', 'b', 'ig', 'e', '▁', 'ane', 'k', 'ke', '▁', 'seo', 'ne', 'un', '▁', '▁', 'ga', 'am', 'ul', '▁', 'e', '▁', 'ame', 'ul', '▁', 'ae', 'ha', '▁', 'on', 'e', 'ow', 'a', 'ji', 'on', '▁', 'ol', '▁', 'i', 'ha', '▁', 'or', 'ae', 'ga', 'e', 'h', 'u', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y']\n","사랑한다 사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","Romanization  Oneulttala  neowa nunbusige hamkke issneun\n","\n","nae mameul\n","\n","nae  mameul wihan  eoryeowassdeon\n","\n","neol  wihan\n","\n","norae yeah\n","\n","yeah\n","\n","yeah yeah\n","\n","yeah  yeah yeah  yeah yeah  yeah yeah\n","\n","yeah\n","\n","yeah  yeah  yeah yeah yeah yeah\n","\n","yeah  yeah\n","\n","yeah  yeah yeah yeah\n","\n","yeah yeah\n","\n","yeah yeah yeah yeah yeah yeah\n","\n","yeah yeah yeah yeah  yeah yeah\n","\n","yeah yeah\n","\n","yeah yeah yeah yeah yeah yeah yeah  yeah  yeah yeah yeah\n","\n","yeah yeah yeah yeah yeah  yeah yeah yeah yeah\n","\n","yeah  yeah yeah yeah yeah yeah yeah yeah yeah  yeah yeah  yeah yeah yeah yeah yeah yeah yeah yeah\n","\n","yeah yeah yeah yeah yeah  yeah yeah  yeah yeahh yeah yeah  yeah  yeah yeah yeah  yeah  yeah yeah  yeah  yeah yeah y\n","to_tokens: ['▁', '해요', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁', '한다', '▁사랑', 'om', 'an', 'ization', '▁', 'ul', 't', 'ara', 'a', '▁', 'o', 'a', '▁na', 'an', 'b', 'ig', 'e', '▁', 'ane', 'k', 'ke', '▁', 'seo', 'ne', 'un', '▁', '▁', 'ga', 'am', 'ul', '▁', 'e', '▁', 'ame', 'ul', '▁', 'ae', 'ha', '▁', 'on', 'e', 'ow', 'a', 'ji', 'on', '▁', 'ol', '▁', 'i', 'ha', '▁', 'or', 'ae', 'ga', 'e', 'h', 'u', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y', 'ea', 'h', '▁y']\n","사랑한다 사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","사랑한다\n","\n","Romanization  Oneulttala  neowa nunbusige hamkke issneun\n","\n","nae mameul\n","\n","nae  mameul wihan  eoryeowassdeon\n","\n","neol  wihan\n","\n","norae yeah\n","\n","yeah\n","\n","yeah yeah\n","\n","yeah  yeah yeah  yeah yeah  yeah yeah\n","\n","yeah\n","\n","yeah  yeah  yeah yeah yeah yeah\n","\n","yeah  yeah\n","\n","yeah  yeah yeah yeah\n","\n","yeah yeah\n","\n","yeah yeah yeah yeah yeah yeah\n","\n","yeah yeah yeah yeah  yeah yeah\n","\n","yeah yeah\n","\n","yeah yeah yeah yeah yeah yeah yeah  yeah  yeah yeah yeah\n","\n","yeah yeah yeah yeah yeah  yeah yeah yeah yeah\n","\n","yeah  yeah yeah yeah yeah yeah yeah yeah yeah  yeah yeah  yeah yeah yeah yeah yeah yeah yeah yeah\n","\n","yeah yeah yeah yeah yeah  yeah yeah  yeah yeahh yeah yeah  yeah  yeah yeah yeah  yeah  yeah yeah  yeah  yeah yeah y\n","epoch no.0 train no.6010  loss = 1.49393 avg_loss = 1.26821\n","epoch no.0 train no.6010  loss = 1.49393 avg_loss = 1.26821\n","epoch no.0 train no.6020  loss = 1.15180 avg_loss = 1.25120\n","epoch no.0 train no.6020  loss = 1.15180 avg_loss = 1.25120\n","epoch no.0 train no.6030  loss = 1.67686 avg_loss = 1.26119\n","epoch no.0 train no.6030  loss = 1.67686 avg_loss = 1.26119\n","epoch no.0 train no.6040  loss = 0.76530 avg_loss = 1.26252\n","epoch no.0 train no.6040  loss = 0.76530 avg_loss = 1.26252\n","epoch no.0 train no.6050  loss = 1.60246 avg_loss = 1.28180\n","epoch no.0 train no.6050  loss = 1.60246 avg_loss = 1.28180\n","epoch no.0 train no.6060  loss = 1.67791 avg_loss = 1.28242\n","epoch no.0 train no.6060  loss = 1.67791 avg_loss = 1.28242\n","epoch no.0 train no.6070  loss = 1.20956 avg_loss = 1.28987\n","epoch no.0 train no.6070  loss = 1.20956 avg_loss = 1.28987\n","epoch no.0 train no.6080  loss = 1.64529 avg_loss = 1.27763\n","epoch no.0 train no.6080  loss = 1.64529 avg_loss = 1.27763\n","epoch no.0 train no.6090  loss = 1.15588 avg_loss = 1.26042\n","epoch no.0 train no.6090  loss = 1.15588 avg_loss = 1.26042\n","epoch no.0 train no.6100  loss = 0.97221 avg_loss = 1.24670\n","epoch no.0 train no.6100  loss = 0.97221 avg_loss = 1.24670\n","epoch no.0 train no.6110  loss = 0.68550 avg_loss = 1.25181\n","epoch no.0 train no.6110  loss = 0.68550 avg_loss = 1.25181\n","epoch no.0 train no.6120  loss = 1.17265 avg_loss = 1.26002\n","epoch no.0 train no.6120  loss = 1.17265 avg_loss = 1.26002\n","epoch no.0 train no.6130  loss = 0.88622 avg_loss = 1.24623\n","epoch no.0 train no.6130  loss = 0.88622 avg_loss = 1.24623\n","epoch no.0 train no.6140  loss = 0.81082 avg_loss = 1.23430\n","epoch no.0 train no.6140  loss = 0.81082 avg_loss = 1.23430\n","epoch no.0 train no.6150  loss = 0.78217 avg_loss = 1.22593\n","epoch no.0 train no.6150  loss = 0.78217 avg_loss = 1.22593\n","epoch no.0 train no.6160  loss = 1.18160 avg_loss = 1.22366\n","epoch no.0 train no.6160  loss = 1.18160 avg_loss = 1.22366\n","epoch no.0 train no.6170  loss = 1.29993 avg_loss = 1.24555\n","epoch no.0 train no.6170  loss = 1.29993 avg_loss = 1.24555\n","epoch no.0 train no.6180  loss = 1.16255 avg_loss = 1.24336\n","epoch no.0 train no.6180  loss = 1.16255 avg_loss = 1.24336\n","epoch no.0 train no.6190  loss = 1.11871 avg_loss = 1.22849\n","epoch no.0 train no.6190  loss = 1.11871 avg_loss = 1.22849\n","epoch no.0 train no.6200  loss = 1.05003 avg_loss = 1.23589\n","epoch no.0 train no.6200  loss = 1.05003 avg_loss = 1.23589\n","epoch no.0 train no.6210  loss = 1.01062 avg_loss = 1.23813\n","epoch no.0 train no.6210  loss = 1.01062 avg_loss = 1.23813\n","epoch no.0 train no.6220  loss = 1.79482 avg_loss = 1.23213\n","epoch no.0 train no.6220  loss = 1.79482 avg_loss = 1.23213\n","epoch no.0 train no.6230  loss = 1.30097 avg_loss = 1.22594\n","epoch no.0 train no.6230  loss = 1.30097 avg_loss = 1.22594\n","epoch no.0 train no.6240  loss = 1.01184 avg_loss = 1.23235\n","epoch no.0 train no.6240  loss = 1.01184 avg_loss = 1.23235\n","epoch no.0 train no.6250  loss = 1.04818 avg_loss = 1.22428\n","epoch no.0 train no.6250  loss = 1.04818 avg_loss = 1.22428\n","epoch no.0 train no.6260  loss = 1.15008 avg_loss = 1.22502\n","epoch no.0 train no.6260  loss = 1.15008 avg_loss = 1.22502\n","epoch no.0 train no.6270  loss = 1.32939 avg_loss = 1.22512\n","epoch no.0 train no.6270  loss = 1.32939 avg_loss = 1.22512\n","epoch no.0 train no.6280  loss = 1.51848 avg_loss = 1.23532\n","epoch no.0 train no.6280  loss = 1.51848 avg_loss = 1.23532\n","epoch no.0 train no.6290  loss = 1.90821 avg_loss = 1.24114\n","epoch no.0 train no.6290  loss = 1.90821 avg_loss = 1.24114\n","epoch no.0 train no.6300  loss = 0.77824 avg_loss = 1.23598\n","epoch no.0 train no.6300  loss = 0.77824 avg_loss = 1.23598\n","epoch no.0 train no.6310  loss = 0.71992 avg_loss = 1.23503\n","epoch no.0 train no.6310  loss = 0.71992 avg_loss = 1.23503\n","epoch no.0 train no.6320  loss = 0.47395 avg_loss = 1.21329\n","epoch no.0 train no.6320  loss = 0.47395 avg_loss = 1.21329\n","epoch no.0 train no.6330  loss = 1.35684 avg_loss = 1.24483\n","epoch no.0 train no.6330  loss = 1.35684 avg_loss = 1.24483\n","epoch no.0 train no.6340  loss = 1.61069 avg_loss = 1.26740\n","epoch no.0 train no.6340  loss = 1.61069 avg_loss = 1.26740\n","epoch no.0 train no.6350  loss = 1.02267 avg_loss = 1.22864\n","epoch no.0 train no.6350  loss = 1.02267 avg_loss = 1.22864\n","epoch no.0 train no.6360  loss = 0.66931 avg_loss = 1.20798\n","epoch no.0 train no.6360  loss = 0.66931 avg_loss = 1.20798\n","epoch no.0 train no.6370  loss = 0.97240 avg_loss = 1.20470\n","epoch no.0 train no.6370  loss = 0.97240 avg_loss = 1.20470\n","epoch no.0 train no.6380  loss = 1.67442 avg_loss = 1.22584\n","epoch no.0 train no.6380  loss = 1.67442 avg_loss = 1.22584\n","epoch no.0 train no.6390  loss = 0.93612 avg_loss = 1.21045\n","epoch no.0 train no.6390  loss = 0.93612 avg_loss = 1.21045\n","epoch no.0 train no.6400  loss = 1.25623 avg_loss = 1.22003\n","epoch no.0 train no.6400  loss = 1.25623 avg_loss = 1.22003\n","epoch no.0 train no.6410  loss = 1.04738 avg_loss = 1.23826\n","epoch no.0 train no.6410  loss = 1.04738 avg_loss = 1.23826\n","epoch no.0 train no.6420  loss = 0.75666 avg_loss = 1.24330\n","epoch no.0 train no.6420  loss = 0.75666 avg_loss = 1.24330\n","epoch no.0 train no.6430  loss = 1.58892 avg_loss = 1.23901\n","epoch no.0 train no.6430  loss = 1.58892 avg_loss = 1.23901\n","epoch no.0 train no.6440  loss = 0.76592 avg_loss = 1.23121\n","epoch no.0 train no.6440  loss = 0.76592 avg_loss = 1.23121\n","epoch no.0 train no.6450  loss = 0.69443 avg_loss = 1.20973\n","epoch no.0 train no.6450  loss = 0.69443 avg_loss = 1.20973\n","epoch no.0 train no.6460  loss = 1.47679 avg_loss = 1.20130\n","epoch no.0 train no.6460  loss = 1.47679 avg_loss = 1.20130\n","epoch no.0 train no.6470  loss = 0.84906 avg_loss = 1.20358\n","epoch no.0 train no.6470  loss = 0.84906 avg_loss = 1.20358\n","epoch no.0 train no.6480  loss = 1.15709 avg_loss = 1.22401\n","epoch no.0 train no.6480  loss = 1.15709 avg_loss = 1.22401\n","epoch no.0 train no.6490  loss = 1.22259 avg_loss = 1.22807\n","epoch no.0 train no.6490  loss = 1.22259 avg_loss = 1.22807\n","epoch no.0 train no.6500  loss = 0.78290 avg_loss = 1.22274\n","epoch no.0 train no.6500  loss = 0.78290 avg_loss = 1.22274\n","epoch no.0 train no.6510  loss = 1.99502 avg_loss = 1.21953\n","epoch no.0 train no.6510  loss = 1.99502 avg_loss = 1.21953\n","epoch no.0 train no.6520  loss = 1.69344 avg_loss = 1.22493\n","epoch no.0 train no.6520  loss = 1.69344 avg_loss = 1.22493\n","epoch no.0 train no.6530  loss = 0.94124 avg_loss = 1.22531\n","epoch no.0 train no.6530  loss = 0.94124 avg_loss = 1.22531\n","epoch no.0 train no.6540  loss = 1.73950 avg_loss = 1.21749\n","epoch no.0 train no.6540  loss = 1.73950 avg_loss = 1.21749\n","epoch no.0 train no.6550  loss = 0.90965 avg_loss = 1.21133\n","epoch no.0 train no.6550  loss = 0.90965 avg_loss = 1.21133\n","epoch no.0 train no.6560  loss = 1.53006 avg_loss = 1.20910\n","epoch no.0 train no.6560  loss = 1.53006 avg_loss = 1.20910\n","epoch no.0 train no.6570  loss = 1.49475 avg_loss = 1.21853\n","epoch no.0 train no.6570  loss = 1.49475 avg_loss = 1.21853\n","epoch no.0 train no.6580  loss = 0.84002 avg_loss = 1.21391\n","epoch no.0 train no.6580  loss = 0.84002 avg_loss = 1.21391\n","epoch no.0 train no.6590  loss = 1.15159 avg_loss = 1.21420\n","epoch no.0 train no.6590  loss = 1.15159 avg_loss = 1.21420\n","epoch no.0 train no.6600  loss = 1.57719 avg_loss = 1.20620\n","epoch no.0 train no.6600  loss = 1.57719 avg_loss = 1.20620\n","epoch no.0 train no.6610  loss = 1.25227 avg_loss = 1.21358\n","epoch no.0 train no.6610  loss = 1.25227 avg_loss = 1.21358\n","epoch no.0 train no.6620  loss = 1.97998 avg_loss = 1.22169\n","epoch no.0 train no.6620  loss = 1.97998 avg_loss = 1.22169\n","epoch no.0 train no.6630  loss = 1.06512 avg_loss = 1.22653\n","epoch no.0 train no.6630  loss = 1.06512 avg_loss = 1.22653\n","epoch no.0 train no.6640  loss = 1.31911 avg_loss = 1.23387\n","epoch no.0 train no.6640  loss = 1.31911 avg_loss = 1.23387\n","epoch no.0 train no.6650  loss = 0.83446 avg_loss = 1.24536\n","epoch no.0 train no.6650  loss = 0.83446 avg_loss = 1.24536\n","epoch no.0 train no.6660  loss = 0.94584 avg_loss = 1.24295\n","epoch no.0 train no.6660  loss = 0.94584 avg_loss = 1.24295\n","epoch no.0 train no.6670  loss = 1.07909 avg_loss = 1.23596\n","epoch no.0 train no.6670  loss = 1.07909 avg_loss = 1.23596\n","epoch no.0 train no.6680  loss = 1.03309 avg_loss = 1.23106\n","epoch no.0 train no.6680  loss = 1.03309 avg_loss = 1.23106\n","epoch no.0 train no.6690  loss = 0.94000 avg_loss = 1.20801\n","epoch no.0 train no.6690  loss = 0.94000 avg_loss = 1.20801\n","epoch no.0 train no.6700  loss = 1.58630 avg_loss = 1.20108\n","epoch no.0 train no.6700  loss = 1.58630 avg_loss = 1.20108\n","epoch no.0 train no.6710  loss = 1.37478 avg_loss = 1.20019\n","epoch no.0 train no.6710  loss = 1.37478 avg_loss = 1.20019\n","epoch no.0 train no.6720  loss = 1.31386 avg_loss = 1.19696\n","epoch no.0 train no.6720  loss = 1.31386 avg_loss = 1.19696\n","epoch no.0 train no.6730  loss = 1.11536 avg_loss = 1.20746\n","epoch no.0 train no.6730  loss = 1.11536 avg_loss = 1.20746\n","epoch no.0 train no.6740  loss = 1.48311 avg_loss = 1.19894\n","epoch no.0 train no.6740  loss = 1.48311 avg_loss = 1.19894\n","epoch no.0 train no.6750  loss = 0.76623 avg_loss = 1.20175\n","epoch no.0 train no.6750  loss = 0.76623 avg_loss = 1.20175\n","epoch no.0 train no.6760  loss = 1.57807 avg_loss = 1.20021\n","epoch no.0 train no.6760  loss = 1.57807 avg_loss = 1.20021\n","epoch no.0 train no.6770  loss = 1.52895 avg_loss = 1.20576\n","epoch no.0 train no.6770  loss = 1.52895 avg_loss = 1.20576\n","epoch no.0 train no.6780  loss = 1.28349 avg_loss = 1.21965\n","epoch no.0 train no.6780  loss = 1.28349 avg_loss = 1.21965\n","epoch no.0 train no.6790  loss = 0.80299 avg_loss = 1.21799\n","epoch no.0 train no.6790  loss = 0.80299 avg_loss = 1.21799\n","epoch no.0 train no.6800  loss = 1.39915 avg_loss = 1.23763\n","epoch no.0 train no.6800  loss = 1.39915 avg_loss = 1.23763\n","epoch no.0 train no.6810  loss = 1.55248 avg_loss = 1.23883\n","epoch no.0 train no.6810  loss = 1.55248 avg_loss = 1.23883\n","epoch no.0 train no.6820  loss = 1.31970 avg_loss = 1.22751\n","epoch no.0 train no.6820  loss = 1.31970 avg_loss = 1.22751\n","epoch no.0 train no.6830  loss = 1.75036 avg_loss = 1.23746\n","epoch no.0 train no.6830  loss = 1.75036 avg_loss = 1.23746\n","epoch no.0 train no.6840  loss = 1.44626 avg_loss = 1.23540\n","epoch no.0 train no.6840  loss = 1.44626 avg_loss = 1.23540\n","epoch no.0 train no.6850  loss = 1.71786 avg_loss = 1.23600\n","epoch no.0 train no.6850  loss = 1.71786 avg_loss = 1.23600\n","epoch no.0 train no.6860  loss = 1.18340 avg_loss = 1.22902\n","epoch no.0 train no.6860  loss = 1.18340 avg_loss = 1.22902\n","epoch no.0 train no.6870  loss = 1.34849 avg_loss = 1.22132\n","epoch no.0 train no.6870  loss = 1.34849 avg_loss = 1.22132\n","epoch no.0 train no.6880  loss = 1.71132 avg_loss = 1.21037\n","epoch no.0 train no.6880  loss = 1.71132 avg_loss = 1.21037\n","epoch no.0 train no.6890  loss = 0.87715 avg_loss = 1.21242\n","epoch no.0 train no.6890  loss = 0.87715 avg_loss = 1.21242\n","epoch no.0 train no.6900  loss = 2.06037 avg_loss = 1.20685\n","epoch no.0 train no.6900  loss = 2.06037 avg_loss = 1.20685\n","epoch no.0 train no.6910  loss = 1.23588 avg_loss = 1.22004\n","epoch no.0 train no.6910  loss = 1.23588 avg_loss = 1.22004\n","epoch no.0 train no.6920  loss = 1.21101 avg_loss = 1.21314\n","epoch no.0 train no.6920  loss = 1.21101 avg_loss = 1.21314\n","epoch no.0 train no.6930  loss = 1.01427 avg_loss = 1.21516\n","epoch no.0 train no.6930  loss = 1.01427 avg_loss = 1.21516\n","epoch no.0 train no.6940  loss = 1.28401 avg_loss = 1.19789\n","epoch no.0 train no.6940  loss = 1.28401 avg_loss = 1.19789\n","epoch no.0 train no.6950  loss = 0.69620 avg_loss = 1.19383\n","epoch no.0 train no.6950  loss = 0.69620 avg_loss = 1.19383\n","epoch no.0 train no.6960  loss = 1.40535 avg_loss = 1.19253\n","epoch no.0 train no.6960  loss = 1.40535 avg_loss = 1.19253\n","epoch no.0 train no.6970  loss = 1.18895 avg_loss = 1.18894\n","epoch no.0 train no.6970  loss = 1.18895 avg_loss = 1.18894\n","epoch no.0 train no.6980  loss = 1.27761 avg_loss = 1.19966\n","epoch no.0 train no.6980  loss = 1.27761 avg_loss = 1.19966\n","epoch no.0 train no.6990  loss = 1.28474 avg_loss = 1.20704\n","epoch no.0 train no.6990  loss = 1.28474 avg_loss = 1.20704\n","epoch no.0 train no.7000  loss = 1.29417 avg_loss = 1.22125\n","epoch no.0 train no.7000  loss = 1.29417 avg_loss = 1.22125\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁날카', '로워', ',', '▁b', 'itch', ',', '▁때로는', '▁부드럽', '지', '▁', '은', '▁척', '관', '▁다르지', '▁예상이', '▁되니', '▁움직임', '▁', '나', '▁더', '▁매', '겨', '▁보', '렴', ',', '▁한', '▁방을', '▁더', '▁먹여', '▁드림', '▁', '라', '▁하던', '▁양질의', '▁음악을', '▁퍼', '다', '▁줄', '▁뿐', '임', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n","넘 날카로워, bitch, 때로는 부드럽지\n","\n","멋은 척관 다르지 예상이 되니 움직임\n","\n","점수나 더 매겨 보렴, 한 방을 더 먹여 드림\n","\n","뭐라 하던 양질의 음악을 퍼다 줄 뿐임\n","\n","\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁날카', '로워', ',', '▁b', 'itch', ',', '▁때로는', '▁부드럽', '지', '▁', '은', '▁척', '관', '▁다르지', '▁예상이', '▁되니', '▁움직임', '▁', '나', '▁더', '▁매', '겨', '▁보', '렴', ',', '▁한', '▁방을', '▁더', '▁먹여', '▁드림', '▁', '라', '▁하던', '▁양질의', '▁음악을', '▁퍼', '다', '▁줄', '▁뿐', '임', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n","넘 날카로워, bitch, 때로는 부드럽지\n","\n","멋은 척관 다르지 예상이 되니 움직임\n","\n","점수나 더 매겨 보렴, 한 방을 더 먹여 드림\n","\n","뭐라 하던 양질의 음악을 퍼다 줄 뿐임\n","\n","\n","epoch no.0 train no.7010  loss = 0.64335 avg_loss = 1.21505\n","epoch no.0 train no.7010  loss = 0.64335 avg_loss = 1.21505\n","epoch no.0 train no.7020  loss = 1.18082 avg_loss = 1.20949\n","epoch no.0 train no.7020  loss = 1.18082 avg_loss = 1.20949\n","epoch no.0 train no.7030  loss = 0.77068 avg_loss = 1.21477\n","epoch no.0 train no.7030  loss = 0.77068 avg_loss = 1.21477\n","epoch no.0 train no.7040  loss = 1.55137 avg_loss = 1.21569\n","epoch no.0 train no.7040  loss = 1.55137 avg_loss = 1.21569\n","epoch no.0 train no.7050  loss = 0.99232 avg_loss = 1.21467\n","epoch no.0 train no.7050  loss = 0.99232 avg_loss = 1.21467\n","epoch no.0 train no.7060  loss = 0.42811 avg_loss = 1.21469\n","epoch no.0 train no.7060  loss = 0.42811 avg_loss = 1.21469\n","epoch no.0 train no.7070  loss = 1.03571 avg_loss = 1.22029\n","epoch no.0 train no.7070  loss = 1.03571 avg_loss = 1.22029\n","epoch no.0 train no.7080  loss = 1.42391 avg_loss = 1.24775\n","epoch no.0 train no.7080  loss = 1.42391 avg_loss = 1.24775\n","epoch no.0 train no.7090  loss = 1.49403 avg_loss = 1.24583\n","epoch no.0 train no.7090  loss = 1.49403 avg_loss = 1.24583\n","epoch no.0 train no.7100  loss = 1.65181 avg_loss = 1.24657\n","epoch no.0 train no.7100  loss = 1.65181 avg_loss = 1.24657\n","epoch no.0 train no.7110  loss = 1.06561 avg_loss = 1.24963\n","epoch no.0 train no.7110  loss = 1.06561 avg_loss = 1.24963\n","epoch no.0 train no.7120  loss = 1.18696 avg_loss = 1.24937\n","epoch no.0 train no.7120  loss = 1.18696 avg_loss = 1.24937\n","epoch no.0 train no.7130  loss = 1.52066 avg_loss = 1.24751\n","epoch no.0 train no.7130  loss = 1.52066 avg_loss = 1.24751\n","epoch no.0 train no.7140  loss = 0.89965 avg_loss = 1.24006\n","epoch no.0 train no.7140  loss = 0.89965 avg_loss = 1.24006\n","epoch no.0 train no.7150  loss = 0.36083 avg_loss = 1.23350\n","epoch no.0 train no.7150  loss = 0.36083 avg_loss = 1.23350\n","epoch no.0 train no.7160  loss = 1.17831 avg_loss = 1.24187\n","epoch no.0 train no.7160  loss = 1.17831 avg_loss = 1.24187\n","epoch no.0 train no.7170  loss = 0.96229 avg_loss = 1.22594\n","epoch no.0 train no.7170  loss = 0.96229 avg_loss = 1.22594\n","epoch no.0 train no.7180  loss = 0.82026 avg_loss = 1.21433\n","epoch no.0 train no.7180  loss = 0.82026 avg_loss = 1.21433\n","epoch no.0 train no.7190  loss = 0.98947 avg_loss = 1.19498\n","epoch no.0 train no.7190  loss = 0.98947 avg_loss = 1.19498\n","epoch no.0 train no.7200  loss = 1.54089 avg_loss = 1.19542\n","epoch no.0 train no.7200  loss = 1.54089 avg_loss = 1.19542\n","epoch no.0 train no.7210  loss = 0.55990 avg_loss = 1.19257\n","epoch no.0 train no.7210  loss = 0.55990 avg_loss = 1.19257\n","epoch no.0 train no.7220  loss = 1.74191 avg_loss = 1.19721\n","epoch no.0 train no.7220  loss = 1.74191 avg_loss = 1.19721\n","epoch no.0 train no.7230  loss = 1.37139 avg_loss = 1.21331\n","epoch no.0 train no.7230  loss = 1.37139 avg_loss = 1.21331\n","epoch no.0 train no.7240  loss = 1.34534 avg_loss = 1.21778\n","epoch no.0 train no.7240  loss = 1.34534 avg_loss = 1.21778\n","epoch no.0 train no.7250  loss = 0.91555 avg_loss = 1.21535\n","epoch no.0 train no.7250  loss = 0.91555 avg_loss = 1.21535\n","epoch no.0 train no.7260  loss = 1.07241 avg_loss = 1.22602\n","epoch no.0 train no.7260  loss = 1.07241 avg_loss = 1.22602\n","epoch no.0 train no.7270  loss = 1.23707 avg_loss = 1.22162\n","epoch no.0 train no.7270  loss = 1.23707 avg_loss = 1.22162\n","epoch no.0 train no.7280  loss = 1.41326 avg_loss = 1.23611\n","epoch no.0 train no.7280  loss = 1.41326 avg_loss = 1.23611\n","epoch no.0 train no.7290  loss = 0.91892 avg_loss = 1.24552\n","epoch no.0 train no.7290  loss = 0.91892 avg_loss = 1.24552\n","epoch no.0 train no.7300  loss = 0.83320 avg_loss = 1.25048\n","epoch no.0 train no.7300  loss = 0.83320 avg_loss = 1.25048\n","epoch no.0 train no.7310  loss = 1.17844 avg_loss = 1.24768\n","epoch no.0 train no.7310  loss = 1.17844 avg_loss = 1.24768\n","epoch no.0 train no.7320  loss = 1.04796 avg_loss = 1.25800\n","epoch no.0 train no.7320  loss = 1.04796 avg_loss = 1.25800\n","epoch no.0 train no.7330  loss = 1.56878 avg_loss = 1.27186\n","epoch no.0 train no.7330  loss = 1.56878 avg_loss = 1.27186\n","epoch no.0 train no.7340  loss = 1.09674 avg_loss = 1.26913\n","epoch no.0 train no.7340  loss = 1.09674 avg_loss = 1.26913\n","epoch no.0 train no.7350  loss = 1.44332 avg_loss = 1.27400\n","epoch no.0 train no.7350  loss = 1.44332 avg_loss = 1.27400\n","epoch no.0 train no.7360  loss = 1.05793 avg_loss = 1.25145\n","epoch no.0 train no.7360  loss = 1.05793 avg_loss = 1.25145\n","epoch no.0 train no.7370  loss = 1.24065 avg_loss = 1.25097\n","epoch no.0 train no.7370  loss = 1.24065 avg_loss = 1.25097\n","epoch no.0 train no.7380  loss = 0.90481 avg_loss = 1.22139\n","epoch no.0 train no.7380  loss = 0.90481 avg_loss = 1.22139\n","epoch no.0 train no.7390  loss = 1.39755 avg_loss = 1.23705\n","epoch no.0 train no.7390  loss = 1.39755 avg_loss = 1.23705\n","epoch no.0 train no.7400  loss = 1.17990 avg_loss = 1.22273\n","epoch no.0 train no.7400  loss = 1.17990 avg_loss = 1.22273\n","epoch no.0 train no.7410  loss = 1.55865 avg_loss = 1.22814\n","epoch no.0 train no.7410  loss = 1.55865 avg_loss = 1.22814\n","epoch no.0 train no.7420  loss = 2.30719 avg_loss = 1.26711\n","epoch no.0 train no.7420  loss = 2.30719 avg_loss = 1.26711\n","epoch no.0 train no.7430  loss = 0.89862 avg_loss = 1.26113\n","epoch no.0 train no.7430  loss = 0.89862 avg_loss = 1.26113\n","epoch no.0 train no.7440  loss = 1.28401 avg_loss = 1.25957\n","epoch no.0 train no.7440  loss = 1.28401 avg_loss = 1.25957\n","epoch no.0 train no.7450  loss = 0.93418 avg_loss = 1.25651\n","epoch no.0 train no.7450  loss = 0.93418 avg_loss = 1.25651\n","epoch no.0 train no.7460  loss = 1.13829 avg_loss = 1.25642\n","epoch no.0 train no.7460  loss = 1.13829 avg_loss = 1.25642\n","epoch no.0 train no.7470  loss = 0.99571 avg_loss = 1.23971\n","epoch no.0 train no.7470  loss = 0.99571 avg_loss = 1.23971\n","epoch no.0 train no.7480  loss = 0.53780 avg_loss = 1.22459\n","epoch no.0 train no.7480  loss = 0.53780 avg_loss = 1.22459\n","epoch no.0 train no.7490  loss = 1.09401 avg_loss = 1.22157\n","epoch no.0 train no.7490  loss = 1.09401 avg_loss = 1.22157\n","epoch no.0 train no.7500  loss = 0.82674 avg_loss = 1.22119\n","epoch no.0 train no.7500  loss = 0.82674 avg_loss = 1.22119\n","epoch no.0 train no.7510  loss = 1.27750 avg_loss = 1.22338\n","epoch no.0 train no.7510  loss = 1.27750 avg_loss = 1.22338\n","epoch no.0 train no.7520  loss = 1.15022 avg_loss = 1.21340\n","epoch no.0 train no.7520  loss = 1.15022 avg_loss = 1.21340\n","epoch no.0 train no.7530  loss = 1.35107 avg_loss = 1.21374\n","epoch no.0 train no.7530  loss = 1.35107 avg_loss = 1.21374\n","epoch no.0 train no.7540  loss = 1.20320 avg_loss = 1.21002\n","epoch no.0 train no.7540  loss = 1.20320 avg_loss = 1.21002\n","epoch no.0 train no.7550  loss = 1.55736 avg_loss = 1.22270\n","epoch no.0 train no.7550  loss = 1.55736 avg_loss = 1.22270\n","epoch no.0 train no.7560  loss = 0.84902 avg_loss = 1.23344\n","epoch no.0 train no.7560  loss = 0.84902 avg_loss = 1.23344\n","epoch no.0 train no.7570  loss = 0.59031 avg_loss = 1.22630\n","epoch no.0 train no.7570  loss = 0.59031 avg_loss = 1.22630\n","epoch no.0 train no.7580  loss = 0.61921 avg_loss = 1.21328\n","epoch no.0 train no.7580  loss = 0.61921 avg_loss = 1.21328\n","epoch no.0 train no.7590  loss = 1.49716 avg_loss = 1.22299\n","epoch no.0 train no.7590  loss = 1.49716 avg_loss = 1.22299\n","epoch no.0 train no.7600  loss = 0.55966 avg_loss = 1.21779\n","epoch no.0 train no.7600  loss = 0.55966 avg_loss = 1.21779\n","epoch no.0 train no.7610  loss = 1.79364 avg_loss = 1.24942\n","epoch no.0 train no.7610  loss = 1.79364 avg_loss = 1.24942\n","epoch no.0 train no.7620  loss = 0.97325 avg_loss = 1.26168\n","epoch no.0 train no.7620  loss = 0.97325 avg_loss = 1.26168\n","epoch no.0 train no.7630  loss = 1.12392 avg_loss = 1.25928\n","epoch no.0 train no.7630  loss = 1.12392 avg_loss = 1.25928\n","epoch no.0 train no.7640  loss = 1.30931 avg_loss = 1.26065\n","epoch no.0 train no.7640  loss = 1.30931 avg_loss = 1.26065\n","epoch no.0 train no.7650  loss = 1.29604 avg_loss = 1.25063\n","epoch no.0 train no.7650  loss = 1.29604 avg_loss = 1.25063\n","epoch no.0 train no.7660  loss = 1.20526 avg_loss = 1.25340\n","epoch no.0 train no.7660  loss = 1.20526 avg_loss = 1.25340\n","epoch no.0 train no.7670  loss = 0.69192 avg_loss = 1.24427\n","epoch no.0 train no.7670  loss = 0.69192 avg_loss = 1.24427\n","epoch no.0 train no.7680  loss = 1.05872 avg_loss = 1.22163\n","epoch no.0 train no.7680  loss = 1.05872 avg_loss = 1.22163\n","epoch no.0 train no.7690  loss = 0.80570 avg_loss = 1.21904\n","epoch no.0 train no.7690  loss = 0.80570 avg_loss = 1.21904\n","epoch no.0 train no.7700  loss = 1.17077 avg_loss = 1.22729\n","epoch no.0 train no.7700  loss = 1.17077 avg_loss = 1.22729\n","epoch no.0 train no.7710  loss = 1.13275 avg_loss = 1.22478\n","epoch no.0 train no.7710  loss = 1.13275 avg_loss = 1.22478\n","epoch no.0 train no.7720  loss = 1.07509 avg_loss = 1.22323\n","epoch no.0 train no.7720  loss = 1.07509 avg_loss = 1.22323\n","epoch no.0 train no.7730  loss = 1.00104 avg_loss = 1.20933\n","epoch no.0 train no.7730  loss = 1.00104 avg_loss = 1.20933\n","epoch no.0 train no.7740  loss = 1.47346 avg_loss = 1.20169\n","epoch no.0 train no.7740  loss = 1.47346 avg_loss = 1.20169\n","epoch no.0 train no.7750  loss = 1.92486 avg_loss = 1.20204\n","epoch no.0 train no.7750  loss = 1.92486 avg_loss = 1.20204\n","epoch no.0 train no.7760  loss = 1.05243 avg_loss = 1.20025\n","epoch no.0 train no.7760  loss = 1.05243 avg_loss = 1.20025\n","epoch no.0 train no.7770  loss = 0.86090 avg_loss = 1.21493\n","epoch no.0 train no.7770  loss = 0.86090 avg_loss = 1.21493\n","epoch no.0 train no.7780  loss = 1.02016 avg_loss = 1.23409\n","epoch no.0 train no.7780  loss = 1.02016 avg_loss = 1.23409\n","epoch no.0 train no.7790  loss = 0.72232 avg_loss = 1.22447\n","epoch no.0 train no.7790  loss = 0.72232 avg_loss = 1.22447\n","epoch no.0 train no.7800  loss = 2.02402 avg_loss = 1.23785\n","epoch no.0 train no.7800  loss = 2.02402 avg_loss = 1.23785\n","epoch no.0 train no.7810  loss = 1.31019 avg_loss = 1.25095\n","epoch no.0 train no.7810  loss = 1.31019 avg_loss = 1.25095\n","epoch no.0 train no.7820  loss = 0.82261 avg_loss = 1.23170\n","epoch no.0 train no.7820  loss = 0.82261 avg_loss = 1.23170\n","epoch no.0 train no.7830  loss = 1.09570 avg_loss = 1.21708\n","epoch no.0 train no.7830  loss = 1.09570 avg_loss = 1.21708\n","epoch no.0 train no.7840  loss = 0.76696 avg_loss = 1.21465\n","epoch no.0 train no.7840  loss = 0.76696 avg_loss = 1.21465\n","epoch no.0 train no.7850  loss = 1.35335 avg_loss = 1.22154\n","epoch no.0 train no.7850  loss = 1.35335 avg_loss = 1.22154\n","epoch no.0 train no.7860  loss = 1.90409 avg_loss = 1.22000\n","epoch no.0 train no.7860  loss = 1.90409 avg_loss = 1.22000\n","epoch no.0 train no.7870  loss = 2.02254 avg_loss = 1.21586\n","epoch no.0 train no.7870  loss = 2.02254 avg_loss = 1.21586\n","epoch no.0 train no.7880  loss = 0.85084 avg_loss = 1.20824\n","epoch no.0 train no.7880  loss = 0.85084 avg_loss = 1.20824\n","epoch no.0 train no.7890  loss = 1.28290 avg_loss = 1.20867\n","epoch no.0 train no.7890  loss = 1.28290 avg_loss = 1.20867\n","epoch no.0 train no.7900  loss = 1.55818 avg_loss = 1.20890\n","epoch no.0 train no.7900  loss = 1.55818 avg_loss = 1.20890\n","epoch no.0 train no.7910  loss = 0.90455 avg_loss = 1.20513\n","epoch no.0 train no.7910  loss = 0.90455 avg_loss = 1.20513\n","epoch no.0 train no.7920  loss = 1.65338 avg_loss = 1.21044\n","epoch no.0 train no.7920  loss = 1.65338 avg_loss = 1.21044\n","epoch no.0 train no.7930  loss = 1.06321 avg_loss = 1.22733\n","epoch no.0 train no.7930  loss = 1.06321 avg_loss = 1.22733\n","epoch no.0 train no.7940  loss = 1.96322 avg_loss = 1.23819\n","epoch no.0 train no.7940  loss = 1.96322 avg_loss = 1.23819\n","epoch no.0 train no.7950  loss = 0.67923 avg_loss = 1.23115\n","epoch no.0 train no.7950  loss = 0.67923 avg_loss = 1.23115\n","epoch no.0 train no.7960  loss = 0.97976 avg_loss = 1.23018\n","epoch no.0 train no.7960  loss = 0.97976 avg_loss = 1.23018\n","epoch no.0 train no.7970  loss = 1.84638 avg_loss = 1.21508\n","epoch no.0 train no.7970  loss = 1.84638 avg_loss = 1.21508\n","epoch no.0 train no.7980  loss = 0.75404 avg_loss = 1.19939\n","epoch no.0 train no.7980  loss = 0.75404 avg_loss = 1.19939\n","epoch no.0 train no.7990  loss = 1.06393 avg_loss = 1.19107\n","epoch no.0 train no.7990  loss = 1.06393 avg_loss = 1.19107\n","epoch no.0 train no.8000  loss = 2.09812 avg_loss = 1.21700\n","epoch no.0 train no.8000  loss = 2.09812 avg_loss = 1.21700\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁날카', '로워', ',', '▁b', 'itch', ',', '▁때로는', '▁부드럽', '지', '▁', '은', '▁척', '관', '▁다르지', '▁예상이', '▁되니', '▁움직임', '▁', '나', '▁더', '▁매', '겨', '▁', '렴', ',', '▁한', '▁방을', '▁더', '▁먹여', '▁드림', '▁', '▁', 'e', '▁', 'ry', '.', '▁c', 'co', '▁', 's', '▁', 'ut', '▁', '▁', '▁t', 'wn', '▁', '▁번', '▁할', '인데', '▁', '▁일', ',', '▁', 'h', '▁']\n","사랑 노랠 부를까  봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n","넘 날카로워, bitch, 때로는 부드럽지\n","\n","멋은 척관 다르지 예상이 되니 움직임\n","\n","점수나 더 매겨 보렴, 한 방을 더 먹여 드림\n","\n",".  comma\n","\n","ts\n","\n","iacon\n","\n","domin out\n","\n",", write dope 한 번뿐인데 합법적인 일,\n","\n","uh\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁날카', '로워', ',', '▁b', 'itch', ',', '▁때로는', '▁부드럽', '지', '▁', '은', '▁척', '관', '▁다르지', '▁예상이', '▁되니', '▁움직임', '▁', '나', '▁더', '▁매', '겨', '▁', '렴', ',', '▁한', '▁방을', '▁더', '▁먹여', '▁드림', '▁', '▁', 'e', '▁', 'ry', '.', '▁c', 'co', '▁', 's', '▁', 'ut', '▁', '▁', '▁t', 'wn', '▁', '▁번', '▁할', '인데', '▁', '▁일', ',', '▁', 'h', '▁']\n","사랑 노랠 부를까  봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n","넘 날카로워, bitch, 때로는 부드럽지\n","\n","멋은 척관 다르지 예상이 되니 움직임\n","\n","점수나 더 매겨 보렴, 한 방을 더 먹여 드림\n","\n",".  comma\n","\n","ts\n","\n","iacon\n","\n","domin out\n","\n",", write dope 한 번뿐인데 합법적인 일,\n","\n","uh\n","epoch no.0 train no.8010  loss = 1.15226 avg_loss = 1.21517\n","epoch no.0 train no.8010  loss = 1.15226 avg_loss = 1.21517\n","epoch no.0 train no.8020  loss = 0.95833 avg_loss = 1.21302\n","epoch no.0 train no.8020  loss = 0.95833 avg_loss = 1.21302\n","epoch no.0 train no.8030  loss = 1.05450 avg_loss = 1.21136\n","epoch no.0 train no.8030  loss = 1.05450 avg_loss = 1.21136\n","epoch no.0 train no.8040  loss = 1.18880 avg_loss = 1.19636\n","epoch no.0 train no.8040  loss = 1.18880 avg_loss = 1.19636\n","epoch no.0 train no.8050  loss = 0.81868 avg_loss = 1.19831\n","epoch no.0 train no.8050  loss = 0.81868 avg_loss = 1.19831\n","epoch no.0 train no.8060  loss = 1.58927 avg_loss = 1.19848\n","epoch no.0 train no.8060  loss = 1.58927 avg_loss = 1.19848\n","epoch no.0 train no.8070  loss = 1.25587 avg_loss = 1.18058\n","epoch no.0 train no.8070  loss = 1.25587 avg_loss = 1.18058\n","epoch no.0 train no.8080  loss = 1.94966 avg_loss = 1.19783\n","epoch no.0 train no.8080  loss = 1.94966 avg_loss = 1.19783\n","epoch no.0 train no.8090  loss = 1.27125 avg_loss = 1.22388\n","epoch no.0 train no.8090  loss = 1.27125 avg_loss = 1.22388\n","epoch no.0 train no.8100  loss = 1.69981 avg_loss = 1.22175\n","epoch no.0 train no.8100  loss = 1.69981 avg_loss = 1.22175\n","epoch no.0 train no.8110  loss = 1.13214 avg_loss = 1.22902\n","epoch no.0 train no.8110  loss = 1.13214 avg_loss = 1.22902\n","epoch no.0 train no.8120  loss = 1.31522 avg_loss = 1.22487\n","epoch no.0 train no.8120  loss = 1.31522 avg_loss = 1.22487\n","epoch no.0 train no.8130  loss = 1.16102 avg_loss = 1.23570\n","epoch no.0 train no.8130  loss = 1.16102 avg_loss = 1.23570\n","epoch no.0 train no.8140  loss = 1.55419 avg_loss = 1.21479\n","epoch no.0 train no.8140  loss = 1.55419 avg_loss = 1.21479\n","epoch no.0 train no.8150  loss = 1.52911 avg_loss = 1.21673\n","epoch no.0 train no.8150  loss = 1.52911 avg_loss = 1.21673\n","epoch no.0 train no.8160  loss = 0.92610 avg_loss = 1.20029\n","epoch no.0 train no.8160  loss = 0.92610 avg_loss = 1.20029\n","epoch no.0 train no.8170  loss = 1.60946 avg_loss = 1.22439\n","epoch no.0 train no.8170  loss = 1.60946 avg_loss = 1.22439\n","epoch no.0 train no.8180  loss = 1.09034 avg_loss = 1.22804\n","epoch no.0 train no.8180  loss = 1.09034 avg_loss = 1.22804\n","epoch no.0 train no.8190  loss = 0.83213 avg_loss = 1.21784\n","epoch no.0 train no.8190  loss = 0.83213 avg_loss = 1.21784\n","epoch no.0 train no.8200  loss = 1.05819 avg_loss = 1.23495\n","epoch no.0 train no.8200  loss = 1.05819 avg_loss = 1.23495\n","epoch no.0 train no.8210  loss = 1.50404 avg_loss = 1.26745\n","epoch no.0 train no.8210  loss = 1.50404 avg_loss = 1.26745\n","epoch no.0 train no.8220  loss = 1.84341 avg_loss = 1.29477\n","epoch no.0 train no.8220  loss = 1.84341 avg_loss = 1.29477\n","epoch no.0 train no.8230  loss = 0.63447 avg_loss = 1.27955\n","epoch no.0 train no.8230  loss = 0.63447 avg_loss = 1.27955\n","epoch no.0 train no.8240  loss = 1.63128 avg_loss = 1.27877\n","epoch no.0 train no.8240  loss = 1.63128 avg_loss = 1.27877\n","epoch no.0 train no.8250  loss = 1.34594 avg_loss = 1.27330\n","epoch no.0 train no.8250  loss = 1.34594 avg_loss = 1.27330\n","epoch no.0 train no.8260  loss = 1.83683 avg_loss = 1.27876\n","epoch no.0 train no.8260  loss = 1.83683 avg_loss = 1.27876\n","epoch no.0 train no.8270  loss = 1.22770 avg_loss = 1.26610\n","epoch no.0 train no.8270  loss = 1.22770 avg_loss = 1.26610\n","epoch no.0 train no.8280  loss = 0.59040 avg_loss = 1.26661\n","epoch no.0 train no.8280  loss = 0.59040 avg_loss = 1.26661\n","epoch no.0 train no.8290  loss = 1.27362 avg_loss = 1.26858\n","epoch no.0 train no.8290  loss = 1.27362 avg_loss = 1.26858\n","epoch no.0 train no.8300  loss = 1.34341 avg_loss = 1.25888\n","epoch no.0 train no.8300  loss = 1.34341 avg_loss = 1.25888\n","epoch no.0 train no.8310  loss = 1.45977 avg_loss = 1.26759\n","epoch no.0 train no.8310  loss = 1.45977 avg_loss = 1.26759\n","epoch no.0 train no.8320  loss = 1.57059 avg_loss = 1.27668\n","epoch no.0 train no.8320  loss = 1.57059 avg_loss = 1.27668\n","epoch no.0 train no.8330  loss = 0.85807 avg_loss = 1.25911\n","epoch no.0 train no.8330  loss = 0.85807 avg_loss = 1.25911\n","epoch no.0 train no.8340  loss = 1.48743 avg_loss = 1.25114\n","epoch no.0 train no.8340  loss = 1.48743 avg_loss = 1.25114\n","epoch no.0 train no.8350  loss = 0.72295 avg_loss = 1.23441\n","epoch no.0 train no.8350  loss = 0.72295 avg_loss = 1.23441\n","epoch no.0 train no.8360  loss = 2.00500 avg_loss = 1.23943\n","epoch no.0 train no.8360  loss = 2.00500 avg_loss = 1.23943\n","epoch no.0 train no.8370  loss = 0.87149 avg_loss = 1.24162\n","epoch no.0 train no.8370  loss = 0.87149 avg_loss = 1.24162\n","epoch no.0 train no.8380  loss = 0.98330 avg_loss = 1.23850\n","epoch no.0 train no.8380  loss = 0.98330 avg_loss = 1.23850\n","epoch no.0 train no.8390  loss = 2.05473 avg_loss = 1.23985\n","epoch no.0 train no.8390  loss = 2.05473 avg_loss = 1.23985\n","epoch no.0 train no.8400  loss = 1.27468 avg_loss = 1.23327\n","epoch no.0 train no.8400  loss = 1.27468 avg_loss = 1.23327\n","epoch no.0 train no.8410  loss = 1.22667 avg_loss = 1.21778\n","epoch no.0 train no.8410  loss = 1.22667 avg_loss = 1.21778\n","epoch no.0 train no.8420  loss = 0.73213 avg_loss = 1.21250\n","epoch no.0 train no.8420  loss = 0.73213 avg_loss = 1.21250\n","epoch no.0 train no.8430  loss = 0.67293 avg_loss = 1.21271\n","epoch no.0 train no.8430  loss = 0.67293 avg_loss = 1.21271\n","epoch no.0 train no.8440  loss = 1.57707 avg_loss = 1.22035\n","epoch no.0 train no.8440  loss = 1.57707 avg_loss = 1.22035\n","epoch no.0 train no.8450  loss = 0.88529 avg_loss = 1.20620\n","epoch no.0 train no.8450  loss = 0.88529 avg_loss = 1.20620\n","epoch no.0 train no.8460  loss = 1.11600 avg_loss = 1.21223\n","epoch no.0 train no.8460  loss = 1.11600 avg_loss = 1.21223\n","epoch no.0 train no.8470  loss = 1.14931 avg_loss = 1.21353\n","epoch no.0 train no.8470  loss = 1.14931 avg_loss = 1.21353\n","epoch no.0 train no.8480  loss = 1.91987 avg_loss = 1.22605\n","epoch no.0 train no.8480  loss = 1.91987 avg_loss = 1.22605\n","epoch no.0 train no.8490  loss = 0.66564 avg_loss = 1.23269\n","epoch no.0 train no.8490  loss = 0.66564 avg_loss = 1.23269\n","epoch no.0 train no.8500  loss = 1.58592 avg_loss = 1.23532\n","epoch no.0 train no.8500  loss = 1.58592 avg_loss = 1.23532\n","epoch no.0 train no.8510  loss = 0.51496 avg_loss = 1.23073\n","epoch no.0 train no.8510  loss = 0.51496 avg_loss = 1.23073\n","epoch no.0 train no.8520  loss = 1.53769 avg_loss = 1.22556\n","epoch no.0 train no.8520  loss = 1.53769 avg_loss = 1.22556\n","epoch no.0 train no.8530  loss = 1.54503 avg_loss = 1.21880\n","epoch no.0 train no.8530  loss = 1.54503 avg_loss = 1.21880\n","epoch no.0 train no.8540  loss = 0.88905 avg_loss = 1.20565\n","epoch no.0 train no.8540  loss = 0.88905 avg_loss = 1.20565\n","epoch no.0 train no.8550  loss = 1.11720 avg_loss = 1.20043\n","epoch no.0 train no.8550  loss = 1.11720 avg_loss = 1.20043\n","epoch no.0 train no.8560  loss = 0.52098 avg_loss = 1.18698\n","epoch no.0 train no.8560  loss = 0.52098 avg_loss = 1.18698\n","epoch no.0 train no.8570  loss = 1.03320 avg_loss = 1.19428\n","epoch no.0 train no.8570  loss = 1.03320 avg_loss = 1.19428\n","epoch no.0 train no.8580  loss = 1.15764 avg_loss = 1.21122\n","epoch no.0 train no.8580  loss = 1.15764 avg_loss = 1.21122\n","epoch no.0 train no.8590  loss = 0.79984 avg_loss = 1.21324\n","epoch no.0 train no.8590  loss = 0.79984 avg_loss = 1.21324\n","epoch no.0 train no.8600  loss = 1.00460 avg_loss = 1.21188\n","epoch no.0 train no.8600  loss = 1.00460 avg_loss = 1.21188\n","epoch no.0 train no.8610  loss = 1.61682 avg_loss = 1.20758\n","epoch no.0 train no.8610  loss = 1.61682 avg_loss = 1.20758\n","epoch no.0 train no.8620  loss = 1.08256 avg_loss = 1.22360\n","epoch no.0 train no.8620  loss = 1.08256 avg_loss = 1.22360\n","epoch no.0 train no.8630  loss = 1.22844 avg_loss = 1.23210\n","epoch no.0 train no.8630  loss = 1.22844 avg_loss = 1.23210\n","epoch no.0 train no.8640  loss = 1.68884 avg_loss = 1.24139\n","epoch no.0 train no.8640  loss = 1.68884 avg_loss = 1.24139\n","epoch no.0 train no.8650  loss = 1.39101 avg_loss = 1.23778\n","epoch no.0 train no.8650  loss = 1.39101 avg_loss = 1.23778\n","epoch no.0 train no.8660  loss = 0.95131 avg_loss = 1.22829\n","epoch no.0 train no.8660  loss = 0.95131 avg_loss = 1.22829\n","epoch no.0 train no.8670  loss = 2.14974 avg_loss = 1.22616\n","epoch no.0 train no.8670  loss = 2.14974 avg_loss = 1.22616\n","epoch no.0 train no.8680  loss = 0.95442 avg_loss = 1.22600\n","epoch no.0 train no.8680  loss = 0.95442 avg_loss = 1.22600\n","epoch no.0 train no.8690  loss = 1.57665 avg_loss = 1.21615\n","epoch no.0 train no.8690  loss = 1.57665 avg_loss = 1.21615\n","epoch no.0 train no.8700  loss = 2.19497 avg_loss = 1.23618\n","epoch no.0 train no.8700  loss = 2.19497 avg_loss = 1.23618\n","epoch no.0 train no.8710  loss = 1.19881 avg_loss = 1.22495\n","epoch no.0 train no.8710  loss = 1.19881 avg_loss = 1.22495\n","epoch no.0 train no.8720  loss = 1.62674 avg_loss = 1.24520\n","epoch no.0 train no.8720  loss = 1.62674 avg_loss = 1.24520\n","epoch no.0 train no.8730  loss = 1.05888 avg_loss = 1.23222\n","epoch no.0 train no.8730  loss = 1.05888 avg_loss = 1.23222\n","epoch no.0 train no.8740  loss = 0.95503 avg_loss = 1.22344\n","epoch no.0 train no.8740  loss = 0.95503 avg_loss = 1.22344\n","epoch no.0 train no.8750  loss = 1.08809 avg_loss = 1.22433\n","epoch no.0 train no.8750  loss = 1.08809 avg_loss = 1.22433\n","epoch no.0 train no.8760  loss = 0.63911 avg_loss = 1.21812\n","epoch no.0 train no.8760  loss = 0.63911 avg_loss = 1.21812\n","epoch no.0 train no.8770  loss = 1.68993 avg_loss = 1.22444\n","epoch no.0 train no.8770  loss = 1.68993 avg_loss = 1.22444\n","epoch no.0 train no.8780  loss = 1.35687 avg_loss = 1.22235\n","epoch no.0 train no.8780  loss = 1.35687 avg_loss = 1.22235\n","epoch no.0 train no.8790  loss = 0.93602 avg_loss = 1.22979\n","epoch no.0 train no.8790  loss = 0.93602 avg_loss = 1.22979\n","epoch no.0 train no.8800  loss = 0.78522 avg_loss = 1.22148\n","epoch no.0 train no.8800  loss = 0.78522 avg_loss = 1.22148\n","epoch no.0 train no.8810  loss = 0.64822 avg_loss = 1.21198\n","epoch no.0 train no.8810  loss = 0.64822 avg_loss = 1.21198\n","epoch no.0 train no.8820  loss = 0.71625 avg_loss = 1.21499\n","epoch no.0 train no.8820  loss = 0.71625 avg_loss = 1.21499\n","epoch no.0 train no.8830  loss = 0.86345 avg_loss = 1.20481\n","epoch no.0 train no.8830  loss = 0.86345 avg_loss = 1.20481\n","epoch no.0 train no.8840  loss = 1.32186 avg_loss = 1.21162\n","epoch no.0 train no.8840  loss = 1.32186 avg_loss = 1.21162\n","epoch no.0 train no.8850  loss = 1.13522 avg_loss = 1.20381\n","epoch no.0 train no.8850  loss = 1.13522 avg_loss = 1.20381\n","epoch no.0 train no.8860  loss = 1.55117 avg_loss = 1.21783\n","epoch no.0 train no.8860  loss = 1.55117 avg_loss = 1.21783\n","epoch no.0 train no.8870  loss = 0.81916 avg_loss = 1.21402\n","epoch no.0 train no.8870  loss = 0.81916 avg_loss = 1.21402\n","epoch no.0 train no.8880  loss = 1.18066 avg_loss = 1.21932\n","epoch no.0 train no.8880  loss = 1.18066 avg_loss = 1.21932\n","epoch no.0 train no.8890  loss = 1.21539 avg_loss = 1.20459\n","epoch no.0 train no.8890  loss = 1.21539 avg_loss = 1.20459\n","epoch no.0 train no.8900  loss = 0.79563 avg_loss = 1.21748\n","epoch no.0 train no.8900  loss = 0.79563 avg_loss = 1.21748\n","epoch no.0 train no.8910  loss = 1.41141 avg_loss = 1.21886\n","epoch no.0 train no.8910  loss = 1.41141 avg_loss = 1.21886\n","epoch no.0 train no.8920  loss = 0.64323 avg_loss = 1.20741\n","epoch no.0 train no.8920  loss = 0.64323 avg_loss = 1.20741\n","epoch no.0 train no.8930  loss = 1.25162 avg_loss = 1.20894\n","epoch no.0 train no.8930  loss = 1.25162 avg_loss = 1.20894\n","epoch no.0 train no.8940  loss = 1.58663 avg_loss = 1.20867\n","epoch no.0 train no.8940  loss = 1.58663 avg_loss = 1.20867\n","epoch no.0 train no.8950  loss = 1.41111 avg_loss = 1.21201\n","epoch no.0 train no.8950  loss = 1.41111 avg_loss = 1.21201\n","epoch no.0 train no.8960  loss = 1.00145 avg_loss = 1.19392\n","epoch no.0 train no.8960  loss = 1.00145 avg_loss = 1.19392\n","epoch no.0 train no.8970  loss = 1.94454 avg_loss = 1.21394\n","epoch no.0 train no.8970  loss = 1.94454 avg_loss = 1.21394\n","epoch no.0 train no.8980  loss = 1.41099 avg_loss = 1.22319\n","epoch no.0 train no.8980  loss = 1.41099 avg_loss = 1.22319\n","epoch no.0 train no.8990  loss = 1.15497 avg_loss = 1.21493\n","epoch no.0 train no.8990  loss = 1.15497 avg_loss = 1.21493\n","epoch no.0 train no.9000  loss = 1.22677 avg_loss = 1.20479\n","epoch no.0 train no.9000  loss = 1.22677 avg_loss = 1.20479\n","to_tokens: ['▁', '해요', '▁', '▁', '하고', '▁있어', '▁', '▁', '▁', '▁사랑해', '▁', '▁사랑해', '▁사랑해', '▁', '▁사랑해', '▁사랑해', '▁', '▁사랑해', '▁사랑해', '▁사랑해', '▁사랑해', '▁사랑해', '▁', '▁사랑해', '▁', '▁', '요', '뻐', '▁', '만을', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁']\n","사랑하고 있어  사랑받고 싶어 널\n","\n","사랑해\n","\n","사랑해 사랑해 사랑해\n","\n","사랑해\n","\n","사랑해\n","\n","사랑해 사랑해 사랑해\n","\n","사랑해 사랑해 사랑해 사랑해  사랑해 사랑해 사랑해 사랑해 말해 요뻐요  나뻐요 나뻐요 나뻐요 나뻐요 나뻐요 나뻐요  나뻐요 나뻐요 나뻐요 나뻐요 나뻐요 나뻐요 나뻐요\n","\n","나뻐요 나뻐요  나뻐요 나뻐요 나뻐요 나뻐요\n","\n","나뻐요  나뻐요  나뻐요 나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요</s>\n","to_tokens: ['▁', '해요', '▁', '▁', '하고', '▁있어', '▁', '▁', '▁', '▁사랑해', '▁', '▁사랑해', '▁사랑해', '▁', '▁사랑해', '▁사랑해', '▁', '▁사랑해', '▁사랑해', '▁사랑해', '▁사랑해', '▁사랑해', '▁', '▁사랑해', '▁', '▁', '요', '뻐', '▁', '만을', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁나', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁', '뻐', '요', '▁']\n","사랑하고 있어  사랑받고 싶어 널\n","\n","사랑해\n","\n","사랑해 사랑해 사랑해\n","\n","사랑해\n","\n","사랑해\n","\n","사랑해 사랑해 사랑해\n","\n","사랑해 사랑해 사랑해 사랑해  사랑해 사랑해 사랑해 사랑해 말해 요뻐요  나뻐요 나뻐요 나뻐요 나뻐요 나뻐요 나뻐요  나뻐요 나뻐요 나뻐요 나뻐요 나뻐요 나뻐요 나뻐요\n","\n","나뻐요 나뻐요  나뻐요 나뻐요 나뻐요 나뻐요\n","\n","나뻐요  나뻐요  나뻐요 나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요 나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요\n","\n","나뻐요</s>\n","epoch no.0 train no.9010  loss = 0.92489 avg_loss = 1.18834\n","epoch no.0 train no.9010  loss = 0.92489 avg_loss = 1.18834\n","epoch no.0 train no.9020  loss = 1.23022 avg_loss = 1.19242\n","epoch no.0 train no.9020  loss = 1.23022 avg_loss = 1.19242\n","epoch no.0 train no.9030  loss = 1.51244 avg_loss = 1.18227\n","epoch no.0 train no.9030  loss = 1.51244 avg_loss = 1.18227\n","epoch no.0 train no.9040  loss = 0.72038 avg_loss = 1.17973\n","epoch no.0 train no.9040  loss = 0.72038 avg_loss = 1.17973\n","epoch no.0 train no.9050  loss = 1.11899 avg_loss = 1.16286\n","epoch no.0 train no.9050  loss = 1.11899 avg_loss = 1.16286\n","epoch no.0 train no.9060  loss = 1.70015 avg_loss = 1.17645\n","epoch no.0 train no.9060  loss = 1.70015 avg_loss = 1.17645\n","epoch no.0 train no.9070  loss = 1.68654 avg_loss = 1.18580\n","epoch no.0 train no.9070  loss = 1.68654 avg_loss = 1.18580\n","epoch no.0 train no.9080  loss = 1.58227 avg_loss = 1.18578\n","epoch no.0 train no.9080  loss = 1.58227 avg_loss = 1.18578\n","epoch no.0 train no.9090  loss = 1.31565 avg_loss = 1.18065\n","epoch no.0 train no.9090  loss = 1.31565 avg_loss = 1.18065\n","epoch no.0 train no.9100  loss = 1.18638 avg_loss = 1.17564\n","epoch no.0 train no.9100  loss = 1.18638 avg_loss = 1.17564\n","epoch no.0 train no.9110  loss = 0.36312 avg_loss = 1.17129\n","epoch no.0 train no.9110  loss = 0.36312 avg_loss = 1.17129\n","epoch no.0 train no.9120  loss = 1.34040 avg_loss = 1.17696\n","epoch no.0 train no.9120  loss = 1.34040 avg_loss = 1.17696\n","epoch no.0 train no.9130  loss = 0.38464 avg_loss = 1.18262\n","epoch no.0 train no.9130  loss = 0.38464 avg_loss = 1.18262\n","epoch no.0 train no.9140  loss = 0.65904 avg_loss = 1.18642\n","epoch no.0 train no.9140  loss = 0.65904 avg_loss = 1.18642\n","epoch no.0 train no.9150  loss = 0.98503 avg_loss = 1.18633\n","epoch no.0 train no.9150  loss = 0.98503 avg_loss = 1.18633\n","epoch no.0 train no.9160  loss = 1.50580 avg_loss = 1.19453\n","epoch no.0 train no.9160  loss = 1.50580 avg_loss = 1.19453\n","epoch no.0 train no.9170  loss = 1.08234 avg_loss = 1.18529\n","epoch no.0 train no.9170  loss = 1.08234 avg_loss = 1.18529\n","epoch no.0 train no.9180  loss = 0.94802 avg_loss = 1.18397\n","epoch no.0 train no.9180  loss = 0.94802 avg_loss = 1.18397\n","epoch no.0 train no.9190  loss = 0.63787 avg_loss = 1.18216\n","epoch no.0 train no.9190  loss = 0.63787 avg_loss = 1.18216\n","epoch no.0 train no.9200  loss = 1.36838 avg_loss = 1.18895\n","epoch no.0 train no.9200  loss = 1.36838 avg_loss = 1.18895\n","epoch no.0 train no.9210  loss = 1.82630 avg_loss = 1.20014\n","epoch no.0 train no.9210  loss = 1.82630 avg_loss = 1.20014\n","epoch no.0 train no.9220  loss = 1.86380 avg_loss = 1.20393\n","epoch no.0 train no.9220  loss = 1.86380 avg_loss = 1.20393\n","epoch no.0 train no.9230  loss = 1.11221 avg_loss = 1.20721\n","epoch no.0 train no.9230  loss = 1.11221 avg_loss = 1.20721\n","epoch no.0 train no.9240  loss = 1.85977 avg_loss = 1.22925\n","epoch no.0 train no.9240  loss = 1.85977 avg_loss = 1.22925\n","epoch no.0 train no.9250  loss = 1.03788 avg_loss = 1.23186\n","epoch no.0 train no.9250  loss = 1.03788 avg_loss = 1.23186\n","epoch no.0 train no.9260  loss = 1.15068 avg_loss = 1.22802\n","epoch no.0 train no.9260  loss = 1.15068 avg_loss = 1.22802\n","epoch no.0 train no.9270  loss = 1.38544 avg_loss = 1.22877\n","epoch no.0 train no.9270  loss = 1.38544 avg_loss = 1.22877\n","epoch no.0 train no.9280  loss = 0.85106 avg_loss = 1.23154\n","epoch no.0 train no.9280  loss = 0.85106 avg_loss = 1.23154\n","epoch no.0 train no.9290  loss = 1.84865 avg_loss = 1.24141\n","epoch no.0 train no.9290  loss = 1.84865 avg_loss = 1.24141\n","epoch no.0 train no.9300  loss = 1.69070 avg_loss = 1.22428\n","epoch no.0 train no.9300  loss = 1.69070 avg_loss = 1.22428\n","epoch no.0 train no.9310  loss = 0.79688 avg_loss = 1.21807\n","epoch no.0 train no.9310  loss = 0.79688 avg_loss = 1.21807\n","epoch no.0 train no.9320  loss = 1.32623 avg_loss = 1.23315\n","epoch no.0 train no.9320  loss = 1.32623 avg_loss = 1.23315\n","epoch no.0 train no.9330  loss = 1.19848 avg_loss = 1.23349\n","epoch no.0 train no.9330  loss = 1.19848 avg_loss = 1.23349\n","epoch no.0 train no.9340  loss = 0.91808 avg_loss = 1.21914\n","epoch no.0 train no.9340  loss = 0.91808 avg_loss = 1.21914\n","epoch no.0 train no.9350  loss = 1.32365 avg_loss = 1.22331\n","epoch no.0 train no.9350  loss = 1.32365 avg_loss = 1.22331\n","epoch no.0 train no.9360  loss = 0.89914 avg_loss = 1.21541\n","epoch no.0 train no.9360  loss = 0.89914 avg_loss = 1.21541\n","epoch no.0 train no.9370  loss = 1.38826 avg_loss = 1.22624\n","epoch no.0 train no.9370  loss = 1.38826 avg_loss = 1.22624\n","epoch no.0 train no.9380  loss = 0.87737 avg_loss = 1.21836\n","epoch no.0 train no.9380  loss = 0.87737 avg_loss = 1.21836\n","epoch no.0 train no.9390  loss = 0.96829 avg_loss = 1.22289\n","epoch no.0 train no.9390  loss = 0.96829 avg_loss = 1.22289\n","epoch no.0 train no.9400  loss = 0.68628 avg_loss = 1.21655\n","epoch no.0 train no.9400  loss = 0.68628 avg_loss = 1.21655\n","epoch no.0 train no.9410  loss = 1.68117 avg_loss = 1.22346\n","epoch no.0 train no.9410  loss = 1.68117 avg_loss = 1.22346\n","epoch no.0 train no.9420  loss = 0.72845 avg_loss = 1.20271\n","epoch no.0 train no.9420  loss = 0.72845 avg_loss = 1.20271\n","epoch no.0 train no.9430  loss = 0.76175 avg_loss = 1.20931\n","epoch no.0 train no.9430  loss = 0.76175 avg_loss = 1.20931\n","epoch no.0 train no.9440  loss = 0.94327 avg_loss = 1.19641\n","epoch no.0 train no.9440  loss = 0.94327 avg_loss = 1.19641\n","epoch no.0 train no.9450  loss = 1.08654 avg_loss = 1.20639\n","epoch no.0 train no.9450  loss = 1.08654 avg_loss = 1.20639\n","epoch no.0 train no.9460  loss = 0.95909 avg_loss = 1.20046\n","epoch no.0 train no.9460  loss = 0.95909 avg_loss = 1.20046\n","epoch no.0 train no.9470  loss = 1.52235 avg_loss = 1.20350\n","epoch no.0 train no.9470  loss = 1.52235 avg_loss = 1.20350\n","epoch no.0 train no.9480  loss = 1.32993 avg_loss = 1.19993\n","epoch no.0 train no.9480  loss = 1.32993 avg_loss = 1.19993\n","epoch no.0 train no.9490  loss = 0.88394 avg_loss = 1.20510\n","epoch no.0 train no.9490  loss = 0.88394 avg_loss = 1.20510\n","epoch no.0 train no.9500  loss = 1.79344 avg_loss = 1.22272\n","epoch no.0 train no.9500  loss = 1.79344 avg_loss = 1.22272\n","epoch no.0 train no.9510  loss = 0.73525 avg_loss = 1.22798\n","epoch no.0 train no.9510  loss = 0.73525 avg_loss = 1.22798\n","epoch no.0 train no.9520  loss = 1.16267 avg_loss = 1.22096\n","epoch no.0 train no.9520  loss = 1.16267 avg_loss = 1.22096\n","epoch no.0 train no.9530  loss = 1.29632 avg_loss = 1.21063\n","epoch no.0 train no.9530  loss = 1.29632 avg_loss = 1.21063\n","epoch no.0 train no.9540  loss = 1.23292 avg_loss = 1.18827\n","epoch no.0 train no.9540  loss = 1.23292 avg_loss = 1.18827\n","epoch no.0 train no.9550  loss = 0.93278 avg_loss = 1.17488\n","epoch no.0 train no.9550  loss = 0.93278 avg_loss = 1.17488\n","epoch no.0 train no.9560  loss = 1.51033 avg_loss = 1.17892\n","epoch no.0 train no.9560  loss = 1.51033 avg_loss = 1.17892\n","epoch no.0 train no.9570  loss = 1.03780 avg_loss = 1.19678\n","epoch no.0 train no.9570  loss = 1.03780 avg_loss = 1.19678\n","epoch no.0 train no.9580  loss = 0.84099 avg_loss = 1.19148\n","epoch no.0 train no.9580  loss = 0.84099 avg_loss = 1.19148\n","epoch no.0 train no.9590  loss = 1.28894 avg_loss = 1.18613\n","epoch no.0 train no.9590  loss = 1.28894 avg_loss = 1.18613\n","epoch no.0 train no.9600  loss = 1.28071 avg_loss = 1.18312\n","epoch no.0 train no.9600  loss = 1.28071 avg_loss = 1.18312\n","epoch no.0 train no.9610  loss = 2.05191 avg_loss = 1.19494\n","epoch no.0 train no.9610  loss = 2.05191 avg_loss = 1.19494\n","epoch no.0 train no.9620  loss = 1.46335 avg_loss = 1.19336\n","epoch no.0 train no.9620  loss = 1.46335 avg_loss = 1.19336\n","epoch no.0 train no.9630  loss = 1.75603 avg_loss = 1.20922\n","epoch no.0 train no.9630  loss = 1.75603 avg_loss = 1.20922\n","epoch no.0 train no.9640  loss = 1.41733 avg_loss = 1.19818\n","epoch no.0 train no.9640  loss = 1.41733 avg_loss = 1.19818\n","epoch no.0 train no.9650  loss = 1.52215 avg_loss = 1.20777\n","epoch no.0 train no.9650  loss = 1.52215 avg_loss = 1.20777\n","epoch no.0 train no.9660  loss = 1.03687 avg_loss = 1.18820\n","epoch no.0 train no.9660  loss = 1.03687 avg_loss = 1.18820\n","epoch no.0 train no.9670  loss = 1.03406 avg_loss = 1.17952\n","epoch no.0 train no.9670  loss = 1.03406 avg_loss = 1.17952\n","epoch no.0 train no.9680  loss = 1.92331 avg_loss = 1.19536\n","epoch no.0 train no.9680  loss = 1.92331 avg_loss = 1.19536\n","epoch no.0 train no.9690  loss = 1.48537 avg_loss = 1.19943\n","epoch no.0 train no.9690  loss = 1.48537 avg_loss = 1.19943\n","epoch no.0 train no.9700  loss = 1.44619 avg_loss = 1.20250\n","epoch no.0 train no.9700  loss = 1.44619 avg_loss = 1.20250\n","epoch no.0 train no.9710  loss = 1.05357 avg_loss = 1.20324\n","epoch no.0 train no.9710  loss = 1.05357 avg_loss = 1.20324\n","epoch no.0 train no.9720  loss = 0.73279 avg_loss = 1.17665\n","epoch no.0 train no.9720  loss = 0.73279 avg_loss = 1.17665\n","epoch no.0 train no.9730  loss = 0.71647 avg_loss = 1.17334\n","epoch no.0 train no.9730  loss = 0.71647 avg_loss = 1.17334\n","epoch no.0 train no.9740  loss = 1.71366 avg_loss = 1.17444\n","epoch no.0 train no.9740  loss = 1.71366 avg_loss = 1.17444\n","epoch no.0 train no.9750  loss = 1.76222 avg_loss = 1.17807\n","epoch no.0 train no.9750  loss = 1.76222 avg_loss = 1.17807\n","epoch no.0 train no.9760  loss = 0.61109 avg_loss = 1.15819\n","epoch no.0 train no.9760  loss = 0.61109 avg_loss = 1.15819\n","epoch no.0 train no.9770  loss = 0.39551 avg_loss = 1.16720\n","epoch no.0 train no.9770  loss = 0.39551 avg_loss = 1.16720\n","epoch no.0 train no.9780  loss = 1.40221 avg_loss = 1.17974\n","epoch no.0 train no.9780  loss = 1.40221 avg_loss = 1.17974\n","epoch no.0 train no.9790  loss = 1.22251 avg_loss = 1.15546\n","epoch no.0 train no.9790  loss = 1.22251 avg_loss = 1.15546\n","epoch no.0 train no.9800  loss = 1.28703 avg_loss = 1.17227\n","epoch no.0 train no.9800  loss = 1.28703 avg_loss = 1.17227\n","epoch no.0 train no.9810  loss = 0.21213 avg_loss = 1.16112\n","epoch no.0 train no.9810  loss = 0.21213 avg_loss = 1.16112\n","epoch no.0 train no.9820  loss = 0.81745 avg_loss = 1.14793\n","epoch no.0 train no.9820  loss = 0.81745 avg_loss = 1.14793\n","epoch no.0 train no.9830  loss = 0.98666 avg_loss = 1.15925\n","epoch no.0 train no.9830  loss = 0.98666 avg_loss = 1.15925\n","epoch no.0 train no.9840  loss = 1.33440 avg_loss = 1.14899\n","epoch no.0 train no.9840  loss = 1.33440 avg_loss = 1.14899\n","epoch no.0 train no.9850  loss = 1.52103 avg_loss = 1.14844\n","epoch no.0 train no.9850  loss = 1.52103 avg_loss = 1.14844\n","epoch no.0 train no.9860  loss = 1.23679 avg_loss = 1.13871\n","epoch no.0 train no.9860  loss = 1.23679 avg_loss = 1.13871\n","epoch no.0 train no.9870  loss = 1.40553 avg_loss = 1.13523\n","epoch no.0 train no.9870  loss = 1.40553 avg_loss = 1.13523\n","epoch no.0 train no.9880  loss = 1.35890 avg_loss = 1.14210\n","epoch no.0 train no.9880  loss = 1.35890 avg_loss = 1.14210\n","epoch no.0 train no.9890  loss = 0.88458 avg_loss = 1.13357\n","epoch no.0 train no.9890  loss = 0.88458 avg_loss = 1.13357\n","epoch no.0 train no.9900  loss = 0.73069 avg_loss = 1.11621\n","epoch no.0 train no.9900  loss = 0.73069 avg_loss = 1.11621\n","epoch no.0 train no.9910  loss = 1.52128 avg_loss = 1.12081\n","epoch no.0 train no.9910  loss = 1.52128 avg_loss = 1.12081\n","epoch no.0 train no.9920  loss = 1.85977 avg_loss = 1.12617\n","epoch no.0 train no.9920  loss = 1.85977 avg_loss = 1.12617\n","epoch no.0 train no.9930  loss = 0.88625 avg_loss = 1.15547\n","epoch no.0 train no.9930  loss = 0.88625 avg_loss = 1.15547\n","epoch no.0 train no.9940  loss = 0.67957 avg_loss = 1.15271\n","epoch no.0 train no.9940  loss = 0.67957 avg_loss = 1.15271\n","epoch no.0 train no.9950  loss = 1.00914 avg_loss = 1.15641\n","epoch no.0 train no.9950  loss = 1.00914 avg_loss = 1.15641\n","epoch no.0 train no.9960  loss = 0.47953 avg_loss = 1.15775\n","epoch no.0 train no.9960  loss = 0.47953 avg_loss = 1.15775\n","epoch no.0 train no.9970  loss = 1.20060 avg_loss = 1.15539\n","epoch no.0 train no.9970  loss = 1.20060 avg_loss = 1.15539\n","epoch no.0 train no.9980  loss = 1.18043 avg_loss = 1.15509\n","epoch no.0 train no.9980  loss = 1.18043 avg_loss = 1.15509\n","epoch no.0 train no.9990  loss = 0.61014 avg_loss = 1.16272\n","epoch no.0 train no.9990  loss = 0.61014 avg_loss = 1.16272\n","epoch no.0 train no.10000  loss = 0.64065 avg_loss = 1.14147\n","epoch no.0 train no.10000  loss = 0.64065 avg_loss = 1.14147\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁', ',', '▁워', '밍', '업', '▁', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁D', 'hat', '▁', '▁f', 'uck', '?', '▁', '▁be', '▁연락', '▁차', '▁하지', '▁', '▁', '한', '▁경쟁', '쟁', '▁속에', '▁지', '▁년', '째', '▁한', '▁', '▁하러', '▁', '▁전', 'ir', 'et', 'ar', ',', 'ug', 'ht', 'ag', '▁', '▁s', 'ing', '▁', 'ive', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n",", What the fuck ?\n","\n","이라며\n","\n","시기 좀 타네\n","\n","괜한  언쟁으로 몇 곡 못 내서\n","\n","음악 하러 ,igargon hashtag and sing\n","\n","alone\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁', ',', '▁워', '밍', '업', '▁', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁D', 'hat', '▁', '▁f', 'uck', '?', '▁', '▁be', '▁연락', '▁차', '▁하지', '▁', '▁', '한', '▁경쟁', '쟁', '▁속에', '▁지', '▁년', '째', '▁한', '▁', '▁하러', '▁', '▁전', 'ir', 'et', 'ar', ',', 'ug', 'ht', 'ag', '▁', '▁s', 'ing', '▁', 'ive', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n",", What the fuck ?\n","\n","이라며\n","\n","시기 좀 타네\n","\n","괜한  언쟁으로 몇 곡 못 내서\n","\n","음악 하러 ,igargon hashtag and sing\n","\n","alone\n","epoch no.0 train no.10010  loss = 1.34304 avg_loss = 1.15074\n","epoch no.0 train no.10010  loss = 1.34304 avg_loss = 1.15074\n","epoch no.0 train no.10020  loss = 1.08747 avg_loss = 1.15771\n","epoch no.0 train no.10020  loss = 1.08747 avg_loss = 1.15771\n","epoch no.0 train no.10030  loss = 1.50979 avg_loss = 1.17367\n","epoch no.0 train no.10030  loss = 1.50979 avg_loss = 1.17367\n","epoch no.0 train no.10040  loss = 1.45372 avg_loss = 1.18729\n","epoch no.0 train no.10040  loss = 1.45372 avg_loss = 1.18729\n","epoch no.0 train no.10050  loss = 1.24012 avg_loss = 1.18120\n","epoch no.0 train no.10050  loss = 1.24012 avg_loss = 1.18120\n","epoch no.0 train no.10060  loss = 0.83764 avg_loss = 1.17559\n","epoch no.0 train no.10060  loss = 0.83764 avg_loss = 1.17559\n","epoch no.0 train no.10070  loss = 1.90534 avg_loss = 1.19324\n","epoch no.0 train no.10070  loss = 1.90534 avg_loss = 1.19324\n","epoch no.0 train no.10080  loss = 1.53081 avg_loss = 1.20011\n","epoch no.0 train no.10080  loss = 1.53081 avg_loss = 1.20011\n","epoch no.0 train no.10090  loss = 0.70527 avg_loss = 1.19160\n","epoch no.0 train no.10090  loss = 0.70527 avg_loss = 1.19160\n","epoch no.0 train no.10100  loss = 1.15904 avg_loss = 1.19310\n","epoch no.0 train no.10100  loss = 1.15904 avg_loss = 1.19310\n","epoch no.0 train no.10110  loss = 0.77984 avg_loss = 1.18900\n","epoch no.0 train no.10110  loss = 0.77984 avg_loss = 1.18900\n","epoch no.0 train no.10120  loss = 1.02140 avg_loss = 1.18593\n","epoch no.0 train no.10120  loss = 1.02140 avg_loss = 1.18593\n","epoch no.0 train no.10130  loss = 1.01534 avg_loss = 1.19735\n","epoch no.0 train no.10130  loss = 1.01534 avg_loss = 1.19735\n","epoch no.0 train no.10140  loss = 0.70613 avg_loss = 1.19738\n","epoch no.0 train no.10140  loss = 0.70613 avg_loss = 1.19738\n","epoch no.0 train no.10150  loss = 1.14238 avg_loss = 1.18079\n","epoch no.0 train no.10150  loss = 1.14238 avg_loss = 1.18079\n","epoch no.0 train no.10160  loss = 1.45158 avg_loss = 1.19020\n","epoch no.0 train no.10160  loss = 1.45158 avg_loss = 1.19020\n","epoch no.0 train no.10170  loss = 1.74431 avg_loss = 1.17801\n","epoch no.0 train no.10170  loss = 1.74431 avg_loss = 1.17801\n","epoch no.0 train no.10180  loss = 1.56222 avg_loss = 1.19399\n","epoch no.0 train no.10180  loss = 1.56222 avg_loss = 1.19399\n","epoch no.0 train no.10190  loss = 0.22444 avg_loss = 1.17831\n","epoch no.0 train no.10190  loss = 0.22444 avg_loss = 1.17831\n","epoch no.0 train no.10200  loss = 1.17574 avg_loss = 1.19295\n","epoch no.0 train no.10200  loss = 1.17574 avg_loss = 1.19295\n","epoch no.0 train no.10210  loss = 1.71771 avg_loss = 1.19165\n","epoch no.0 train no.10210  loss = 1.71771 avg_loss = 1.19165\n","epoch no.0 train no.10220  loss = 0.87588 avg_loss = 1.19118\n","epoch no.0 train no.10220  loss = 0.87588 avg_loss = 1.19118\n","epoch no.0 train no.10230  loss = 0.92435 avg_loss = 1.19187\n","epoch no.0 train no.10230  loss = 0.92435 avg_loss = 1.19187\n","epoch no.0 train no.10240  loss = 1.34712 avg_loss = 1.18065\n","epoch no.0 train no.10240  loss = 1.34712 avg_loss = 1.18065\n","epoch no.0 train no.10250  loss = 1.16224 avg_loss = 1.17732\n","epoch no.0 train no.10250  loss = 1.16224 avg_loss = 1.17732\n","epoch no.0 train no.10260  loss = 1.35129 avg_loss = 1.17858\n","epoch no.0 train no.10260  loss = 1.35129 avg_loss = 1.17858\n","epoch no.0 train no.10270  loss = 1.53328 avg_loss = 1.15935\n","epoch no.0 train no.10270  loss = 1.53328 avg_loss = 1.15935\n","epoch no.0 train no.10280  loss = 0.89695 avg_loss = 1.17238\n","epoch no.0 train no.10280  loss = 0.89695 avg_loss = 1.17238\n","epoch no.0 train no.10290  loss = 1.51734 avg_loss = 1.17096\n","epoch no.0 train no.10290  loss = 1.51734 avg_loss = 1.17096\n","epoch no.0 train no.10300  loss = 1.97880 avg_loss = 1.17985\n","epoch no.0 train no.10300  loss = 1.97880 avg_loss = 1.17985\n","epoch no.0 train no.10310  loss = 0.39886 avg_loss = 1.16825\n","epoch no.0 train no.10310  loss = 0.39886 avg_loss = 1.16825\n","epoch no.0 train no.10320  loss = 1.00935 avg_loss = 1.18385\n","epoch no.0 train no.10320  loss = 1.00935 avg_loss = 1.18385\n","epoch no.0 train no.10330  loss = 0.38269 avg_loss = 1.16924\n","epoch no.0 train no.10330  loss = 0.38269 avg_loss = 1.16924\n","epoch no.0 train no.10340  loss = 1.20824 avg_loss = 1.19477\n","epoch no.0 train no.10340  loss = 1.20824 avg_loss = 1.19477\n","epoch no.0 train no.10350  loss = 1.15730 avg_loss = 1.18460\n","epoch no.0 train no.10350  loss = 1.15730 avg_loss = 1.18460\n","epoch no.0 train no.10360  loss = 0.88126 avg_loss = 1.18820\n","epoch no.0 train no.10360  loss = 0.88126 avg_loss = 1.18820\n","epoch no.0 train no.10370  loss = 0.68081 avg_loss = 1.19410\n","epoch no.0 train no.10370  loss = 0.68081 avg_loss = 1.19410\n","epoch no.0 train no.10380  loss = 1.10705 avg_loss = 1.18735\n","epoch no.0 train no.10380  loss = 1.10705 avg_loss = 1.18735\n","epoch no.0 train no.10390  loss = 1.27011 avg_loss = 1.19025\n","epoch no.0 train no.10390  loss = 1.27011 avg_loss = 1.19025\n","epoch no.0 train no.10400  loss = 0.53995 avg_loss = 1.17796\n","epoch no.0 train no.10400  loss = 0.53995 avg_loss = 1.17796\n","epoch no.0 train no.10410  loss = 1.78245 avg_loss = 1.18947\n","epoch no.0 train no.10410  loss = 1.78245 avg_loss = 1.18947\n","epoch no.0 train no.10420  loss = 1.50642 avg_loss = 1.19196\n","epoch no.0 train no.10420  loss = 1.50642 avg_loss = 1.19196\n","epoch no.0 train no.10430  loss = 1.06571 avg_loss = 1.19118\n","epoch no.0 train no.10430  loss = 1.06571 avg_loss = 1.19118\n","epoch no.0 train no.10440  loss = 0.82254 avg_loss = 1.19445\n","epoch no.0 train no.10440  loss = 0.82254 avg_loss = 1.19445\n","epoch no.0 train no.10450  loss = 1.05602 avg_loss = 1.19890\n","epoch no.0 train no.10450  loss = 1.05602 avg_loss = 1.19890\n","epoch no.0 train no.10460  loss = 1.21476 avg_loss = 1.20301\n","epoch no.0 train no.10460  loss = 1.21476 avg_loss = 1.20301\n","epoch no.0 train no.10470  loss = 1.41377 avg_loss = 1.19909\n","epoch no.0 train no.10470  loss = 1.41377 avg_loss = 1.19909\n","epoch no.0 train no.10480  loss = 0.85573 avg_loss = 1.19596\n","epoch no.0 train no.10480  loss = 0.85573 avg_loss = 1.19596\n","epoch no.0 train no.10490  loss = 0.96659 avg_loss = 1.19305\n","epoch no.0 train no.10490  loss = 0.96659 avg_loss = 1.19305\n","epoch no.0 train no.10500  loss = 1.39636 avg_loss = 1.18378\n","epoch no.0 train no.10500  loss = 1.39636 avg_loss = 1.18378\n","epoch no.0 train no.10510  loss = 0.79400 avg_loss = 1.17724\n","epoch no.0 train no.10510  loss = 0.79400 avg_loss = 1.17724\n","epoch no.0 train no.10520  loss = 1.36418 avg_loss = 1.15727\n","epoch no.0 train no.10520  loss = 1.36418 avg_loss = 1.15727\n","epoch no.0 train no.10530  loss = 1.42152 avg_loss = 1.16697\n","epoch no.0 train no.10530  loss = 1.42152 avg_loss = 1.16697\n","epoch no.0 train no.10540  loss = 1.57648 avg_loss = 1.17871\n","epoch no.0 train no.10540  loss = 1.57648 avg_loss = 1.17871\n","epoch no.0 train no.10550  loss = 1.49571 avg_loss = 1.17998\n","epoch no.0 train no.10550  loss = 1.49571 avg_loss = 1.17998\n","epoch no.0 train no.10560  loss = 1.13916 avg_loss = 1.19634\n","epoch no.0 train no.10560  loss = 1.13916 avg_loss = 1.19634\n","epoch no.0 train no.10570  loss = 0.78497 avg_loss = 1.18859\n","epoch no.0 train no.10570  loss = 0.78497 avg_loss = 1.18859\n","epoch no.0 train no.10580  loss = 1.50038 avg_loss = 1.17349\n","epoch no.0 train no.10580  loss = 1.50038 avg_loss = 1.17349\n","epoch no.0 train no.10590  loss = 1.35472 avg_loss = 1.18140\n","epoch no.0 train no.10590  loss = 1.35472 avg_loss = 1.18140\n","epoch no.0 train no.10600  loss = 1.69969 avg_loss = 1.19339\n","epoch no.0 train no.10600  loss = 1.69969 avg_loss = 1.19339\n","epoch no.0 train no.10610  loss = 0.83535 avg_loss = 1.19264\n","epoch no.0 train no.10610  loss = 0.83535 avg_loss = 1.19264\n","epoch no.0 train no.10620  loss = 1.04687 avg_loss = 1.19131\n","epoch no.0 train no.10620  loss = 1.04687 avg_loss = 1.19131\n","epoch no.0 train no.10630  loss = 0.94233 avg_loss = 1.16826\n","epoch no.0 train no.10630  loss = 0.94233 avg_loss = 1.16826\n","epoch no.0 train no.10640  loss = 0.56937 avg_loss = 1.18888\n","epoch no.0 train no.10640  loss = 0.56937 avg_loss = 1.18888\n","epoch no.0 train no.10650  loss = 1.64339 avg_loss = 1.18468\n","epoch no.0 train no.10650  loss = 1.64339 avg_loss = 1.18468\n","epoch no.0 train no.10660  loss = 1.15627 avg_loss = 1.18931\n","epoch no.0 train no.10660  loss = 1.15627 avg_loss = 1.18931\n","epoch no.0 train no.10670  loss = 0.95867 avg_loss = 1.17882\n","epoch no.0 train no.10670  loss = 0.95867 avg_loss = 1.17882\n","epoch no.0 train no.10680  loss = 1.55469 avg_loss = 1.19903\n","epoch no.0 train no.10680  loss = 1.55469 avg_loss = 1.19903\n","epoch no.0 train no.10690  loss = 1.25018 avg_loss = 1.19964\n","epoch no.0 train no.10690  loss = 1.25018 avg_loss = 1.19964\n","epoch no.0 train no.10700  loss = 0.52877 avg_loss = 1.18519\n","epoch no.0 train no.10700  loss = 0.52877 avg_loss = 1.18519\n","epoch no.0 train no.10710  loss = 0.60251 avg_loss = 1.19198\n","epoch no.0 train no.10710  loss = 0.60251 avg_loss = 1.19198\n","epoch no.0 train no.10720  loss = 1.21704 avg_loss = 1.19307\n","epoch no.0 train no.10720  loss = 1.21704 avg_loss = 1.19307\n","epoch no.0 train no.10730  loss = 1.04270 avg_loss = 1.18693\n","epoch no.0 train no.10730  loss = 1.04270 avg_loss = 1.18693\n","epoch no.0 train no.10740  loss = 1.34072 avg_loss = 1.18576\n","epoch no.0 train no.10740  loss = 1.34072 avg_loss = 1.18576\n","epoch no.0 train no.10750  loss = 1.05682 avg_loss = 1.19062\n","epoch no.0 train no.10750  loss = 1.05682 avg_loss = 1.19062\n","epoch no.0 train no.10760  loss = 1.49160 avg_loss = 1.17406\n","epoch no.0 train no.10760  loss = 1.49160 avg_loss = 1.17406\n","epoch no.0 train no.10770  loss = 0.75139 avg_loss = 1.16568\n","epoch no.0 train no.10770  loss = 0.75139 avg_loss = 1.16568\n","epoch no.0 train no.10780  loss = 1.40735 avg_loss = 1.15704\n","epoch no.0 train no.10780  loss = 1.40735 avg_loss = 1.15704\n","epoch no.0 train no.10790  loss = 0.66467 avg_loss = 1.16208\n","epoch no.0 train no.10790  loss = 0.66467 avg_loss = 1.16208\n","epoch no.0 train no.10800  loss = 1.86082 avg_loss = 1.17451\n","epoch no.0 train no.10800  loss = 1.86082 avg_loss = 1.17451\n","epoch no.0 train no.10810  loss = 0.82834 avg_loss = 1.15804\n","epoch no.0 train no.10810  loss = 0.82834 avg_loss = 1.15804\n","epoch no.0 train no.10820  loss = 0.96192 avg_loss = 1.14792\n","epoch no.0 train no.10820  loss = 0.96192 avg_loss = 1.14792\n","epoch no.0 train no.10830  loss = 1.37429 avg_loss = 1.15910\n","epoch no.0 train no.10830  loss = 1.37429 avg_loss = 1.15910\n","epoch no.0 train no.10840  loss = 1.25552 avg_loss = 1.15464\n","epoch no.0 train no.10840  loss = 1.25552 avg_loss = 1.15464\n","epoch no.0 train no.10850  loss = 1.35436 avg_loss = 1.14817\n","epoch no.0 train no.10850  loss = 1.35436 avg_loss = 1.14817\n","epoch no.0 train no.10860  loss = 0.81188 avg_loss = 1.14522\n","epoch no.0 train no.10860  loss = 0.81188 avg_loss = 1.14522\n","epoch no.0 train no.10870  loss = 1.65821 avg_loss = 1.16879\n","epoch no.0 train no.10870  loss = 1.65821 avg_loss = 1.16879\n","epoch no.0 train no.10880  loss = 1.35887 avg_loss = 1.18151\n","epoch no.0 train no.10880  loss = 1.35887 avg_loss = 1.18151\n","epoch no.0 train no.10890  loss = 1.18642 avg_loss = 1.18725\n","epoch no.0 train no.10890  loss = 1.18642 avg_loss = 1.18725\n","epoch no.0 train no.10900  loss = 1.08854 avg_loss = 1.20047\n","epoch no.0 train no.10900  loss = 1.08854 avg_loss = 1.20047\n","epoch no.0 train no.10910  loss = 1.30663 avg_loss = 1.20677\n","epoch no.0 train no.10910  loss = 1.30663 avg_loss = 1.20677\n","epoch no.0 train no.10920  loss = 0.74603 avg_loss = 1.20214\n","epoch no.0 train no.10920  loss = 0.74603 avg_loss = 1.20214\n","epoch no.0 train no.10930  loss = 1.51210 avg_loss = 1.19440\n","epoch no.0 train no.10930  loss = 1.51210 avg_loss = 1.19440\n","epoch no.0 train no.10940  loss = 1.31009 avg_loss = 1.17805\n","epoch no.0 train no.10940  loss = 1.31009 avg_loss = 1.17805\n","epoch no.0 train no.10950  loss = 1.62225 avg_loss = 1.17525\n","epoch no.0 train no.10950  loss = 1.62225 avg_loss = 1.17525\n","epoch no.0 train no.10960  loss = 1.86511 avg_loss = 1.18065\n","epoch no.0 train no.10960  loss = 1.86511 avg_loss = 1.18065\n","epoch no.0 train no.10970  loss = 0.65605 avg_loss = 1.18455\n","epoch no.0 train no.10970  loss = 0.65605 avg_loss = 1.18455\n","epoch no.0 train no.10980  loss = 1.21552 avg_loss = 1.18189\n","epoch no.0 train no.10980  loss = 1.21552 avg_loss = 1.18189\n","epoch no.0 train no.10990  loss = 1.10165 avg_loss = 1.18612\n","epoch no.0 train no.10990  loss = 1.10165 avg_loss = 1.18612\n","epoch no.0 train no.11000  loss = 1.60937 avg_loss = 1.17804\n","epoch no.0 train no.11000  loss = 1.60937 avg_loss = 1.17804\n","to_tokens: ['▁', '해요', '▁', '해요', '▁그대', '▁', '▁', '▁', '요', '▁', '▁듣고', '▁싶어', '▁', '▁', '▁수', '▁']\n","사랑해요\n","\n","사랑해요 그대만을\n","\n","사랑해\n","\n","줘요\n","\n","평생 듣고 싶은  말도\n","\n","할게요\n","\n","\n","to_tokens: ['▁', '해요', '▁', '해요', '▁그대', '▁', '▁', '▁', '요', '▁', '▁듣고', '▁싶어', '▁', '▁', '▁수', '▁']\n","사랑해요\n","\n","사랑해요 그대만을\n","\n","사랑해\n","\n","줘요\n","\n","평생 듣고 싶은  말도\n","\n","할게요\n","\n","\n","epoch no.0 train no.11010  loss = 0.64639 avg_loss = 1.15876\n","epoch no.0 train no.11010  loss = 0.64639 avg_loss = 1.15876\n","epoch no.0 train no.11020  loss = 0.96961 avg_loss = 1.15076\n","epoch no.0 train no.11020  loss = 0.96961 avg_loss = 1.15076\n","epoch no.0 train no.11030  loss = 1.13964 avg_loss = 1.14592\n","epoch no.0 train no.11030  loss = 1.13964 avg_loss = 1.14592\n","epoch no.0 train no.11040  loss = 1.21938 avg_loss = 1.14448\n","epoch no.0 train no.11040  loss = 1.21938 avg_loss = 1.14448\n","epoch no.0 train no.11050  loss = 1.32554 avg_loss = 1.13949\n","epoch no.0 train no.11050  loss = 1.32554 avg_loss = 1.13949\n","epoch no.0 train no.11060  loss = 1.27529 avg_loss = 1.13551\n","epoch no.0 train no.11060  loss = 1.27529 avg_loss = 1.13551\n","epoch no.0 train no.11070  loss = 0.91387 avg_loss = 1.13326\n","epoch no.0 train no.11070  loss = 0.91387 avg_loss = 1.13326\n","epoch no.0 train no.11080  loss = 0.91859 avg_loss = 1.14443\n","epoch no.0 train no.11080  loss = 0.91859 avg_loss = 1.14443\n","epoch no.0 train no.11090  loss = 1.51087 avg_loss = 1.17750\n","epoch no.0 train no.11090  loss = 1.51087 avg_loss = 1.17750\n","epoch no.0 train no.11100  loss = 1.14757 avg_loss = 1.16365\n","epoch no.0 train no.11100  loss = 1.14757 avg_loss = 1.16365\n","epoch no.0 train no.11110  loss = 1.07209 avg_loss = 1.17587\n","epoch no.0 train no.11110  loss = 1.07209 avg_loss = 1.17587\n","epoch no.0 train no.11120  loss = 0.84368 avg_loss = 1.14593\n","epoch no.0 train no.11120  loss = 0.84368 avg_loss = 1.14593\n","epoch no.0 train no.11130  loss = 1.08949 avg_loss = 1.14581\n","epoch no.0 train no.11130  loss = 1.08949 avg_loss = 1.14581\n","epoch no.0 train no.11140  loss = 1.35744 avg_loss = 1.15634\n","epoch no.0 train no.11140  loss = 1.35744 avg_loss = 1.15634\n","epoch no.0 train no.11150  loss = 1.53324 avg_loss = 1.15952\n","epoch no.0 train no.11150  loss = 1.53324 avg_loss = 1.15952\n","epoch no.0 train no.11160  loss = 0.83368 avg_loss = 1.16223\n","epoch no.0 train no.11160  loss = 0.83368 avg_loss = 1.16223\n","epoch no.0 train no.11170  loss = 1.07253 avg_loss = 1.15939\n","epoch no.0 train no.11170  loss = 1.07253 avg_loss = 1.15939\n","epoch no.0 train no.11180  loss = 1.28054 avg_loss = 1.16026\n","epoch no.0 train no.11180  loss = 1.28054 avg_loss = 1.16026\n","epoch no.0 train no.11190  loss = 0.41453 avg_loss = 1.14616\n","epoch no.0 train no.11190  loss = 0.41453 avg_loss = 1.14616\n","epoch no.0 train no.11200  loss = 0.80022 avg_loss = 1.14645\n","epoch no.0 train no.11200  loss = 0.80022 avg_loss = 1.14645\n","epoch no.0 train no.11210  loss = 2.07955 avg_loss = 1.16071\n","epoch no.0 train no.11210  loss = 2.07955 avg_loss = 1.16071\n","epoch no.0 train no.11220  loss = 1.09028 avg_loss = 1.15210\n","epoch no.0 train no.11220  loss = 1.09028 avg_loss = 1.15210\n","epoch no.0 train no.11230  loss = 1.24415 avg_loss = 1.15668\n","epoch no.0 train no.11230  loss = 1.24415 avg_loss = 1.15668\n","epoch no.0 train no.11240  loss = 1.00094 avg_loss = 1.16613\n","epoch no.0 train no.11240  loss = 1.00094 avg_loss = 1.16613\n","epoch no.0 train no.11250  loss = 1.06613 avg_loss = 1.16554\n","epoch no.0 train no.11250  loss = 1.06613 avg_loss = 1.16554\n","epoch no.0 train no.11260  loss = 1.28159 avg_loss = 1.16498\n","epoch no.0 train no.11260  loss = 1.28159 avg_loss = 1.16498\n","epoch no.0 train no.11270  loss = 2.26732 avg_loss = 1.17238\n","epoch no.0 train no.11270  loss = 2.26732 avg_loss = 1.17238\n","epoch no.0 train no.11280  loss = 1.77797 avg_loss = 1.18723\n","epoch no.0 train no.11280  loss = 1.77797 avg_loss = 1.18723\n","epoch no.0 train no.11290  loss = 1.48236 avg_loss = 1.19655\n","epoch no.0 train no.11290  loss = 1.48236 avg_loss = 1.19655\n","epoch no.0 train no.11300  loss = 0.79329 avg_loss = 1.18600\n","epoch no.0 train no.11300  loss = 0.79329 avg_loss = 1.18600\n","epoch no.0 train no.11310  loss = 1.01565 avg_loss = 1.19142\n","epoch no.0 train no.11310  loss = 1.01565 avg_loss = 1.19142\n","epoch no.0 train no.11320  loss = 0.76475 avg_loss = 1.18498\n","epoch no.0 train no.11320  loss = 0.76475 avg_loss = 1.18498\n","epoch no.0 train no.11330  loss = 1.17944 avg_loss = 1.18806\n","epoch no.0 train no.11330  loss = 1.17944 avg_loss = 1.18806\n","epoch no.0 train no.11340  loss = 0.43397 avg_loss = 1.17244\n","epoch no.0 train no.11340  loss = 0.43397 avg_loss = 1.17244\n","epoch no.0 train no.11350  loss = 0.98769 avg_loss = 1.17786\n","epoch no.0 train no.11350  loss = 0.98769 avg_loss = 1.17786\n","epoch no.0 train no.11360  loss = 0.45144 avg_loss = 1.18183\n","epoch no.0 train no.11360  loss = 0.45144 avg_loss = 1.18183\n","epoch no.0 train no.11370  loss = 0.51028 avg_loss = 1.15580\n","epoch no.0 train no.11370  loss = 0.51028 avg_loss = 1.15580\n","epoch no.0 train no.11380  loss = 0.45445 avg_loss = 1.13511\n","epoch no.0 train no.11380  loss = 0.45445 avg_loss = 1.13511\n","epoch no.0 train no.11390  loss = 0.71823 avg_loss = 1.13200\n","epoch no.0 train no.11390  loss = 0.71823 avg_loss = 1.13200\n","epoch no.0 train no.11400  loss = 1.34098 avg_loss = 1.12950\n","epoch no.0 train no.11400  loss = 1.34098 avg_loss = 1.12950\n","epoch no.0 train no.11410  loss = 1.56162 avg_loss = 1.13987\n","epoch no.0 train no.11410  loss = 1.56162 avg_loss = 1.13987\n","epoch no.0 train no.11420  loss = 1.22091 avg_loss = 1.14055\n","epoch no.0 train no.11420  loss = 1.22091 avg_loss = 1.14055\n","epoch no.0 train no.11430  loss = 1.99795 avg_loss = 1.13575\n","epoch no.0 train no.11430  loss = 1.99795 avg_loss = 1.13575\n","epoch no.0 train no.11440  loss = 1.75835 avg_loss = 1.16534\n","epoch no.0 train no.11440  loss = 1.75835 avg_loss = 1.16534\n","epoch no.0 train no.11450  loss = 1.50102 avg_loss = 1.17013\n","epoch no.0 train no.11450  loss = 1.50102 avg_loss = 1.17013\n","epoch no.0 train no.11460  loss = 1.62146 avg_loss = 1.17648\n","epoch no.0 train no.11460  loss = 1.62146 avg_loss = 1.17648\n","epoch no.0 train no.11470  loss = 1.94607 avg_loss = 1.18480\n","epoch no.0 train no.11470  loss = 1.94607 avg_loss = 1.18480\n","epoch no.0 train no.11480  loss = 1.28088 avg_loss = 1.16656\n","epoch no.0 train no.11480  loss = 1.28088 avg_loss = 1.16656\n","epoch no.0 train no.11490  loss = 0.98485 avg_loss = 1.16163\n","epoch no.0 train no.11490  loss = 0.98485 avg_loss = 1.16163\n","epoch no.0 train no.11500  loss = 1.90844 avg_loss = 1.18144\n","epoch no.0 train no.11500  loss = 1.90844 avg_loss = 1.18144\n","epoch no.0 train no.11510  loss = 0.74906 avg_loss = 1.14434\n","epoch no.0 train no.11510  loss = 0.74906 avg_loss = 1.14434\n","epoch no.0 train no.11520  loss = 0.96868 avg_loss = 1.14704\n","epoch no.0 train no.11520  loss = 0.96868 avg_loss = 1.14704\n","epoch no.0 train no.11530  loss = 1.68650 avg_loss = 1.15400\n","epoch no.0 train no.11530  loss = 1.68650 avg_loss = 1.15400\n","epoch no.0 train no.11540  loss = 1.11077 avg_loss = 1.15361\n","epoch no.0 train no.11540  loss = 1.11077 avg_loss = 1.15361\n","epoch no.0 train no.11550  loss = 1.18047 avg_loss = 1.15231\n","epoch no.0 train no.11550  loss = 1.18047 avg_loss = 1.15231\n","epoch no.0 train no.11560  loss = 1.06785 avg_loss = 1.14405\n","epoch no.0 train no.11560  loss = 1.06785 avg_loss = 1.14405\n","epoch no.0 train no.11570  loss = 1.48006 avg_loss = 1.15318\n","epoch no.0 train no.11570  loss = 1.48006 avg_loss = 1.15318\n","epoch no.0 train no.11580  loss = 1.10016 avg_loss = 1.14582\n","epoch no.0 train no.11580  loss = 1.10016 avg_loss = 1.14582\n","epoch no.0 train no.11590  loss = 1.56972 avg_loss = 1.15825\n","epoch no.0 train no.11590  loss = 1.56972 avg_loss = 1.15825\n","epoch no.0 train no.11600  loss = 0.88034 avg_loss = 1.15963\n","epoch no.0 train no.11600  loss = 0.88034 avg_loss = 1.15963\n","epoch no.0 train no.11610  loss = 1.12038 avg_loss = 1.17115\n","epoch no.0 train no.11610  loss = 1.12038 avg_loss = 1.17115\n","epoch no.0 train no.11620  loss = 1.19987 avg_loss = 1.16597\n","epoch no.0 train no.11620  loss = 1.19987 avg_loss = 1.16597\n","epoch no.0 train no.11630  loss = 1.17363 avg_loss = 1.15444\n","epoch no.0 train no.11630  loss = 1.17363 avg_loss = 1.15444\n","epoch no.0 train no.11640  loss = 0.93962 avg_loss = 1.17150\n","epoch no.0 train no.11640  loss = 0.93962 avg_loss = 1.17150\n","epoch no.0 train no.11650  loss = 0.57158 avg_loss = 1.16748\n","epoch no.0 train no.11650  loss = 0.57158 avg_loss = 1.16748\n","epoch no.0 train no.11660  loss = 0.64589 avg_loss = 1.17606\n","epoch no.0 train no.11660  loss = 0.64589 avg_loss = 1.17606\n","epoch no.0 train no.11670  loss = 1.15933 avg_loss = 1.20072\n","epoch no.0 train no.11670  loss = 1.15933 avg_loss = 1.20072\n","epoch no.0 train no.11680  loss = 1.68931 avg_loss = 1.21741\n","epoch no.0 train no.11680  loss = 1.68931 avg_loss = 1.21741\n","epoch no.0 train no.11690  loss = 1.01625 avg_loss = 1.21640\n","epoch no.0 train no.11690  loss = 1.01625 avg_loss = 1.21640\n","epoch no.0 train no.11700  loss = 1.10846 avg_loss = 1.22027\n","epoch no.0 train no.11700  loss = 1.10846 avg_loss = 1.22027\n","epoch no.0 train no.11710  loss = 0.67956 avg_loss = 1.22585\n","epoch no.0 train no.11710  loss = 0.67956 avg_loss = 1.22585\n","epoch no.0 train no.11720  loss = 1.58271 avg_loss = 1.20706\n","epoch no.0 train no.11720  loss = 1.58271 avg_loss = 1.20706\n","epoch no.0 train no.11730  loss = 1.23717 avg_loss = 1.20478\n","epoch no.0 train no.11730  loss = 1.23717 avg_loss = 1.20478\n","epoch no.0 train no.11740  loss = 0.96746 avg_loss = 1.22122\n","epoch no.0 train no.11740  loss = 0.96746 avg_loss = 1.22122\n","epoch no.0 train no.11750  loss = 0.82723 avg_loss = 1.22207\n","epoch no.0 train no.11750  loss = 0.82723 avg_loss = 1.22207\n","epoch no.0 train no.11760  loss = 1.58467 avg_loss = 1.23777\n","epoch no.0 train no.11760  loss = 1.58467 avg_loss = 1.23777\n","epoch no.0 train no.11770  loss = 1.80245 avg_loss = 1.24884\n","epoch no.0 train no.11770  loss = 1.80245 avg_loss = 1.24884\n","epoch no.0 train no.11780  loss = 1.81711 avg_loss = 1.23458\n","epoch no.0 train no.11780  loss = 1.81711 avg_loss = 1.23458\n","epoch no.0 train no.11790  loss = 1.09389 avg_loss = 1.23564\n","epoch no.0 train no.11790  loss = 1.09389 avg_loss = 1.23564\n","epoch no.0 train no.11800  loss = 1.70805 avg_loss = 1.23226\n","epoch no.0 train no.11800  loss = 1.70805 avg_loss = 1.23226\n","epoch no.0 train no.11810  loss = 1.08858 avg_loss = 1.23255\n","epoch no.0 train no.11810  loss = 1.08858 avg_loss = 1.23255\n","epoch no.0 train no.11820  loss = 0.72139 avg_loss = 1.21312\n","epoch no.0 train no.11820  loss = 0.72139 avg_loss = 1.21312\n","epoch no.0 train no.11830  loss = 1.37855 avg_loss = 1.21471\n","epoch no.0 train no.11830  loss = 1.37855 avg_loss = 1.21471\n","epoch no.0 train no.11840  loss = 0.84931 avg_loss = 1.21631\n","epoch no.0 train no.11840  loss = 0.84931 avg_loss = 1.21631\n","epoch no.0 train no.11850  loss = 1.32130 avg_loss = 1.21144\n","epoch no.0 train no.11850  loss = 1.32130 avg_loss = 1.21144\n","epoch no.0 train no.11860  loss = 1.28766 avg_loss = 1.22190\n","epoch no.0 train no.11860  loss = 1.28766 avg_loss = 1.22190\n","epoch no.0 train no.11870  loss = 0.59837 avg_loss = 1.21023\n","epoch no.0 train no.11870  loss = 0.59837 avg_loss = 1.21023\n","epoch no.0 train no.11880  loss = 1.00771 avg_loss = 1.23750\n","epoch no.0 train no.11880  loss = 1.00771 avg_loss = 1.23750\n","epoch no.0 train no.11890  loss = 1.20368 avg_loss = 1.23231\n","epoch no.0 train no.11890  loss = 1.20368 avg_loss = 1.23231\n","epoch no.0 train no.11900  loss = 1.35337 avg_loss = 1.23010\n","epoch no.0 train no.11900  loss = 1.35337 avg_loss = 1.23010\n","epoch no.0 train no.11910  loss = 1.42257 avg_loss = 1.21400\n","epoch no.0 train no.11910  loss = 1.42257 avg_loss = 1.21400\n","epoch no.0 train no.11920  loss = 1.15701 avg_loss = 1.21206\n","epoch no.0 train no.11920  loss = 1.15701 avg_loss = 1.21206\n","epoch no.0 train no.11930  loss = 0.80815 avg_loss = 1.19204\n","epoch no.0 train no.11930  loss = 0.80815 avg_loss = 1.19204\n","epoch no.0 train no.11940  loss = 1.16109 avg_loss = 1.17592\n","epoch no.0 train no.11940  loss = 1.16109 avg_loss = 1.17592\n","epoch no.0 train no.11950  loss = 1.71391 avg_loss = 1.19488\n","epoch no.0 train no.11950  loss = 1.71391 avg_loss = 1.19488\n","epoch no.0 train no.11960  loss = 1.75924 avg_loss = 1.19950\n","epoch no.0 train no.11960  loss = 1.75924 avg_loss = 1.19950\n","epoch no.0 train no.11970  loss = 1.09197 avg_loss = 1.20352\n","epoch no.0 train no.11970  loss = 1.09197 avg_loss = 1.20352\n","epoch no.0 train no.11980  loss = 0.89675 avg_loss = 1.19524\n","epoch no.0 train no.11980  loss = 0.89675 avg_loss = 1.19524\n","epoch no.0 train no.11990  loss = 1.34595 avg_loss = 1.19381\n","epoch no.0 train no.11990  loss = 1.34595 avg_loss = 1.19381\n","epoch no.0 train no.12000  loss = 0.36960 avg_loss = 1.17961\n","epoch no.0 train no.12000  loss = 0.36960 avg_loss = 1.17961\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', 'M', '▁', '▁', '▁받는', '버려', '▁', '▁', '▁단계', 'te', 'p', '으로', '▁', '▁', '▁', '▁앞', '▁', '▁', '▁그래', '올', '보다', '▁원', '니까', '링', '해주', '▁때', '▁', '▁뒤에', '치가', '▁', '타', '머', '▁영감을', 'ain', '▁하던', '▁하지', '▁', 'I', '엔', '분이', '놈', '▁안', '▁가도', '▁돼', '▁', '덕', '▁내게', '<unk>', 'now', 'fin', 'ook', '▁', '픽', '하이', '의', '▁\"', '빈', '화', '(', '開', '花', '\"', '▁가사', '▁', '개', 'ver', ')', '▁', '▁', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n","DM\n","\n",", 무시해버려도 다음  step으로 넘겨주지\n","\n","내 경험치가 ,타 가수 모두가 피쳐링할 테니\n","\n","내 경험치가\n","\n",",나에게 뭐라 안 해 , 주위 덕소 멀리  안 가도 돼\n","\n",", 넌 어<unk>\n","\n","에픽하이의 \"개화(開花)\" 가사\n","\n","(cover)\n","\n","가사\n","\n",")\n","\n","\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', 'M', '▁', '▁', '▁받는', '버려', '▁', '▁', '▁단계', 'te', 'p', '으로', '▁', '▁', '▁', '▁앞', '▁', '▁', '▁그래', '올', '보다', '▁원', '니까', '링', '해주', '▁때', '▁', '▁뒤에', '치가', '▁', '타', '머', '▁영감을', 'ain', '▁하던', '▁하지', '▁', 'I', '엔', '분이', '놈', '▁안', '▁가도', '▁돼', '▁', '덕', '▁내게', '<unk>', 'now', 'fin', 'ook', '▁', '픽', '하이', '의', '▁\"', '빈', '화', '(', '開', '花', '\"', '▁가사', '▁', '개', 'ver', ')', '▁', '▁', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n","DM\n","\n",", 무시해버려도 다음  step으로 넘겨주지\n","\n","내 경험치가 ,타 가수 모두가 피쳐링할 테니\n","\n","내 경험치가\n","\n",",나에게 뭐라 안 해 , 주위 덕소 멀리  안 가도 돼\n","\n",", 넌 어<unk>\n","\n","에픽하이의 \"개화(開花)\" 가사\n","\n","(cover)\n","\n","가사\n","\n",")\n","\n","\n","epoch no.0 train no.12010  loss = 1.08337 avg_loss = 1.17985\n","epoch no.0 train no.12010  loss = 1.08337 avg_loss = 1.17985\n","epoch no.0 train no.12020  loss = 1.28299 avg_loss = 1.16706\n","epoch no.0 train no.12020  loss = 1.28299 avg_loss = 1.16706\n","epoch no.0 train no.12030  loss = 1.62749 avg_loss = 1.17570\n","epoch no.0 train no.12030  loss = 1.62749 avg_loss = 1.17570\n","epoch no.0 train no.12040  loss = 1.16936 avg_loss = 1.17591\n","epoch no.0 train no.12040  loss = 1.16936 avg_loss = 1.17591\n","epoch no.0 train no.12050  loss = 1.00771 avg_loss = 1.17554\n","epoch no.0 train no.12050  loss = 1.00771 avg_loss = 1.17554\n","epoch no.0 train no.12060  loss = 0.95763 avg_loss = 1.15973\n","epoch no.0 train no.12060  loss = 0.95763 avg_loss = 1.15973\n","epoch no.0 train no.12070  loss = 1.17759 avg_loss = 1.15801\n","epoch no.0 train no.12070  loss = 1.17759 avg_loss = 1.15801\n","epoch no.0 train no.12080  loss = 1.14582 avg_loss = 1.15872\n","epoch no.0 train no.12080  loss = 1.14582 avg_loss = 1.15872\n","epoch no.0 train no.12090  loss = 0.42330 avg_loss = 1.13957\n","epoch no.0 train no.12090  loss = 0.42330 avg_loss = 1.13957\n","epoch no.0 train no.12100  loss = 0.62338 avg_loss = 1.14628\n","epoch no.0 train no.12100  loss = 0.62338 avg_loss = 1.14628\n","epoch no.0 train no.12110  loss = 1.32454 avg_loss = 1.14785\n","epoch no.0 train no.12110  loss = 1.32454 avg_loss = 1.14785\n","epoch no.0 train no.12120  loss = 0.42650 avg_loss = 1.14058\n","epoch no.0 train no.12120  loss = 0.42650 avg_loss = 1.14058\n","epoch no.0 train no.12130  loss = 0.62777 avg_loss = 1.11222\n","epoch no.0 train no.12130  loss = 0.62777 avg_loss = 1.11222\n","epoch no.0 train no.12140  loss = 1.12080 avg_loss = 1.11942\n","epoch no.0 train no.12140  loss = 1.12080 avg_loss = 1.11942\n","epoch no.0 train no.12150  loss = 1.26171 avg_loss = 1.12844\n","epoch no.0 train no.12150  loss = 1.26171 avg_loss = 1.12844\n","epoch no.0 train no.12160  loss = 1.48956 avg_loss = 1.13461\n","epoch no.0 train no.12160  loss = 1.48956 avg_loss = 1.13461\n","epoch no.0 train no.12170  loss = 0.76042 avg_loss = 1.11993\n","epoch no.0 train no.12170  loss = 0.76042 avg_loss = 1.11993\n","epoch no.0 train no.12180  loss = 1.55843 avg_loss = 1.14443\n","epoch no.0 train no.12180  loss = 1.55843 avg_loss = 1.14443\n","epoch no.0 train no.12190  loss = 0.84166 avg_loss = 1.14671\n","epoch no.0 train no.12190  loss = 0.84166 avg_loss = 1.14671\n","epoch no.0 train no.12200  loss = 0.61584 avg_loss = 1.12577\n","epoch no.0 train no.12200  loss = 0.61584 avg_loss = 1.12577\n","epoch no.0 train no.12210  loss = 1.39204 avg_loss = 1.11980\n","epoch no.0 train no.12210  loss = 1.39204 avg_loss = 1.11980\n","epoch no.0 train no.12220  loss = 1.59132 avg_loss = 1.13849\n","epoch no.0 train no.12220  loss = 1.59132 avg_loss = 1.13849\n","epoch no.0 train no.12230  loss = 0.73331 avg_loss = 1.12710\n","epoch no.0 train no.12230  loss = 0.73331 avg_loss = 1.12710\n","epoch no.0 train no.12240  loss = 1.26287 avg_loss = 1.12929\n","epoch no.0 train no.12240  loss = 1.26287 avg_loss = 1.12929\n","epoch no.0 train no.12250  loss = 0.68485 avg_loss = 1.12331\n","epoch no.0 train no.12250  loss = 0.68485 avg_loss = 1.12331\n","epoch no.0 train no.12260  loss = 1.24342 avg_loss = 1.12092\n","epoch no.0 train no.12260  loss = 1.24342 avg_loss = 1.12092\n","epoch no.0 train no.12270  loss = 1.38233 avg_loss = 1.13543\n","epoch no.0 train no.12270  loss = 1.38233 avg_loss = 1.13543\n","epoch no.0 train no.12280  loss = 1.18648 avg_loss = 1.13524\n","epoch no.0 train no.12280  loss = 1.18648 avg_loss = 1.13524\n","epoch no.0 train no.12290  loss = 0.45301 avg_loss = 1.12059\n","epoch no.0 train no.12290  loss = 0.45301 avg_loss = 1.12059\n","epoch no.0 train no.12300  loss = 1.50094 avg_loss = 1.14251\n","epoch no.0 train no.12300  loss = 1.50094 avg_loss = 1.14251\n","epoch no.0 train no.12310  loss = 1.23215 avg_loss = 1.15318\n","epoch no.0 train no.12310  loss = 1.23215 avg_loss = 1.15318\n","epoch no.0 train no.12320  loss = 1.30944 avg_loss = 1.14004\n","epoch no.0 train no.12320  loss = 1.30944 avg_loss = 1.14004\n","epoch no.0 train no.12330  loss = 1.23325 avg_loss = 1.14522\n","epoch no.0 train no.12330  loss = 1.23325 avg_loss = 1.14522\n","epoch no.0 train no.12340  loss = 1.18660 avg_loss = 1.15851\n","epoch no.0 train no.12340  loss = 1.18660 avg_loss = 1.15851\n","epoch no.0 train no.12350  loss = 1.53936 avg_loss = 1.16199\n","epoch no.0 train no.12350  loss = 1.53936 avg_loss = 1.16199\n","epoch no.0 train no.12360  loss = 0.86966 avg_loss = 1.14310\n","epoch no.0 train no.12360  loss = 0.86966 avg_loss = 1.14310\n","epoch no.0 train no.12370  loss = 0.95330 avg_loss = 1.13887\n","epoch no.0 train no.12370  loss = 0.95330 avg_loss = 1.13887\n","epoch no.0 train no.12380  loss = 0.76916 avg_loss = 1.13821\n","epoch no.0 train no.12380  loss = 0.76916 avg_loss = 1.13821\n","epoch no.0 train no.12390  loss = 1.37259 avg_loss = 1.13740\n","epoch no.0 train no.12390  loss = 1.37259 avg_loss = 1.13740\n","epoch no.0 train no.12400  loss = 0.57032 avg_loss = 1.14273\n","epoch no.0 train no.12400  loss = 0.57032 avg_loss = 1.14273\n","epoch no.0 train no.12410  loss = 1.20817 avg_loss = 1.14236\n","epoch no.0 train no.12410  loss = 1.20817 avg_loss = 1.14236\n","epoch no.0 train no.12420  loss = 0.72187 avg_loss = 1.12381\n","epoch no.0 train no.12420  loss = 0.72187 avg_loss = 1.12381\n","epoch no.0 train no.12430  loss = 1.15334 avg_loss = 1.12556\n","epoch no.0 train no.12430  loss = 1.15334 avg_loss = 1.12556\n","epoch no.0 train no.12440  loss = 1.18655 avg_loss = 1.13625\n","epoch no.0 train no.12440  loss = 1.18655 avg_loss = 1.13625\n","epoch no.0 train no.12450  loss = 0.80832 avg_loss = 1.13578\n","epoch no.0 train no.12450  loss = 0.80832 avg_loss = 1.13578\n","epoch no.0 train no.12460  loss = 1.06912 avg_loss = 1.13174\n","epoch no.0 train no.12460  loss = 1.06912 avg_loss = 1.13174\n","epoch no.0 train no.12470  loss = 0.80427 avg_loss = 1.13434\n","epoch no.0 train no.12470  loss = 0.80427 avg_loss = 1.13434\n","epoch no.0 train no.12480  loss = 0.99397 avg_loss = 1.13038\n","epoch no.0 train no.12480  loss = 0.99397 avg_loss = 1.13038\n","epoch no.0 train no.12490  loss = 1.90658 avg_loss = 1.12814\n","epoch no.0 train no.12490  loss = 1.90658 avg_loss = 1.12814\n","epoch no.0 train no.12500  loss = 0.67506 avg_loss = 1.12669\n","epoch no.0 train no.12500  loss = 0.67506 avg_loss = 1.12669\n","epoch no.0 train no.12510  loss = 1.60639 avg_loss = 1.13512\n","epoch no.0 train no.12510  loss = 1.60639 avg_loss = 1.13512\n","epoch no.0 train no.12520  loss = 0.92370 avg_loss = 1.16304\n","epoch no.0 train no.12520  loss = 0.92370 avg_loss = 1.16304\n","epoch no.0 train no.12530  loss = 0.86388 avg_loss = 1.17093\n","epoch no.0 train no.12530  loss = 0.86388 avg_loss = 1.17093\n","epoch no.0 train no.12540  loss = 1.41549 avg_loss = 1.17105\n","epoch no.0 train no.12540  loss = 1.41549 avg_loss = 1.17105\n","epoch no.0 train no.12550  loss = 0.80142 avg_loss = 1.18339\n","epoch no.0 train no.12550  loss = 0.80142 avg_loss = 1.18339\n","epoch no.0 train no.12560  loss = 1.10545 avg_loss = 1.16914\n","epoch no.0 train no.12560  loss = 1.10545 avg_loss = 1.16914\n","epoch no.0 train no.12570  loss = 1.52294 avg_loss = 1.15926\n","epoch no.0 train no.12570  loss = 1.52294 avg_loss = 1.15926\n","epoch no.0 train no.12580  loss = 0.97700 avg_loss = 1.15516\n","epoch no.0 train no.12580  loss = 0.97700 avg_loss = 1.15516\n","epoch no.0 train no.12590  loss = 0.83938 avg_loss = 1.15692\n","epoch no.0 train no.12590  loss = 0.83938 avg_loss = 1.15692\n","epoch no.0 train no.12600  loss = 0.97340 avg_loss = 1.14653\n","epoch no.0 train no.12600  loss = 0.97340 avg_loss = 1.14653\n","epoch no.0 train no.12610  loss = 1.01348 avg_loss = 1.14585\n","epoch no.0 train no.12610  loss = 1.01348 avg_loss = 1.14585\n","epoch no.0 train no.12620  loss = 0.90583 avg_loss = 1.14709\n","epoch no.0 train no.12620  loss = 0.90583 avg_loss = 1.14709\n","epoch no.0 train no.12630  loss = 1.24586 avg_loss = 1.13923\n","epoch no.0 train no.12630  loss = 1.24586 avg_loss = 1.13923\n","epoch no.0 train no.12640  loss = 1.76940 avg_loss = 1.15077\n","epoch no.0 train no.12640  loss = 1.76940 avg_loss = 1.15077\n","epoch no.0 train no.12650  loss = 1.29410 avg_loss = 1.15554\n","epoch no.0 train no.12650  loss = 1.29410 avg_loss = 1.15554\n","epoch no.0 train no.12660  loss = 1.50286 avg_loss = 1.16280\n","epoch no.0 train no.12660  loss = 1.50286 avg_loss = 1.16280\n","epoch no.0 train no.12670  loss = 1.34031 avg_loss = 1.15711\n","epoch no.0 train no.12670  loss = 1.34031 avg_loss = 1.15711\n","epoch no.0 train no.12680  loss = 0.97917 avg_loss = 1.16697\n","epoch no.0 train no.12680  loss = 0.97917 avg_loss = 1.16697\n","epoch no.0 train no.12690  loss = 0.91077 avg_loss = 1.15991\n","epoch no.0 train no.12690  loss = 0.91077 avg_loss = 1.15991\n","epoch no.0 train no.12700  loss = 1.16500 avg_loss = 1.16160\n","epoch no.0 train no.12700  loss = 1.16500 avg_loss = 1.16160\n","epoch no.0 train no.12710  loss = 0.51046 avg_loss = 1.16630\n","epoch no.0 train no.12710  loss = 0.51046 avg_loss = 1.16630\n","epoch no.0 train no.12720  loss = 0.99235 avg_loss = 1.14785\n","epoch no.0 train no.12720  loss = 0.99235 avg_loss = 1.14785\n","epoch no.0 train no.12730  loss = 0.48114 avg_loss = 1.14697\n","epoch no.0 train no.12730  loss = 0.48114 avg_loss = 1.14697\n","epoch no.0 train no.12740  loss = 1.57474 avg_loss = 1.15449\n","epoch no.0 train no.12740  loss = 1.57474 avg_loss = 1.15449\n","epoch no.0 train no.12750  loss = 1.80542 avg_loss = 1.16956\n","epoch no.0 train no.12750  loss = 1.80542 avg_loss = 1.16956\n","epoch no.0 train no.12760  loss = 1.46569 avg_loss = 1.19008\n","epoch no.0 train no.12760  loss = 1.46569 avg_loss = 1.19008\n","epoch no.0 train no.12770  loss = 0.33590 avg_loss = 1.17127\n","epoch no.0 train no.12770  loss = 0.33590 avg_loss = 1.17127\n","epoch no.0 train no.12780  loss = 1.15240 avg_loss = 1.19328\n","epoch no.0 train no.12780  loss = 1.15240 avg_loss = 1.19328\n","epoch no.0 train no.12790  loss = 1.32447 avg_loss = 1.20568\n","epoch no.0 train no.12790  loss = 1.32447 avg_loss = 1.20568\n","epoch no.0 train no.12800  loss = 1.29449 avg_loss = 1.18231\n","epoch no.0 train no.12800  loss = 1.29449 avg_loss = 1.18231\n","epoch no.0 train no.12810  loss = 0.99449 avg_loss = 1.18437\n","epoch no.0 train no.12810  loss = 0.99449 avg_loss = 1.18437\n","epoch no.0 train no.12820  loss = 1.40386 avg_loss = 1.17452\n","epoch no.0 train no.12820  loss = 1.40386 avg_loss = 1.17452\n","epoch no.0 train no.12830  loss = 1.02490 avg_loss = 1.17563\n","epoch no.0 train no.12830  loss = 1.02490 avg_loss = 1.17563\n","epoch no.0 train no.12840  loss = 1.56315 avg_loss = 1.17205\n","epoch no.0 train no.12840  loss = 1.56315 avg_loss = 1.17205\n","epoch no.0 train no.12850  loss = 1.34990 avg_loss = 1.17178\n","epoch no.0 train no.12850  loss = 1.34990 avg_loss = 1.17178\n","epoch no.0 train no.12860  loss = 1.16046 avg_loss = 1.17769\n","epoch no.0 train no.12860  loss = 1.16046 avg_loss = 1.17769\n","epoch no.0 train no.12870  loss = 1.28450 avg_loss = 1.18579\n","epoch no.0 train no.12870  loss = 1.28450 avg_loss = 1.18579\n","epoch no.0 train no.12880  loss = 0.39175 avg_loss = 1.18840\n","epoch no.0 train no.12880  loss = 0.39175 avg_loss = 1.18840\n","epoch no.0 train no.12890  loss = 0.94908 avg_loss = 1.17604\n","epoch no.0 train no.12890  loss = 0.94908 avg_loss = 1.17604\n","epoch no.0 train no.12900  loss = 0.96616 avg_loss = 1.19830\n","epoch no.0 train no.12900  loss = 0.96616 avg_loss = 1.19830\n","epoch no.0 train no.12910  loss = 1.04929 avg_loss = 1.20553\n","epoch no.0 train no.12910  loss = 1.04929 avg_loss = 1.20553\n","epoch no.0 train no.12920  loss = 1.87550 avg_loss = 1.22784\n","epoch no.0 train no.12920  loss = 1.87550 avg_loss = 1.22784\n","epoch no.0 train no.12930  loss = 1.64308 avg_loss = 1.23073\n","epoch no.0 train no.12930  loss = 1.64308 avg_loss = 1.23073\n","epoch no.0 train no.12940  loss = 1.49026 avg_loss = 1.22378\n","epoch no.0 train no.12940  loss = 1.49026 avg_loss = 1.22378\n","epoch no.0 train no.12950  loss = 1.25632 avg_loss = 1.21775\n","epoch no.0 train no.12950  loss = 1.25632 avg_loss = 1.21775\n","epoch no.0 train no.12960  loss = 1.83921 avg_loss = 1.22376\n","epoch no.0 train no.12960  loss = 1.83921 avg_loss = 1.22376\n","epoch no.0 train no.12970  loss = 0.69682 avg_loss = 1.24160\n","epoch no.0 train no.12970  loss = 0.69682 avg_loss = 1.24160\n","epoch no.0 train no.12980  loss = 0.77386 avg_loss = 1.23617\n","epoch no.0 train no.12980  loss = 0.77386 avg_loss = 1.23617\n","epoch no.0 train no.12990  loss = 1.04576 avg_loss = 1.24309\n","epoch no.0 train no.12990  loss = 1.04576 avg_loss = 1.24309\n","epoch no.0 train no.13000  loss = 0.62370 avg_loss = 1.24043\n","epoch no.0 train no.13000  loss = 0.62370 avg_loss = 1.24043\n","to_tokens: ['▁', '▁노', '단', '▁말', '▁없이', '▁의미', '▁없는', '▁', '▁', '▁걸', '▁', '도', '▁그런', '이란', '▁', '하던', '▁', '▁소용', '▁없는', '▁거야', '▁']\n","사랑한단 말도  아무 의미 없는 말인 거야\n","\n","이별도 사랑이라고\n","\n","말해도\n","\n","더는 소용없는 거야\n","to_tokens: ['▁', '▁노', '단', '▁말', '▁없이', '▁의미', '▁없는', '▁', '▁', '▁걸', '▁', '도', '▁그런', '이란', '▁', '하던', '▁', '▁소용', '▁없는', '▁거야', '▁']\n","사랑한단 말도  아무 의미 없는 말인 거야\n","\n","이별도 사랑이라고\n","\n","말해도\n","\n","더는 소용없는 거야\n","epoch no.0 train no.13010  loss = 0.81598 avg_loss = 1.22510\n","epoch no.0 train no.13010  loss = 0.81598 avg_loss = 1.22510\n","epoch no.0 train no.13020  loss = 1.40270 avg_loss = 1.23484\n","epoch no.0 train no.13020  loss = 1.40270 avg_loss = 1.23484\n","epoch no.0 train no.13030  loss = 1.34721 avg_loss = 1.21387\n","epoch no.0 train no.13030  loss = 1.34721 avg_loss = 1.21387\n","epoch no.0 train no.13040  loss = 1.15976 avg_loss = 1.21707\n","epoch no.0 train no.13040  loss = 1.15976 avg_loss = 1.21707\n","epoch no.0 train no.13050  loss = 1.69199 avg_loss = 1.22547\n","epoch no.0 train no.13050  loss = 1.69199 avg_loss = 1.22547\n","epoch no.0 train no.13060  loss = 1.57288 avg_loss = 1.21952\n","epoch no.0 train no.13060  loss = 1.57288 avg_loss = 1.21952\n","epoch no.0 train no.13070  loss = 1.30064 avg_loss = 1.22141\n","epoch no.0 train no.13070  loss = 1.30064 avg_loss = 1.22141\n","epoch no.0 train no.13080  loss = 0.77296 avg_loss = 1.21529\n","epoch no.0 train no.13080  loss = 0.77296 avg_loss = 1.21529\n","epoch no.0 train no.13090  loss = 0.82496 avg_loss = 1.18949\n","epoch no.0 train no.13090  loss = 0.82496 avg_loss = 1.18949\n","epoch no.0 train no.13100  loss = 1.20399 avg_loss = 1.16932\n","epoch no.0 train no.13100  loss = 1.20399 avg_loss = 1.16932\n","epoch no.0 train no.13110  loss = 1.57392 avg_loss = 1.17734\n","epoch no.0 train no.13110  loss = 1.57392 avg_loss = 1.17734\n","epoch no.0 train no.13120  loss = 0.71124 avg_loss = 1.15145\n","epoch no.0 train no.13120  loss = 0.71124 avg_loss = 1.15145\n","epoch no.0 train no.13130  loss = 0.98762 avg_loss = 1.15674\n","epoch no.0 train no.13130  loss = 0.98762 avg_loss = 1.15674\n","epoch no.0 train no.13140  loss = 1.14053 avg_loss = 1.15555\n","epoch no.0 train no.13140  loss = 1.14053 avg_loss = 1.15555\n","epoch no.0 train no.13150  loss = 0.92060 avg_loss = 1.15040\n","epoch no.0 train no.13150  loss = 0.92060 avg_loss = 1.15040\n","epoch no.0 train no.13160  loss = 0.72330 avg_loss = 1.14007\n","epoch no.0 train no.13160  loss = 0.72330 avg_loss = 1.14007\n","epoch no.0 train no.13170  loss = 0.67179 avg_loss = 1.14989\n","epoch no.0 train no.13170  loss = 0.67179 avg_loss = 1.14989\n","epoch no.0 train no.13180  loss = 1.40367 avg_loss = 1.16902\n","epoch no.0 train no.13180  loss = 1.40367 avg_loss = 1.16902\n","epoch no.0 train no.13190  loss = 1.77166 avg_loss = 1.16466\n","epoch no.0 train no.13190  loss = 1.77166 avg_loss = 1.16466\n","epoch no.0 train no.13200  loss = 1.02576 avg_loss = 1.18495\n","epoch no.0 train no.13200  loss = 1.02576 avg_loss = 1.18495\n","epoch no.0 train no.13210  loss = 0.93087 avg_loss = 1.17593\n","epoch no.0 train no.13210  loss = 0.93087 avg_loss = 1.17593\n","epoch no.0 train no.13220  loss = 1.50237 avg_loss = 1.18245\n","epoch no.0 train no.13220  loss = 1.50237 avg_loss = 1.18245\n","epoch no.0 train no.13230  loss = 1.43420 avg_loss = 1.18402\n","epoch no.0 train no.13230  loss = 1.43420 avg_loss = 1.18402\n","epoch no.0 train no.13240  loss = 1.33031 avg_loss = 1.17630\n","epoch no.0 train no.13240  loss = 1.33031 avg_loss = 1.17630\n","epoch no.0 train no.13250  loss = 1.14771 avg_loss = 1.16740\n","epoch no.0 train no.13250  loss = 1.14771 avg_loss = 1.16740\n","epoch no.0 train no.13260  loss = 1.22624 avg_loss = 1.16117\n","epoch no.0 train no.13260  loss = 1.22624 avg_loss = 1.16117\n","epoch no.0 train no.13270  loss = 0.78908 avg_loss = 1.17738\n","epoch no.0 train no.13270  loss = 0.78908 avg_loss = 1.17738\n","epoch no.0 train no.13280  loss = 1.01034 avg_loss = 1.18695\n","epoch no.0 train no.13280  loss = 1.01034 avg_loss = 1.18695\n","epoch no.0 train no.13290  loss = 1.58028 avg_loss = 1.17900\n","epoch no.0 train no.13290  loss = 1.58028 avg_loss = 1.17900\n","epoch no.0 train no.13300  loss = 0.96218 avg_loss = 1.18289\n","epoch no.0 train no.13300  loss = 0.96218 avg_loss = 1.18289\n","epoch no.0 train no.13310  loss = 1.20827 avg_loss = 1.17203\n","epoch no.0 train no.13310  loss = 1.20827 avg_loss = 1.17203\n","epoch no.0 train no.13320  loss = 1.44940 avg_loss = 1.17159\n","epoch no.0 train no.13320  loss = 1.44940 avg_loss = 1.17159\n","epoch no.0 train no.13330  loss = 1.24945 avg_loss = 1.17661\n","epoch no.0 train no.13330  loss = 1.24945 avg_loss = 1.17661\n","epoch no.0 train no.13340  loss = 0.80307 avg_loss = 1.17019\n","epoch no.0 train no.13340  loss = 0.80307 avg_loss = 1.17019\n","epoch no.0 train no.13350  loss = 0.87086 avg_loss = 1.16827\n","epoch no.0 train no.13350  loss = 0.87086 avg_loss = 1.16827\n","epoch no.0 train no.13360  loss = 1.20417 avg_loss = 1.15308\n","epoch no.0 train no.13360  loss = 1.20417 avg_loss = 1.15308\n","epoch no.0 train no.13370  loss = 0.90690 avg_loss = 1.15296\n","epoch no.0 train no.13370  loss = 0.90690 avg_loss = 1.15296\n","epoch no.0 train no.13380  loss = 1.45145 avg_loss = 1.16554\n","epoch no.0 train no.13380  loss = 1.45145 avg_loss = 1.16554\n","epoch no.0 train no.13390  loss = 1.14331 avg_loss = 1.15821\n","epoch no.0 train no.13390  loss = 1.14331 avg_loss = 1.15821\n","epoch no.0 train no.13400  loss = 0.79727 avg_loss = 1.15534\n","epoch no.0 train no.13400  loss = 0.79727 avg_loss = 1.15534\n","epoch no.0 train no.13410  loss = 0.62058 avg_loss = 1.14697\n","epoch no.0 train no.13410  loss = 0.62058 avg_loss = 1.14697\n","epoch no.0 train no.13420  loss = 1.75559 avg_loss = 1.15832\n","epoch no.0 train no.13420  loss = 1.75559 avg_loss = 1.15832\n","epoch no.0 train no.13430  loss = 1.51053 avg_loss = 1.16600\n","epoch no.0 train no.13430  loss = 1.51053 avg_loss = 1.16600\n","epoch no.0 train no.13440  loss = 1.02104 avg_loss = 1.15170\n","epoch no.0 train no.13440  loss = 1.02104 avg_loss = 1.15170\n","epoch no.0 train no.13450  loss = 0.97965 avg_loss = 1.14781\n","epoch no.0 train no.13450  loss = 0.97965 avg_loss = 1.14781\n","epoch no.0 train no.13460  loss = 1.41058 avg_loss = 1.14356\n","epoch no.0 train no.13460  loss = 1.41058 avg_loss = 1.14356\n","epoch no.0 train no.13470  loss = 0.86829 avg_loss = 1.13645\n","epoch no.0 train no.13470  loss = 0.86829 avg_loss = 1.13645\n","epoch no.0 train no.13480  loss = 1.69328 avg_loss = 1.14860\n","epoch no.0 train no.13480  loss = 1.69328 avg_loss = 1.14860\n","epoch no.0 train no.13490  loss = 1.14478 avg_loss = 1.13502\n","epoch no.0 train no.13490  loss = 1.14478 avg_loss = 1.13502\n","epoch no.0 train no.13500  loss = 1.53707 avg_loss = 1.13499\n","epoch no.0 train no.13500  loss = 1.53707 avg_loss = 1.13499\n","epoch no.0 train no.13510  loss = 1.14626 avg_loss = 1.12680\n","epoch no.0 train no.13510  loss = 1.14626 avg_loss = 1.12680\n","epoch no.0 train no.13520  loss = 1.93891 avg_loss = 1.13201\n","epoch no.0 train no.13520  loss = 1.93891 avg_loss = 1.13201\n","epoch no.0 train no.13530  loss = 1.13139 avg_loss = 1.13168\n","epoch no.0 train no.13530  loss = 1.13139 avg_loss = 1.13168\n","epoch no.0 train no.13540  loss = 0.68913 avg_loss = 1.10966\n","epoch no.0 train no.13540  loss = 0.68913 avg_loss = 1.10966\n","epoch no.0 train no.13550  loss = 1.26437 avg_loss = 1.10839\n","epoch no.0 train no.13550  loss = 1.26437 avg_loss = 1.10839\n","epoch no.0 train no.13560  loss = 1.32109 avg_loss = 1.12248\n","epoch no.0 train no.13560  loss = 1.32109 avg_loss = 1.12248\n","epoch no.0 train no.13570  loss = 0.94638 avg_loss = 1.11110\n","epoch no.0 train no.13570  loss = 0.94638 avg_loss = 1.11110\n","epoch no.0 train no.13580  loss = 0.95425 avg_loss = 1.11093\n","epoch no.0 train no.13580  loss = 0.95425 avg_loss = 1.11093\n","epoch no.0 train no.13590  loss = 0.91948 avg_loss = 1.11037\n","epoch no.0 train no.13590  loss = 0.91948 avg_loss = 1.11037\n","epoch no.0 train no.13600  loss = 1.08259 avg_loss = 1.10419\n","epoch no.0 train no.13600  loss = 1.08259 avg_loss = 1.10419\n","epoch no.0 train no.13610  loss = 1.17598 avg_loss = 1.11570\n","epoch no.0 train no.13610  loss = 1.17598 avg_loss = 1.11570\n","epoch no.0 train no.13620  loss = 1.02678 avg_loss = 1.10972\n","epoch no.0 train no.13620  loss = 1.02678 avg_loss = 1.10972\n","epoch no.0 train no.13630  loss = 1.94878 avg_loss = 1.10639\n","epoch no.0 train no.13630  loss = 1.94878 avg_loss = 1.10639\n","epoch no.0 train no.13640  loss = 1.30733 avg_loss = 1.11837\n","epoch no.0 train no.13640  loss = 1.30733 avg_loss = 1.11837\n","epoch no.0 train no.13650  loss = 1.51604 avg_loss = 1.10256\n","epoch no.0 train no.13650  loss = 1.51604 avg_loss = 1.10256\n","epoch no.0 train no.13660  loss = 1.20065 avg_loss = 1.11324\n","epoch no.0 train no.13660  loss = 1.20065 avg_loss = 1.11324\n","epoch no.0 train no.13670  loss = 0.63098 avg_loss = 1.11327\n","epoch no.0 train no.13670  loss = 0.63098 avg_loss = 1.11327\n","epoch no.0 train no.13680  loss = 0.76420 avg_loss = 1.11108\n","epoch no.0 train no.13680  loss = 0.76420 avg_loss = 1.11108\n","epoch no.0 train no.13690  loss = 1.71392 avg_loss = 1.12141\n","epoch no.0 train no.13690  loss = 1.71392 avg_loss = 1.12141\n","epoch no.0 train no.13700  loss = 1.08396 avg_loss = 1.12111\n","epoch no.0 train no.13700  loss = 1.08396 avg_loss = 1.12111\n","epoch no.0 train no.13710  loss = 1.00806 avg_loss = 1.14590\n","epoch no.0 train no.13710  loss = 1.00806 avg_loss = 1.14590\n","epoch no.0 train no.13720  loss = 1.17638 avg_loss = 1.12792\n","epoch no.0 train no.13720  loss = 1.17638 avg_loss = 1.12792\n","epoch no.0 train no.13730  loss = 0.80024 avg_loss = 1.12140\n","epoch no.0 train no.13730  loss = 0.80024 avg_loss = 1.12140\n","epoch no.0 train no.13740  loss = 1.64546 avg_loss = 1.14110\n","epoch no.0 train no.13740  loss = 1.64546 avg_loss = 1.14110\n","epoch no.0 train no.13750  loss = 1.08171 avg_loss = 1.14944\n","epoch no.0 train no.13750  loss = 1.08171 avg_loss = 1.14944\n","epoch no.0 train no.13760  loss = 0.89849 avg_loss = 1.14200\n","epoch no.0 train no.13760  loss = 0.89849 avg_loss = 1.14200\n","epoch no.0 train no.13770  loss = 0.93589 avg_loss = 1.14903\n","epoch no.0 train no.13770  loss = 0.93589 avg_loss = 1.14903\n","epoch no.0 train no.13780  loss = 0.96303 avg_loss = 1.13849\n","epoch no.0 train no.13780  loss = 0.96303 avg_loss = 1.13849\n","epoch no.0 train no.13790  loss = 1.77622 avg_loss = 1.14862\n","epoch no.0 train no.13790  loss = 1.77622 avg_loss = 1.14862\n","epoch no.0 train no.13800  loss = 1.29712 avg_loss = 1.15395\n","epoch no.0 train no.13800  loss = 1.29712 avg_loss = 1.15395\n","epoch no.0 train no.13810  loss = 0.85338 avg_loss = 1.15175\n","epoch no.0 train no.13810  loss = 0.85338 avg_loss = 1.15175\n","epoch no.0 train no.13820  loss = 1.73454 avg_loss = 1.14743\n","epoch no.0 train no.13820  loss = 1.73454 avg_loss = 1.14743\n","epoch no.0 train no.13830  loss = 1.26465 avg_loss = 1.14204\n","epoch no.0 train no.13830  loss = 1.26465 avg_loss = 1.14204\n","epoch no.0 train no.13840  loss = 0.97125 avg_loss = 1.15149\n","epoch no.0 train no.13840  loss = 0.97125 avg_loss = 1.15149\n","epoch no.0 train no.13850  loss = 0.86099 avg_loss = 1.15433\n","epoch no.0 train no.13850  loss = 0.86099 avg_loss = 1.15433\n","epoch no.0 train no.13860  loss = 0.85140 avg_loss = 1.15500\n","epoch no.0 train no.13860  loss = 0.85140 avg_loss = 1.15500\n","epoch no.0 train no.13870  loss = 1.32226 avg_loss = 1.16768\n","epoch no.0 train no.13870  loss = 1.32226 avg_loss = 1.16768\n","epoch no.0 train no.13880  loss = 1.26169 avg_loss = 1.15638\n","epoch no.0 train no.13880  loss = 1.26169 avg_loss = 1.15638\n","epoch no.0 train no.13890  loss = 0.62701 avg_loss = 1.14459\n","epoch no.0 train no.13890  loss = 0.62701 avg_loss = 1.14459\n","epoch no.0 train no.13900  loss = 1.44117 avg_loss = 1.13735\n","epoch no.0 train no.13900  loss = 1.44117 avg_loss = 1.13735\n","epoch no.0 train no.13910  loss = 0.79757 avg_loss = 1.13104\n","epoch no.0 train no.13910  loss = 0.79757 avg_loss = 1.13104\n","epoch no.0 train no.13920  loss = 0.82939 avg_loss = 1.11968\n","epoch no.0 train no.13920  loss = 0.82939 avg_loss = 1.11968\n","epoch no.0 train no.13930  loss = 1.12195 avg_loss = 1.11645\n","epoch no.0 train no.13930  loss = 1.12195 avg_loss = 1.11645\n","epoch no.0 train no.13940  loss = 0.51701 avg_loss = 1.12474\n","epoch no.0 train no.13940  loss = 0.51701 avg_loss = 1.12474\n","epoch no.0 train no.13950  loss = 0.73666 avg_loss = 1.12618\n","epoch no.0 train no.13950  loss = 0.73666 avg_loss = 1.12618\n","epoch no.0 train no.13960  loss = 1.79277 avg_loss = 1.13045\n","epoch no.0 train no.13960  loss = 1.79277 avg_loss = 1.13045\n","epoch no.0 train no.13970  loss = 2.23237 avg_loss = 1.14347\n","epoch no.0 train no.13970  loss = 2.23237 avg_loss = 1.14347\n","epoch no.0 train no.13980  loss = 1.10700 avg_loss = 1.13537\n","epoch no.0 train no.13980  loss = 1.10700 avg_loss = 1.13537\n","epoch no.0 train no.13990  loss = 1.54445 avg_loss = 1.13504\n","epoch no.0 train no.13990  loss = 1.54445 avg_loss = 1.13504\n","epoch no.0 train no.14000  loss = 0.64694 avg_loss = 1.12541\n","epoch no.0 train no.14000  loss = 0.64694 avg_loss = 1.12541\n","to_tokens: ['▁', '해요', '▁수', '▁있다면', '▁', '▁널', '▁', '▁', 'e', '▁to', '▁m', '▁', 'I', 'll', 've', '▁you', ')', 'ore', '▁you', ')', '▁b', 'ab', 'y', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁(', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁(', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁(', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 've', '▁you', '▁lo', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁m', 've', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 've', '▁you', '▁m', 've', '▁you', '▁lo', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁m', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', 've', '▁you', '▁lo', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', 've', '▁you', '▁lo', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', 've', '▁you', ')', '▁', 'I', '▁you', '▁lo', 'ant', '▁it', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁', 'If', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁', 'If', '▁w', 've', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁', 'w', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁', 'I', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁', 'ant', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'oo', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁', 'ant', '▁you', '▁w', 'ant', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'oo', '▁you', 'oo', '▁you', '▁you', 'h', '▁w', '▁w', 'ant', '▁w', 'ant', '▁you', '▁w', '▁w', 'u', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'u', 'h', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁w', 'ant', '▁', 'ant', '▁w', '▁w', 'ant', '▁you', 'ea', '▁w', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ea', '▁w', 'ant', '▁y', 'ant', '▁w', '▁w', '▁', 'h', 'ant', '▁you', '▁w', 'oo', '▁y', 'u', '▁w', 'u', '▁w', 'ant', '▁y', '▁w', 'u', '▁w', 'ant', '▁w', '▁w', 'u', '▁w', 'u', 'u', '▁w', '▁w', '▁w', '▁w', '▁w', 'ant', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', '▁w', 'u', 'u', 'u', '▁w', 'u', 'u', 'u', 'u', 'u', '▁w', 'u', 'u', 'u', 'u', 'u', 'u', 'u', '▁w', 'u', '▁w', 'u', 'u', 'u', '▁w', 'u', '▁w', 'u', 'u', '▁w', 'u', '▁', 'u', 'u']\n","사랑할 수 없어, 난 .come today\n","\n","(I love you miss you, baby)\n","\n","(I love you miss you miss you) (I love you miss you miss you)  (I love you miss you miss you) (I love you miss you miss you) (I love you miss you) (I love you miss you miss you)  (I love you miss you miss you)\n","\n","(I love you miss you miss you) (I love you miss you)\n","\n","(I love you miss you miss you)\n","\n","(I love you miss you miss you)\n","\n","(I love you miss you miss you) (I love you miss you) (I love you miss you miss you)\n","\n","(I love you miss you miss you miss you)\n","\n","(I love you miss you miss you)\n","\n","(I love you miss you miss you)  (I love you love you miss you)\n","\n","(I love you love you love you miss you)\n","\n","(I love you love you love you)\n","\n","(I love you love you love you love you) (I love you love you love you love you)\n","\n","(I love you love you love love you) (I love you love you love you love you love you)\n","\n","(I love you love you love you love you love you)  (I love you love you love you love you love you love you)  (If you want you want you want you to want you want you want you want you)\n","\n","(If you want you want you want you want you want you want you want you want you want you want you want you want you want you)\n","\n","(I love you want you want you want you want you want you want you want you want want you want you want you want you want you want you want you want you want you want)\n","\n","(I want you want you want)\n","\n","(I want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you)  want you want you want you want want you woo want want you want you want  you want you want want you want)  woo want you want you want  you want you want you want you want you want y want you want  you want want)\n","\n","want you woo want you want you want you want you want you want want  you want you woo you want you want you want  you want you want want want yeah you woo want  yu want you want you want you want you want you want you want yeah you want you woo  want want you want yuu want  you want yu wan wuuh u want you want yuhu want you wu wuu wuuhuuuuu wu wu wu wu wu wu wuu wu wu wuu wuu wuu wuu uu wu uu wu wu wu u wu wu wu w\n","to_tokens: ['▁', '해요', '▁수', '▁있다면', '▁', '▁널', '▁', '▁', 'e', '▁to', '▁m', '▁', 'I', 'll', 've', '▁you', ')', 'ore', '▁you', ')', '▁b', 'ab', 'y', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁(', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁(', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁(', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 'iss', '▁you', '▁m', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 've', '▁you', '▁lo', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁m', 've', '▁you', ')', 'iss', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁m', 've', '▁you', '▁m', 've', '▁you', '▁lo', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁m', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', 've', '▁you', '▁lo', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', 've', '▁you', '▁lo', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', '▁', 'I', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', '▁lo', 've', '▁you', ')', 've', '▁you', ')', '▁', 'I', '▁you', '▁lo', 'ant', '▁it', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁', 'If', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁', 'If', '▁w', 've', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁', 'w', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁', 'I', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', '▁', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁', 'ant', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'oo', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁', 'ant', '▁you', '▁w', 'ant', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'oo', '▁you', 'oo', '▁you', '▁you', 'h', '▁w', '▁w', 'ant', '▁w', 'ant', '▁you', '▁w', '▁w', 'u', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'u', 'h', '▁you', '▁w', 'ant', '▁you', '▁w', 'ant', '▁w', 'ant', '▁', 'ant', '▁w', '▁w', 'ant', '▁you', 'ea', '▁w', '▁w', 'ant', '▁you', '▁w', 'ant', '▁you', 'ea', '▁w', 'ant', '▁y', 'ant', '▁w', '▁w', '▁', 'h', 'ant', '▁you', '▁w', 'oo', '▁y', 'u', '▁w', 'u', '▁w', 'ant', '▁y', '▁w', 'u', '▁w', 'ant', '▁w', '▁w', 'u', '▁w', 'u', 'u', '▁w', '▁w', '▁w', '▁w', '▁w', 'ant', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', '▁w', 'u', 'u', 'u', '▁w', 'u', 'u', 'u', 'u', 'u', '▁w', 'u', 'u', 'u', 'u', 'u', 'u', 'u', '▁w', 'u', '▁w', 'u', 'u', 'u', '▁w', 'u', '▁w', 'u', 'u', '▁w', 'u', '▁', 'u', 'u']\n","사랑할 수 없어, 난 .come today\n","\n","(I love you miss you, baby)\n","\n","(I love you miss you miss you) (I love you miss you miss you)  (I love you miss you miss you) (I love you miss you miss you) (I love you miss you) (I love you miss you miss you)  (I love you miss you miss you)\n","\n","(I love you miss you miss you) (I love you miss you)\n","\n","(I love you miss you miss you)\n","\n","(I love you miss you miss you)\n","\n","(I love you miss you miss you) (I love you miss you) (I love you miss you miss you)\n","\n","(I love you miss you miss you miss you)\n","\n","(I love you miss you miss you)\n","\n","(I love you miss you miss you)  (I love you love you miss you)\n","\n","(I love you love you love you miss you)\n","\n","(I love you love you love you)\n","\n","(I love you love you love you love you) (I love you love you love you love you)\n","\n","(I love you love you love love you) (I love you love you love you love you love you)\n","\n","(I love you love you love you love you love you)  (I love you love you love you love you love you love you)  (If you want you want you want you to want you want you want you want you)\n","\n","(If you want you want you want you want you want you want you want you want you want you want you want you want you want you)\n","\n","(I love you want you want you want you want you want you want you want you want want you want you want you want you want you want you want you want you want you want)\n","\n","(I want you want you want)\n","\n","(I want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you want you)  want you want you want you want want you woo want want you want you want  you want you want want you want)  woo want you want you want  you want you want you want you want you want y want you want  you want want)\n","\n","want you woo want you want you want you want you want you want want  you want you woo you want you want you want  you want you want want want yeah you woo want  yu want you want you want you want you want you want you want yeah you want you woo  want want you want yuu want  you want yu wan wuuh u want you want yuhu want you wu wuu wuuhuuuuu wu wu wu wu wu wu wuu wu wu wuu wuu wuu wuu uu wu uu wu wu wu u wu wu wu w\n","epoch no.0 train no.14010  loss = 1.82264 avg_loss = 1.12964\n","epoch no.0 train no.14010  loss = 1.82264 avg_loss = 1.12964\n","epoch no.0 train no.14020  loss = 2.28097 avg_loss = 1.14756\n","epoch no.0 train no.14020  loss = 2.28097 avg_loss = 1.14756\n","epoch no.0 train no.14030  loss = 1.41441 avg_loss = 1.14823\n","epoch no.0 train no.14030  loss = 1.41441 avg_loss = 1.14823\n","epoch no.0 train no.14040  loss = 1.16729 avg_loss = 1.14587\n","epoch no.0 train no.14040  loss = 1.16729 avg_loss = 1.14587\n","epoch no.0 train no.14050  loss = 1.47544 avg_loss = 1.14615\n","epoch no.0 train no.14050  loss = 1.47544 avg_loss = 1.14615\n","epoch no.0 train no.14060  loss = 0.78550 avg_loss = 1.14714\n","epoch no.0 train no.14060  loss = 0.78550 avg_loss = 1.14714\n","epoch no.0 train no.14070  loss = 1.20664 avg_loss = 1.15100\n","epoch no.0 train no.14070  loss = 1.20664 avg_loss = 1.15100\n","epoch no.0 train no.14080  loss = 1.01028 avg_loss = 1.15206\n","epoch no.0 train no.14080  loss = 1.01028 avg_loss = 1.15206\n","epoch no.0 train no.14090  loss = 1.15584 avg_loss = 1.14661\n","epoch no.0 train no.14090  loss = 1.15584 avg_loss = 1.14661\n","epoch no.0 train no.14100  loss = 1.48170 avg_loss = 1.15491\n","epoch no.0 train no.14100  loss = 1.48170 avg_loss = 1.15491\n","epoch no.0 train no.14110  loss = 1.44137 avg_loss = 1.14254\n","epoch no.0 train no.14110  loss = 1.44137 avg_loss = 1.14254\n","epoch no.0 train no.14120  loss = 1.35161 avg_loss = 1.13579\n","epoch no.0 train no.14120  loss = 1.35161 avg_loss = 1.13579\n","epoch no.0 train no.14130  loss = 1.10056 avg_loss = 1.12874\n","epoch no.0 train no.14130  loss = 1.10056 avg_loss = 1.12874\n","epoch no.0 train no.14140  loss = 1.37574 avg_loss = 1.14353\n","epoch no.0 train no.14140  loss = 1.37574 avg_loss = 1.14353\n","epoch no.0 train no.14150  loss = 1.15727 avg_loss = 1.13853\n","epoch no.0 train no.14150  loss = 1.15727 avg_loss = 1.13853\n","epoch no.0 train no.14160  loss = 1.37360 avg_loss = 1.14475\n","epoch no.0 train no.14160  loss = 1.37360 avg_loss = 1.14475\n","epoch no.0 train no.14170  loss = 1.16708 avg_loss = 1.14563\n","epoch no.0 train no.14170  loss = 1.16708 avg_loss = 1.14563\n","epoch no.0 train no.14180  loss = 1.25676 avg_loss = 1.15185\n","epoch no.0 train no.14180  loss = 1.25676 avg_loss = 1.15185\n","epoch no.0 train no.14190  loss = 1.05046 avg_loss = 1.15281\n","epoch no.0 train no.14190  loss = 1.05046 avg_loss = 1.15281\n","epoch no.0 train no.14200  loss = 1.15940 avg_loss = 1.16143\n","epoch no.0 train no.14200  loss = 1.15940 avg_loss = 1.16143\n","epoch no.0 train no.14210  loss = 1.71736 avg_loss = 1.16445\n","epoch no.0 train no.14210  loss = 1.71736 avg_loss = 1.16445\n","epoch no.0 train no.14220  loss = 0.92244 avg_loss = 1.16479\n","epoch no.0 train no.14220  loss = 0.92244 avg_loss = 1.16479\n","epoch no.0 train no.14230  loss = 1.34125 avg_loss = 1.16263\n","epoch no.0 train no.14230  loss = 1.34125 avg_loss = 1.16263\n","epoch no.0 train no.14240  loss = 0.73102 avg_loss = 1.14724\n","epoch no.0 train no.14240  loss = 0.73102 avg_loss = 1.14724\n","epoch no.0 train no.14250  loss = 1.30111 avg_loss = 1.17868\n","epoch no.0 train no.14250  loss = 1.30111 avg_loss = 1.17868\n","epoch no.0 train no.14260  loss = 1.35421 avg_loss = 1.17470\n","epoch no.0 train no.14260  loss = 1.35421 avg_loss = 1.17470\n","epoch no.0 train no.14270  loss = 0.51769 avg_loss = 1.15663\n","epoch no.0 train no.14270  loss = 0.51769 avg_loss = 1.15663\n","epoch no.0 train no.14280  loss = 1.14295 avg_loss = 1.15241\n","epoch no.0 train no.14280  loss = 1.14295 avg_loss = 1.15241\n","epoch no.0 train no.14290  loss = 1.55452 avg_loss = 1.14989\n","epoch no.0 train no.14290  loss = 1.55452 avg_loss = 1.14989\n","epoch no.0 train no.14300  loss = 0.85408 avg_loss = 1.11938\n","epoch no.0 train no.14300  loss = 0.85408 avg_loss = 1.11938\n","epoch no.0 train no.14310  loss = 1.46211 avg_loss = 1.10599\n","epoch no.0 train no.14310  loss = 1.46211 avg_loss = 1.10599\n","epoch no.0 train no.14320  loss = 0.94380 avg_loss = 1.12500\n","epoch no.0 train no.14320  loss = 0.94380 avg_loss = 1.12500\n","epoch no.0 train no.14330  loss = 0.45573 avg_loss = 1.11891\n","epoch no.0 train no.14330  loss = 0.45573 avg_loss = 1.11891\n","epoch no.0 train no.14340  loss = 0.85833 avg_loss = 1.12848\n","epoch no.0 train no.14340  loss = 0.85833 avg_loss = 1.12848\n","epoch no.0 train no.14350  loss = 0.97059 avg_loss = 1.13086\n","epoch no.0 train no.14350  loss = 0.97059 avg_loss = 1.13086\n","epoch no.0 train no.14360  loss = 2.17590 avg_loss = 1.15379\n","epoch no.0 train no.14360  loss = 2.17590 avg_loss = 1.15379\n","epoch no.0 train no.14370  loss = 0.73762 avg_loss = 1.14454\n","epoch no.0 train no.14370  loss = 0.73762 avg_loss = 1.14454\n","epoch no.0 train no.14380  loss = 1.12327 avg_loss = 1.15249\n","epoch no.0 train no.14380  loss = 1.12327 avg_loss = 1.15249\n","epoch no.0 train no.14390  loss = 1.85290 avg_loss = 1.17900\n","epoch no.0 train no.14390  loss = 1.85290 avg_loss = 1.17900\n","epoch no.0 train no.14400  loss = 1.79783 avg_loss = 1.17391\n","epoch no.0 train no.14400  loss = 1.79783 avg_loss = 1.17391\n","epoch no.0 train no.14410  loss = 0.72813 avg_loss = 1.15828\n","epoch no.0 train no.14410  loss = 0.72813 avg_loss = 1.15828\n","epoch no.0 train no.14420  loss = 1.52586 avg_loss = 1.15171\n","epoch no.0 train no.14420  loss = 1.52586 avg_loss = 1.15171\n","epoch no.0 train no.14430  loss = 1.08321 avg_loss = 1.16659\n","epoch no.0 train no.14430  loss = 1.08321 avg_loss = 1.16659\n","epoch no.0 train no.14440  loss = 0.72575 avg_loss = 1.16270\n","epoch no.0 train no.14440  loss = 0.72575 avg_loss = 1.16270\n","epoch no.0 train no.14450  loss = 1.16873 avg_loss = 1.17255\n","epoch no.0 train no.14450  loss = 1.16873 avg_loss = 1.17255\n","epoch no.0 train no.14460  loss = 1.26566 avg_loss = 1.19036\n","epoch no.0 train no.14460  loss = 1.26566 avg_loss = 1.19036\n","epoch no.0 train no.14470  loss = 1.01082 avg_loss = 1.19965\n","epoch no.0 train no.14470  loss = 1.01082 avg_loss = 1.19965\n","epoch no.0 train no.14480  loss = 1.18807 avg_loss = 1.19773\n","epoch no.0 train no.14480  loss = 1.18807 avg_loss = 1.19773\n","epoch no.0 train no.14490  loss = 1.26567 avg_loss = 1.18306\n","epoch no.0 train no.14490  loss = 1.26567 avg_loss = 1.18306\n","epoch no.0 train no.14500  loss = 1.14878 avg_loss = 1.17241\n","epoch no.0 train no.14500  loss = 1.14878 avg_loss = 1.17241\n","epoch no.0 train no.14510  loss = 0.74446 avg_loss = 1.17928\n","epoch no.0 train no.14510  loss = 0.74446 avg_loss = 1.17928\n","epoch no.0 train no.14520  loss = 0.85704 avg_loss = 1.16635\n","epoch no.0 train no.14520  loss = 0.85704 avg_loss = 1.16635\n","epoch no.0 train no.14530  loss = 0.58026 avg_loss = 1.15321\n","epoch no.0 train no.14530  loss = 0.58026 avg_loss = 1.15321\n","epoch no.0 train no.14540  loss = 0.54898 avg_loss = 1.14187\n","epoch no.0 train no.14540  loss = 0.54898 avg_loss = 1.14187\n","epoch no.0 train no.14550  loss = 2.19040 avg_loss = 1.14609\n","epoch no.0 train no.14550  loss = 2.19040 avg_loss = 1.14609\n","epoch no.0 train no.14560  loss = 1.51136 avg_loss = 1.14325\n","epoch no.0 train no.14560  loss = 1.51136 avg_loss = 1.14325\n","epoch no.0 train no.14570  loss = 1.19916 avg_loss = 1.13981\n","epoch no.0 train no.14570  loss = 1.19916 avg_loss = 1.13981\n","epoch no.0 train no.14580  loss = 0.60359 avg_loss = 1.13368\n","epoch no.0 train no.14580  loss = 0.60359 avg_loss = 1.13368\n","epoch no.0 train no.14590  loss = 0.89198 avg_loss = 1.14194\n","epoch no.0 train no.14590  loss = 0.89198 avg_loss = 1.14194\n","epoch no.0 train no.14600  loss = 1.43371 avg_loss = 1.13358\n","epoch no.0 train no.14600  loss = 1.43371 avg_loss = 1.13358\n","epoch no.0 train no.14610  loss = 1.40433 avg_loss = 1.13462\n","epoch no.0 train no.14610  loss = 1.40433 avg_loss = 1.13462\n","epoch no.0 train no.14620  loss = 0.97128 avg_loss = 1.14194\n","epoch no.0 train no.14620  loss = 0.97128 avg_loss = 1.14194\n","epoch no.0 train no.14630  loss = 1.37075 avg_loss = 1.14888\n","epoch no.0 train no.14630  loss = 1.37075 avg_loss = 1.14888\n","epoch no.0 train no.14640  loss = 1.53497 avg_loss = 1.15142\n","epoch no.0 train no.14640  loss = 1.53497 avg_loss = 1.15142\n","epoch no.0 train no.14650  loss = 0.48748 avg_loss = 1.12827\n","epoch no.0 train no.14650  loss = 0.48748 avg_loss = 1.12827\n","epoch no.0 train no.14660  loss = 1.82743 avg_loss = 1.12205\n","epoch no.0 train no.14660  loss = 1.82743 avg_loss = 1.12205\n","epoch no.0 train no.14670  loss = 1.34595 avg_loss = 1.12230\n","epoch no.0 train no.14670  loss = 1.34595 avg_loss = 1.12230\n","epoch no.0 train no.14680  loss = 1.01820 avg_loss = 1.13898\n","epoch no.0 train no.14680  loss = 1.01820 avg_loss = 1.13898\n","epoch no.0 train no.14690  loss = 1.81550 avg_loss = 1.12981\n","epoch no.0 train no.14690  loss = 1.81550 avg_loss = 1.12981\n","epoch no.0 train no.14700  loss = 1.36660 avg_loss = 1.11772\n","epoch no.0 train no.14700  loss = 1.36660 avg_loss = 1.11772\n","epoch no.0 train no.14710  loss = 1.38027 avg_loss = 1.12811\n","epoch no.0 train no.14710  loss = 1.38027 avg_loss = 1.12811\n","epoch no.0 train no.14720  loss = 0.96611 avg_loss = 1.13838\n","epoch no.0 train no.14720  loss = 0.96611 avg_loss = 1.13838\n","epoch no.0 train no.14730  loss = 1.33817 avg_loss = 1.13791\n","epoch no.0 train no.14730  loss = 1.33817 avg_loss = 1.13791\n","epoch no.0 train no.14740  loss = 0.92034 avg_loss = 1.13072\n","epoch no.0 train no.14740  loss = 0.92034 avg_loss = 1.13072\n","epoch no.0 train no.14750  loss = 1.10885 avg_loss = 1.13878\n","epoch no.0 train no.14750  loss = 1.10885 avg_loss = 1.13878\n","epoch no.0 train no.14760  loss = 1.56134 avg_loss = 1.13660\n","epoch no.0 train no.14760  loss = 1.56134 avg_loss = 1.13660\n","epoch no.0 train no.14770  loss = 1.58503 avg_loss = 1.14373\n","epoch no.0 train no.14770  loss = 1.58503 avg_loss = 1.14373\n","epoch no.0 train no.14780  loss = 1.21136 avg_loss = 1.15052\n","epoch no.0 train no.14780  loss = 1.21136 avg_loss = 1.15052\n","epoch no.0 train no.14790  loss = 0.45151 avg_loss = 1.13917\n","epoch no.0 train no.14790  loss = 0.45151 avg_loss = 1.13917\n","epoch no.0 train no.14800  loss = 1.18055 avg_loss = 1.12742\n","epoch no.0 train no.14800  loss = 1.18055 avg_loss = 1.12742\n","epoch no.0 train no.14810  loss = 0.46664 avg_loss = 1.11722\n","epoch no.0 train no.14810  loss = 0.46664 avg_loss = 1.11722\n","epoch no.0 train no.14820  loss = 1.21490 avg_loss = 1.13195\n","epoch no.0 train no.14820  loss = 1.21490 avg_loss = 1.13195\n","epoch no.0 train no.14830  loss = 0.69163 avg_loss = 1.11615\n","epoch no.0 train no.14830  loss = 0.69163 avg_loss = 1.11615\n","epoch no.0 train no.14840  loss = 1.75485 avg_loss = 1.12425\n","epoch no.0 train no.14840  loss = 1.75485 avg_loss = 1.12425\n","epoch no.0 train no.14850  loss = 1.07974 avg_loss = 1.14440\n","epoch no.0 train no.14850  loss = 1.07974 avg_loss = 1.14440\n","epoch no.0 train no.14860  loss = 0.49449 avg_loss = 1.14258\n","epoch no.0 train no.14860  loss = 0.49449 avg_loss = 1.14258\n","epoch no.0 train no.14870  loss = 1.14341 avg_loss = 1.15867\n","epoch no.0 train no.14870  loss = 1.14341 avg_loss = 1.15867\n","epoch no.0 train no.14880  loss = 1.52596 avg_loss = 1.16436\n","epoch no.0 train no.14880  loss = 1.52596 avg_loss = 1.16436\n","epoch no.0 train no.14890  loss = 1.53264 avg_loss = 1.17164\n","epoch no.0 train no.14890  loss = 1.53264 avg_loss = 1.17164\n","epoch no.0 train no.14900  loss = 1.53421 avg_loss = 1.18208\n","epoch no.0 train no.14900  loss = 1.53421 avg_loss = 1.18208\n","epoch no.0 train no.14910  loss = 1.19826 avg_loss = 1.17068\n","epoch no.0 train no.14910  loss = 1.19826 avg_loss = 1.17068\n","epoch no.0 train no.14920  loss = 0.86850 avg_loss = 1.16720\n","epoch no.0 train no.14920  loss = 0.86850 avg_loss = 1.16720\n","epoch no.0 train no.14930  loss = 1.22250 avg_loss = 1.14613\n","epoch no.0 train no.14930  loss = 1.22250 avg_loss = 1.14613\n","epoch no.0 train no.14940  loss = 0.90325 avg_loss = 1.12057\n","epoch no.0 train no.14940  loss = 0.90325 avg_loss = 1.12057\n","epoch no.0 train no.14950  loss = 0.83471 avg_loss = 1.13032\n","epoch no.0 train no.14950  loss = 0.83471 avg_loss = 1.13032\n","epoch no.0 train no.14960  loss = 0.70145 avg_loss = 1.13158\n","epoch no.0 train no.14960  loss = 0.70145 avg_loss = 1.13158\n","epoch no.0 train no.14970  loss = 1.01462 avg_loss = 1.12234\n","epoch no.0 train no.14970  loss = 1.01462 avg_loss = 1.12234\n","epoch no.0 train no.14980  loss = 1.17879 avg_loss = 1.13276\n","epoch no.0 train no.14980  loss = 1.17879 avg_loss = 1.13276\n","epoch no.0 train no.14990  loss = 1.13845 avg_loss = 1.13949\n","epoch no.0 train no.14990  loss = 1.13845 avg_loss = 1.13949\n","epoch no.0 train no.15000  loss = 0.57028 avg_loss = 1.15447\n","epoch no.0 train no.15000  loss = 0.57028 avg_loss = 1.15447\n","to_tokens: ['▁', '해요', '단', '▁말도', '▁', '▁되는', '▁그', '▁', '도', '종', '▁', '리는', '▁', '얼', '▁', '▁나갔', '지', '▁', 'it', 's', '▁n', 'ot', '▁an', '▁', 'mo', 'ist', 'o', '▁', '▁g', '▁g', 'ame', 'ls', 'riend', '▁', 'ay', '▁', 'S', 's', '▁n', 'ot', '▁e', 'as', '▁a', '▁li', 'ar', '\"', '▁']\n","사랑한단 말도  안 되는  가요  계의 분위기에  휩쓸려\n","\n","얼른 정신이 나갔어\n","\n",",its not\n","\n","an exisousy\n","\n","girlfriend\n","\n","said \"its not even a lie\"\n","\n","\n","to_tokens: ['▁', '해요', '단', '▁말도', '▁', '▁되는', '▁그', '▁', '도', '종', '▁', '리는', '▁', '얼', '▁', '▁나갔', '지', '▁', 'it', 's', '▁n', 'ot', '▁an', '▁', 'mo', 'ist', 'o', '▁', '▁g', '▁g', 'ame', 'ls', 'riend', '▁', 'ay', '▁', 'S', 's', '▁n', 'ot', '▁e', 'as', '▁a', '▁li', 'ar', '\"', '▁']\n","사랑한단 말도  안 되는  가요  계의 분위기에  휩쓸려\n","\n","얼른 정신이 나갔어\n","\n",",its not\n","\n","an exisousy\n","\n","girlfriend\n","\n","said \"its not even a lie\"\n","\n","\n","epoch no.0 train no.15010  loss = 0.88926 avg_loss = 1.17365\n","epoch no.0 train no.15010  loss = 0.88926 avg_loss = 1.17365\n","epoch no.0 train no.15020  loss = 1.45573 avg_loss = 1.18139\n","epoch no.0 train no.15020  loss = 1.45573 avg_loss = 1.18139\n","epoch no.0 train no.15030  loss = 1.31683 avg_loss = 1.18940\n","epoch no.0 train no.15030  loss = 1.31683 avg_loss = 1.18940\n","epoch no.0 train no.15040  loss = 0.95662 avg_loss = 1.16454\n","epoch no.0 train no.15040  loss = 0.95662 avg_loss = 1.16454\n","epoch no.0 train no.15050  loss = 1.00670 avg_loss = 1.14451\n","epoch no.0 train no.15050  loss = 1.00670 avg_loss = 1.14451\n","epoch no.0 train no.15060  loss = 0.99644 avg_loss = 1.14157\n","epoch no.0 train no.15060  loss = 0.99644 avg_loss = 1.14157\n","epoch no.0 train no.15070  loss = 0.79954 avg_loss = 1.13741\n","epoch no.0 train no.15070  loss = 0.79954 avg_loss = 1.13741\n","epoch no.0 train no.15080  loss = 0.80290 avg_loss = 1.14598\n","epoch no.0 train no.15080  loss = 0.80290 avg_loss = 1.14598\n","epoch no.0 train no.15090  loss = 0.69641 avg_loss = 1.13650\n","epoch no.0 train no.15090  loss = 0.69641 avg_loss = 1.13650\n","epoch no.0 train no.15100  loss = 1.08314 avg_loss = 1.12341\n","epoch no.0 train no.15100  loss = 1.08314 avg_loss = 1.12341\n","epoch no.0 train no.15110  loss = 1.50485 avg_loss = 1.14436\n","epoch no.0 train no.15110  loss = 1.50485 avg_loss = 1.14436\n","epoch no.0 train no.15120  loss = 0.74083 avg_loss = 1.12284\n","epoch no.0 train no.15120  loss = 0.74083 avg_loss = 1.12284\n","epoch no.0 train no.15130  loss = 0.45979 avg_loss = 1.11358\n","epoch no.0 train no.15130  loss = 0.45979 avg_loss = 1.11358\n","epoch no.0 train no.15140  loss = 0.93171 avg_loss = 1.10635\n","epoch no.0 train no.15140  loss = 0.93171 avg_loss = 1.10635\n","epoch no.0 train no.15150  loss = 0.63329 avg_loss = 1.10507\n","epoch no.0 train no.15150  loss = 0.63329 avg_loss = 1.10507\n","epoch no.0 train no.15160  loss = 1.04766 avg_loss = 1.14581\n","epoch no.0 train no.15160  loss = 1.04766 avg_loss = 1.14581\n","epoch no.0 train no.15170  loss = 1.36615 avg_loss = 1.14295\n","epoch no.0 train no.15170  loss = 1.36615 avg_loss = 1.14295\n","epoch no.0 train no.15180  loss = 0.89704 avg_loss = 1.14495\n","epoch no.0 train no.15180  loss = 0.89704 avg_loss = 1.14495\n","epoch no.0 train no.15190  loss = 1.08725 avg_loss = 1.13951\n","epoch no.0 train no.15190  loss = 1.08725 avg_loss = 1.13951\n","epoch no.0 train no.15200  loss = 1.69217 avg_loss = 1.12693\n","epoch no.0 train no.15200  loss = 1.69217 avg_loss = 1.12693\n","epoch no.0 train no.15210  loss = 1.14150 avg_loss = 1.11813\n","epoch no.0 train no.15210  loss = 1.14150 avg_loss = 1.11813\n","epoch no.0 train no.15220  loss = 1.00149 avg_loss = 1.13296\n","epoch no.0 train no.15220  loss = 1.00149 avg_loss = 1.13296\n","epoch no.0 train no.15230  loss = 1.02268 avg_loss = 1.14566\n","epoch no.0 train no.15230  loss = 1.02268 avg_loss = 1.14566\n","epoch no.0 train no.15240  loss = 1.01157 avg_loss = 1.14273\n","epoch no.0 train no.15240  loss = 1.01157 avg_loss = 1.14273\n","epoch no.0 train no.15250  loss = 0.76770 avg_loss = 1.14809\n","epoch no.0 train no.15250  loss = 0.76770 avg_loss = 1.14809\n","epoch no.0 train no.15260  loss = 1.10359 avg_loss = 1.14989\n","epoch no.0 train no.15260  loss = 1.10359 avg_loss = 1.14989\n","epoch no.0 train no.15270  loss = 0.83706 avg_loss = 1.15339\n","epoch no.0 train no.15270  loss = 0.83706 avg_loss = 1.15339\n","epoch no.0 train no.15280  loss = 1.30575 avg_loss = 1.14782\n","epoch no.0 train no.15280  loss = 1.30575 avg_loss = 1.14782\n","epoch no.0 train no.15290  loss = 1.09293 avg_loss = 1.13558\n","epoch no.0 train no.15290  loss = 1.09293 avg_loss = 1.13558\n","epoch no.0 train no.15300  loss = 1.39089 avg_loss = 1.14934\n","epoch no.0 train no.15300  loss = 1.39089 avg_loss = 1.14934\n","epoch no.0 train no.15310  loss = 0.91289 avg_loss = 1.12991\n","epoch no.0 train no.15310  loss = 0.91289 avg_loss = 1.12991\n","epoch no.0 train no.15320  loss = 1.27250 avg_loss = 1.13531\n","epoch no.0 train no.15320  loss = 1.27250 avg_loss = 1.13531\n","epoch no.0 train no.15330  loss = 1.05174 avg_loss = 1.13929\n","epoch no.0 train no.15330  loss = 1.05174 avg_loss = 1.13929\n","epoch no.0 train no.15340  loss = 0.64861 avg_loss = 1.14069\n","epoch no.0 train no.15340  loss = 0.64861 avg_loss = 1.14069\n","epoch no.0 train no.15350  loss = 1.36521 avg_loss = 1.13321\n","epoch no.0 train no.15350  loss = 1.36521 avg_loss = 1.13321\n","epoch no.0 train no.15360  loss = 0.48282 avg_loss = 1.11662\n","epoch no.0 train no.15360  loss = 0.48282 avg_loss = 1.11662\n","epoch no.0 train no.15370  loss = 1.12684 avg_loss = 1.13050\n","epoch no.0 train no.15370  loss = 1.12684 avg_loss = 1.13050\n","epoch no.0 train no.15380  loss = 1.91695 avg_loss = 1.15178\n","epoch no.0 train no.15380  loss = 1.91695 avg_loss = 1.15178\n","epoch no.0 train no.15390  loss = 1.54366 avg_loss = 1.14555\n","epoch no.0 train no.15390  loss = 1.54366 avg_loss = 1.14555\n","epoch no.0 train no.15400  loss = 1.48511 avg_loss = 1.13541\n","epoch no.0 train no.15400  loss = 1.48511 avg_loss = 1.13541\n","epoch no.0 train no.15410  loss = 1.00727 avg_loss = 1.12995\n","epoch no.0 train no.15410  loss = 1.00727 avg_loss = 1.12995\n","epoch no.0 train no.15420  loss = 1.56973 avg_loss = 1.12631\n","epoch no.0 train no.15420  loss = 1.56973 avg_loss = 1.12631\n","epoch no.0 train no.15430  loss = 1.76847 avg_loss = 1.13269\n","epoch no.0 train no.15430  loss = 1.76847 avg_loss = 1.13269\n","epoch no.0 train no.15440  loss = 0.76737 avg_loss = 1.12924\n","epoch no.0 train no.15440  loss = 0.76737 avg_loss = 1.12924\n","epoch no.0 train no.15450  loss = 1.05992 avg_loss = 1.13386\n","epoch no.0 train no.15450  loss = 1.05992 avg_loss = 1.13386\n","epoch no.0 train no.15460  loss = 0.89318 avg_loss = 1.13366\n","epoch no.0 train no.15460  loss = 0.89318 avg_loss = 1.13366\n","epoch no.0 train no.15470  loss = 0.73368 avg_loss = 1.13691\n","epoch no.0 train no.15470  loss = 0.73368 avg_loss = 1.13691\n","epoch no.0 train no.15480  loss = 0.63736 avg_loss = 1.13383\n","epoch no.0 train no.15480  loss = 0.63736 avg_loss = 1.13383\n","epoch no.0 train no.15490  loss = 0.56719 avg_loss = 1.14123\n","epoch no.0 train no.15490  loss = 0.56719 avg_loss = 1.14123\n","epoch no.0 train no.15500  loss = 0.91363 avg_loss = 1.13030\n","epoch no.0 train no.15500  loss = 0.91363 avg_loss = 1.13030\n","epoch no.0 train no.15510  loss = 1.01359 avg_loss = 1.12021\n","epoch no.0 train no.15510  loss = 1.01359 avg_loss = 1.12021\n","epoch no.0 train no.15520  loss = 1.51965 avg_loss = 1.14534\n","epoch no.0 train no.15520  loss = 1.51965 avg_loss = 1.14534\n","epoch no.0 train no.15530  loss = 0.89202 avg_loss = 1.13294\n","epoch no.0 train no.15530  loss = 0.89202 avg_loss = 1.13294\n","epoch no.0 train no.15540  loss = 1.43721 avg_loss = 1.13033\n","epoch no.0 train no.15540  loss = 1.43721 avg_loss = 1.13033\n","epoch no.0 train no.15550  loss = 1.10816 avg_loss = 1.14198\n","epoch no.0 train no.15550  loss = 1.10816 avg_loss = 1.14198\n","epoch no.0 train no.15560  loss = 1.36193 avg_loss = 1.14960\n","epoch no.0 train no.15560  loss = 1.36193 avg_loss = 1.14960\n","epoch no.0 train no.15570  loss = 0.30591 avg_loss = 1.15419\n","epoch no.0 train no.15570  loss = 0.30591 avg_loss = 1.15419\n","epoch no.0 train no.15580  loss = 0.37976 avg_loss = 1.15034\n","epoch no.0 train no.15580  loss = 0.37976 avg_loss = 1.15034\n","epoch no.0 train no.15590  loss = 1.06535 avg_loss = 1.14559\n","epoch no.0 train no.15590  loss = 1.06535 avg_loss = 1.14559\n","epoch no.0 train no.15600  loss = 0.80947 avg_loss = 1.13998\n","epoch no.0 train no.15600  loss = 0.80947 avg_loss = 1.13998\n","epoch no.0 train no.15610  loss = 0.71835 avg_loss = 1.13933\n","epoch no.0 train no.15610  loss = 0.71835 avg_loss = 1.13933\n","epoch no.0 train no.15620  loss = 1.51470 avg_loss = 1.14916\n","epoch no.0 train no.15620  loss = 1.51470 avg_loss = 1.14916\n","epoch no.0 train no.15630  loss = 0.82323 avg_loss = 1.14470\n","epoch no.0 train no.15630  loss = 0.82323 avg_loss = 1.14470\n","epoch no.0 train no.15640  loss = 0.95438 avg_loss = 1.15033\n","epoch no.0 train no.15640  loss = 0.95438 avg_loss = 1.15033\n","epoch no.0 train no.15650  loss = 0.63559 avg_loss = 1.13650\n","epoch no.0 train no.15650  loss = 0.63559 avg_loss = 1.13650\n","epoch no.0 train no.15660  loss = 1.07804 avg_loss = 1.13738\n","epoch no.0 train no.15660  loss = 1.07804 avg_loss = 1.13738\n","epoch no.0 train no.15670  loss = 0.44921 avg_loss = 1.11881\n","epoch no.0 train no.15670  loss = 0.44921 avg_loss = 1.11881\n","epoch no.0 train no.15680  loss = 1.14835 avg_loss = 1.13374\n","epoch no.0 train no.15680  loss = 1.14835 avg_loss = 1.13374\n","epoch no.0 train no.15690  loss = 1.82328 avg_loss = 1.14591\n","epoch no.0 train no.15690  loss = 1.82328 avg_loss = 1.14591\n","epoch no.0 train no.15700  loss = 1.36868 avg_loss = 1.14884\n","epoch no.0 train no.15700  loss = 1.36868 avg_loss = 1.14884\n","epoch no.0 train no.15710  loss = 1.28838 avg_loss = 1.14278\n","epoch no.0 train no.15710  loss = 1.28838 avg_loss = 1.14278\n","epoch no.0 train no.15720  loss = 1.41263 avg_loss = 1.15603\n","epoch no.0 train no.15720  loss = 1.41263 avg_loss = 1.15603\n","epoch no.0 train no.15730  loss = 1.33406 avg_loss = 1.15480\n","epoch no.0 train no.15730  loss = 1.33406 avg_loss = 1.15480\n","epoch no.0 train no.15740  loss = 1.35933 avg_loss = 1.15491\n","epoch no.0 train no.15740  loss = 1.35933 avg_loss = 1.15491\n","epoch no.0 train no.15750  loss = 1.44510 avg_loss = 1.15049\n","epoch no.0 train no.15750  loss = 1.44510 avg_loss = 1.15049\n","epoch no.0 train no.15760  loss = 1.62543 avg_loss = 1.16016\n","epoch no.0 train no.15760  loss = 1.62543 avg_loss = 1.16016\n","epoch no.0 train no.15770  loss = 0.88366 avg_loss = 1.14691\n","epoch no.0 train no.15770  loss = 0.88366 avg_loss = 1.14691\n","epoch no.0 train no.15780  loss = 1.09633 avg_loss = 1.14459\n","epoch no.0 train no.15780  loss = 1.09633 avg_loss = 1.14459\n","epoch no.0 train no.15790  loss = 1.06670 avg_loss = 1.14755\n","epoch no.0 train no.15790  loss = 1.06670 avg_loss = 1.14755\n","epoch no.0 train no.15800  loss = 1.24489 avg_loss = 1.14836\n","epoch no.0 train no.15800  loss = 1.24489 avg_loss = 1.14836\n","epoch no.0 train no.15810  loss = 1.41586 avg_loss = 1.15703\n","epoch no.0 train no.15810  loss = 1.41586 avg_loss = 1.15703\n","epoch no.0 train no.15820  loss = 0.95935 avg_loss = 1.15153\n","epoch no.0 train no.15820  loss = 0.95935 avg_loss = 1.15153\n","epoch no.0 train no.15830  loss = 0.91392 avg_loss = 1.15475\n","epoch no.0 train no.15830  loss = 0.91392 avg_loss = 1.15475\n","epoch no.0 train no.15840  loss = 0.96530 avg_loss = 1.13101\n","epoch no.0 train no.15840  loss = 0.96530 avg_loss = 1.13101\n","epoch no.0 train no.15850  loss = 0.95316 avg_loss = 1.11484\n","epoch no.0 train no.15850  loss = 0.95316 avg_loss = 1.11484\n","epoch no.0 train no.15860  loss = 1.72195 avg_loss = 1.10692\n","epoch no.0 train no.15860  loss = 1.72195 avg_loss = 1.10692\n","epoch no.0 train no.15870  loss = 0.96625 avg_loss = 1.10843\n","epoch no.0 train no.15870  loss = 0.96625 avg_loss = 1.10843\n","epoch no.0 train no.15880  loss = 1.55639 avg_loss = 1.10894\n","epoch no.0 train no.15880  loss = 1.55639 avg_loss = 1.10894\n","epoch no.0 train no.15890  loss = 1.04620 avg_loss = 1.10320\n","epoch no.0 train no.15890  loss = 1.04620 avg_loss = 1.10320\n","epoch no.0 train no.15900  loss = 0.72770 avg_loss = 1.09640\n","epoch no.0 train no.15900  loss = 0.72770 avg_loss = 1.09640\n","epoch no.0 train no.15910  loss = 0.83734 avg_loss = 1.09178\n","epoch no.0 train no.15910  loss = 0.83734 avg_loss = 1.09178\n","epoch no.0 train no.15920  loss = 1.19290 avg_loss = 1.09429\n","epoch no.0 train no.15920  loss = 1.19290 avg_loss = 1.09429\n","epoch no.0 train no.15930  loss = 0.58497 avg_loss = 1.09631\n","epoch no.0 train no.15930  loss = 0.58497 avg_loss = 1.09631\n","epoch no.0 train no.15940  loss = 0.89782 avg_loss = 1.11567\n","epoch no.0 train no.15940  loss = 0.89782 avg_loss = 1.11567\n","epoch no.0 train no.15950  loss = 1.65351 avg_loss = 1.11047\n","epoch no.0 train no.15950  loss = 1.65351 avg_loss = 1.11047\n","epoch no.0 train no.15960  loss = 1.14388 avg_loss = 1.10309\n","epoch no.0 train no.15960  loss = 1.14388 avg_loss = 1.10309\n","epoch no.0 train no.15970  loss = 0.95097 avg_loss = 1.09528\n","epoch no.0 train no.15970  loss = 0.95097 avg_loss = 1.09528\n","epoch no.0 train no.15980  loss = 1.96025 avg_loss = 1.12198\n","epoch no.0 train no.15980  loss = 1.96025 avg_loss = 1.12198\n","epoch no.0 train no.15990  loss = 1.12498 avg_loss = 1.11800\n","epoch no.0 train no.15990  loss = 1.12498 avg_loss = 1.11800\n","epoch no.0 train no.16000  loss = 1.29945 avg_loss = 1.13187\n","epoch no.0 train no.16000  loss = 1.29945 avg_loss = 1.13187\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁이메일', '▁', '▁다', '▁확인', '▁', '▁', '▁Li', '첩', 'ell', '놓을', '도', '▁베스트', '습니다', '▁임', '▁이제', '가', '▁이', '▁마', 'i', '▁', 'm', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n",", 이메일\n","\n","싹 다 넣어 둬\n","\n",", 사진 다 터쳐\n","\n",", 음악이다, 너는 하지 minimum\n","\n","\n","to_tokens: ['▁', '해요', '랠', '▁부를', '까', '▁봐', '▁이제', ',', '▁워', '밍', '업', '▁사', '버린', '▁기대', '▁', '만', '▁떠', '드는', '▁랩', '퍼', '들', '▁비', '일', '비', '재', '▁후에', '▁작업', '▁D', 'M', '▁', '▁이메일', '▁', '▁다', '▁확인', '▁', '▁', '▁Li', '첩', 'ell', '놓을', '도', '▁베스트', '습니다', '▁임', '▁이제', '가', '▁이', '▁마', 'i', '▁', 'm', '▁']\n","사랑 노랠 부를까 봐 이제, 워밍업 사버린 기대\n","\n","말로만 떠드는 랩퍼들 비일비재 후에 작업 DM\n","\n",", 이메일\n","\n","싹 다 넣어 둬\n","\n",", 사진 다 터쳐\n","\n",", 음악이다, 너는 하지 minimum\n","\n","\n","epoch no.0 train no.16010  loss = 1.83909 avg_loss = 1.16307\n","epoch no.0 train no.16010  loss = 1.83909 avg_loss = 1.16307\n","epoch no.0 train no.16020  loss = 1.49712 avg_loss = 1.17605\n","epoch no.0 train no.16020  loss = 1.49712 avg_loss = 1.17605\n","epoch no.0 train no.16030  loss = 0.70767 avg_loss = 1.18092\n","epoch no.0 train no.16030  loss = 0.70767 avg_loss = 1.18092\n","epoch no.0 train no.16040  loss = 1.29110 avg_loss = 1.17804\n","epoch no.0 train no.16040  loss = 1.29110 avg_loss = 1.17804\n","epoch no.0 train no.16050  loss = 1.06307 avg_loss = 1.17311\n","epoch no.0 train no.16050  loss = 1.06307 avg_loss = 1.17311\n","epoch no.0 train no.16060  loss = 0.78571 avg_loss = 1.18636\n","epoch no.0 train no.16060  loss = 0.78571 avg_loss = 1.18636\n","epoch no.0 train no.16070  loss = 1.39212 avg_loss = 1.19478\n","epoch no.0 train no.16070  loss = 1.39212 avg_loss = 1.19478\n","epoch no.0 train no.16080  loss = 1.09851 avg_loss = 1.19707\n","epoch no.0 train no.16080  loss = 1.09851 avg_loss = 1.19707\n","epoch no.0 train no.16090  loss = 0.85166 avg_loss = 1.18319\n","epoch no.0 train no.16090  loss = 0.85166 avg_loss = 1.18319\n","epoch no.0 train no.16100  loss = 1.52839 avg_loss = 1.17385\n","epoch no.0 train no.16100  loss = 1.52839 avg_loss = 1.17385\n","epoch no.0 train no.16110  loss = 1.35004 avg_loss = 1.18118\n","epoch no.0 train no.16110  loss = 1.35004 avg_loss = 1.18118\n","epoch no.0 train no.16120  loss = 0.85475 avg_loss = 1.17338\n","epoch no.0 train no.16120  loss = 0.85475 avg_loss = 1.17338\n","epoch no.0 train no.16130  loss = 0.81920 avg_loss = 1.19298\n","epoch no.0 train no.16130  loss = 0.81920 avg_loss = 1.19298\n","epoch no.0 train no.16140  loss = 1.47890 avg_loss = 1.19556\n","epoch no.0 train no.16140  loss = 1.47890 avg_loss = 1.19556\n","epoch no.0 train no.16150  loss = 1.47974 avg_loss = 1.19003\n","epoch no.0 train no.16150  loss = 1.47974 avg_loss = 1.19003\n","epoch no.0 train no.16160  loss = 1.29872 avg_loss = 1.18350\n","epoch no.0 train no.16160  loss = 1.29872 avg_loss = 1.18350\n","epoch no.0 train no.16170  loss = 1.13803 avg_loss = 1.17069\n","epoch no.0 train no.16170  loss = 1.13803 avg_loss = 1.17069\n","epoch no.0 train no.16180  loss = 1.13157 avg_loss = 1.16974\n","epoch no.0 train no.16180  loss = 1.13157 avg_loss = 1.16974\n","epoch no.0 train no.16190  loss = 1.21612 avg_loss = 1.15769\n","epoch no.0 train no.16190  loss = 1.21612 avg_loss = 1.15769\n","epoch no.0 train no.16200  loss = 1.22952 avg_loss = 1.15672\n","epoch no.0 train no.16200  loss = 1.22952 avg_loss = 1.15672\n","epoch no.0 train no.16210  loss = 0.79975 avg_loss = 1.14125\n","epoch no.0 train no.16210  loss = 0.79975 avg_loss = 1.14125\n","epoch no.0 train no.16220  loss = 1.15306 avg_loss = 1.12046\n","epoch no.0 train no.16220  loss = 1.15306 avg_loss = 1.12046\n","epoch no.0 train no.16230  loss = 1.25875 avg_loss = 1.11420\n","epoch no.0 train no.16230  loss = 1.25875 avg_loss = 1.11420\n","epoch no.0 train no.16240  loss = 0.83836 avg_loss = 1.11009\n","epoch no.0 train no.16240  loss = 0.83836 avg_loss = 1.11009\n","epoch no.0 train no.16250  loss = 0.97933 avg_loss = 1.11475\n","epoch no.0 train no.16250  loss = 0.97933 avg_loss = 1.11475\n","epoch no.0 train no.16260  loss = 0.64840 avg_loss = 1.11988\n","epoch no.0 train no.16260  loss = 0.64840 avg_loss = 1.11988\n","epoch no.0 train no.16270  loss = 1.83898 avg_loss = 1.12239\n","epoch no.0 train no.16270  loss = 1.83898 avg_loss = 1.12239\n","epoch no.0 train no.16280  loss = 1.58642 avg_loss = 1.12198\n","epoch no.0 train no.16280  loss = 1.58642 avg_loss = 1.12198\n","epoch no.0 train no.16290  loss = 1.07128 avg_loss = 1.11721\n","epoch no.0 train no.16290  loss = 1.07128 avg_loss = 1.11721\n","epoch no.0 train no.16300  loss = 1.34618 avg_loss = 1.11036\n","epoch no.0 train no.16300  loss = 1.34618 avg_loss = 1.11036\n","epoch no.0 train no.16310  loss = 1.18528 avg_loss = 1.12228\n","epoch no.0 train no.16310  loss = 1.18528 avg_loss = 1.12228\n","epoch no.0 train no.16320  loss = 1.15274 avg_loss = 1.11585\n","epoch no.0 train no.16320  loss = 1.15274 avg_loss = 1.11585\n","epoch no.0 train no.16330  loss = 1.09502 avg_loss = 1.11385\n","epoch no.0 train no.16330  loss = 1.09502 avg_loss = 1.11385\n","epoch no.0 train no.16340  loss = 1.19607 avg_loss = 1.11786\n","epoch no.0 train no.16340  loss = 1.19607 avg_loss = 1.11786\n","epoch no.0 train no.16350  loss = 1.95275 avg_loss = 1.13490\n","epoch no.0 train no.16350  loss = 1.95275 avg_loss = 1.13490\n","epoch no.0 train no.16360  loss = 1.09918 avg_loss = 1.13309\n","epoch no.0 train no.16360  loss = 1.09918 avg_loss = 1.13309\n","epoch no.0 train no.16370  loss = 1.35935 avg_loss = 1.14223\n","epoch no.0 train no.16370  loss = 1.35935 avg_loss = 1.14223\n","epoch no.0 train no.16380  loss = 0.97111 avg_loss = 1.14083\n","epoch no.0 train no.16380  loss = 0.97111 avg_loss = 1.14083\n","epoch no.0 train no.16390  loss = 2.10810 avg_loss = 1.14402\n","epoch no.0 train no.16390  loss = 2.10810 avg_loss = 1.14402\n","epoch no.0 train no.16400  loss = 1.59902 avg_loss = 1.16228\n","epoch no.0 train no.16400  loss = 1.59902 avg_loss = 1.16228\n","epoch no.0 train no.16410  loss = 0.79019 avg_loss = 1.15383\n","epoch no.0 train no.16410  loss = 0.79019 avg_loss = 1.15383\n","epoch no.0 train no.16420  loss = 0.85949 avg_loss = 1.14116\n","epoch no.0 train no.16420  loss = 0.85949 avg_loss = 1.14116\n","epoch no.0 train no.16430  loss = 1.11539 avg_loss = 1.12968\n","epoch no.0 train no.16430  loss = 1.11539 avg_loss = 1.12968\n","epoch no.0 train no.16440  loss = 1.37414 avg_loss = 1.12010\n","epoch no.0 train no.16440  loss = 1.37414 avg_loss = 1.12010\n","epoch no.0 train no.16450  loss = 0.69757 avg_loss = 1.10058\n","epoch no.0 train no.16450  loss = 0.69757 avg_loss = 1.10058\n","epoch no.0 train no.16460  loss = 1.18053 avg_loss = 1.12964\n","epoch no.0 train no.16460  loss = 1.18053 avg_loss = 1.12964\n","epoch no.0 train no.16470  loss = 1.20145 avg_loss = 1.13877\n","epoch no.0 train no.16470  loss = 1.20145 avg_loss = 1.13877\n","epoch no.0 train no.16480  loss = 1.12768 avg_loss = 1.14650\n","epoch no.0 train no.16480  loss = 1.12768 avg_loss = 1.14650\n","epoch no.0 train no.16490  loss = 1.62440 avg_loss = 1.15829\n","epoch no.0 train no.16490  loss = 1.62440 avg_loss = 1.15829\n","epoch no.0 train no.16500  loss = 1.10832 avg_loss = 1.13823\n","epoch no.0 train no.16500  loss = 1.10832 avg_loss = 1.13823\n","epoch no.0 train no.16510  loss = 1.19206 avg_loss = 1.14509\n","epoch no.0 train no.16510  loss = 1.19206 avg_loss = 1.14509\n","epoch no.0 train no.16520  loss = 0.79408 avg_loss = 1.14521\n","epoch no.0 train no.16520  loss = 0.79408 avg_loss = 1.14521\n","epoch no.0 train no.16530  loss = 1.07496 avg_loss = 1.13691\n","epoch no.0 train no.16530  loss = 1.07496 avg_loss = 1.13691\n","epoch no.0 train no.16540  loss = 1.02833 avg_loss = 1.12293\n","epoch no.0 train no.16540  loss = 1.02833 avg_loss = 1.12293\n","epoch no.0 train no.16550  loss = 1.09441 avg_loss = 1.12675\n","epoch no.0 train no.16550  loss = 1.09441 avg_loss = 1.12675\n","epoch no.0 train no.16560  loss = 1.71425 avg_loss = 1.13148\n","epoch no.0 train no.16560  loss = 1.71425 avg_loss = 1.13148\n","epoch no.0 train no.16570  loss = 0.94609 avg_loss = 1.12474\n","epoch no.0 train no.16570  loss = 0.94609 avg_loss = 1.12474\n","epoch no.0 train no.16580  loss = 1.11367 avg_loss = 1.14777\n","epoch no.0 train no.16580  loss = 1.11367 avg_loss = 1.14777\n","epoch no.0 train no.16590  loss = 1.00334 avg_loss = 1.14030\n","epoch no.0 train no.16590  loss = 1.00334 avg_loss = 1.14030\n","epoch no.0 train no.16600  loss = 0.45324 avg_loss = 1.13376\n","epoch no.0 train no.16600  loss = 0.45324 avg_loss = 1.13376\n","epoch no.0 train no.16610  loss = 1.75245 avg_loss = 1.13537\n","epoch no.0 train no.16610  loss = 1.75245 avg_loss = 1.13537\n","epoch no.0 train no.16620  loss = 1.13786 avg_loss = 1.13621\n","epoch no.0 train no.16620  loss = 1.13786 avg_loss = 1.13621\n","epoch no.0 train no.16630  loss = 0.77913 avg_loss = 1.13242\n","epoch no.0 train no.16630  loss = 0.77913 avg_loss = 1.13242\n","epoch no.0 train no.16640  loss = 0.78781 avg_loss = 1.12190\n","epoch no.0 train no.16640  loss = 0.78781 avg_loss = 1.12190\n","epoch no.0 train no.16650  loss = 0.87600 avg_loss = 1.11956\n","epoch no.0 train no.16650  loss = 0.87600 avg_loss = 1.11956\n","epoch no.0 train no.16660  loss = 1.54496 avg_loss = 1.13437\n","epoch no.0 train no.16660  loss = 1.54496 avg_loss = 1.13437\n","epoch no.0 train no.16670  loss = 1.00457 avg_loss = 1.12390\n","epoch no.0 train no.16670  loss = 1.00457 avg_loss = 1.12390\n","epoch no.0 train no.16680  loss = 1.05940 avg_loss = 1.11836\n","epoch no.0 train no.16680  loss = 1.05940 avg_loss = 1.11836\n","epoch no.0 train no.16690  loss = 0.95212 avg_loss = 1.13445\n","epoch no.0 train no.16690  loss = 0.95212 avg_loss = 1.13445\n","epoch no.0 train no.16700  loss = 1.62756 avg_loss = 1.13300\n","epoch no.0 train no.16700  loss = 1.62756 avg_loss = 1.13300\n","epoch no.0 train no.16710  loss = 2.31619 avg_loss = 1.15053\n","epoch no.0 train no.16710  loss = 2.31619 avg_loss = 1.15053\n","epoch no.0 train no.16720  loss = 1.12993 avg_loss = 1.15034\n","epoch no.0 train no.16720  loss = 1.12993 avg_loss = 1.15034\n","epoch no.0 train no.16730  loss = 0.98002 avg_loss = 1.15675\n","epoch no.0 train no.16730  loss = 0.98002 avg_loss = 1.15675\n","epoch no.0 train no.16740  loss = 1.19293 avg_loss = 1.18088\n","epoch no.0 train no.16740  loss = 1.19293 avg_loss = 1.18088\n","epoch no.0 train no.16750  loss = 1.11408 avg_loss = 1.17326\n","epoch no.0 train no.16750  loss = 1.11408 avg_loss = 1.17326\n","epoch no.0 train no.16760  loss = 1.47193 avg_loss = 1.17739\n","epoch no.0 train no.16760  loss = 1.47193 avg_loss = 1.17739\n","epoch no.0 train no.16770  loss = 1.27150 avg_loss = 1.18859\n","epoch no.0 train no.16770  loss = 1.27150 avg_loss = 1.18859\n","epoch no.0 train no.16780  loss = 0.80449 avg_loss = 1.18002\n","epoch no.0 train no.16780  loss = 0.80449 avg_loss = 1.18002\n","epoch no.0 train no.16790  loss = 1.16972 avg_loss = 1.17480\n","epoch no.0 train no.16790  loss = 1.16972 avg_loss = 1.17480\n","epoch no.0 train no.16800  loss = 0.99034 avg_loss = 1.17812\n","epoch no.0 train no.16800  loss = 0.99034 avg_loss = 1.17812\n","epoch no.0 train no.16810  loss = 1.40798 avg_loss = 1.16213\n","epoch no.0 train no.16810  loss = 1.40798 avg_loss = 1.16213\n","epoch no.0 train no.16820  loss = 1.52779 avg_loss = 1.17667\n","epoch no.0 train no.16820  loss = 1.52779 avg_loss = 1.17667\n","epoch no.0 train no.16830  loss = 1.83193 avg_loss = 1.17692\n","epoch no.0 train no.16830  loss = 1.83193 avg_loss = 1.17692\n","epoch no.0 train no.16840  loss = 0.80957 avg_loss = 1.18228\n","epoch no.0 train no.16840  loss = 0.80957 avg_loss = 1.18228\n","epoch no.0 train no.16850  loss = 0.60680 avg_loss = 1.15442\n","epoch no.0 train no.16850  loss = 0.60680 avg_loss = 1.15442\n","epoch no.0 train no.16860  loss = 0.36238 avg_loss = 1.15363\n","epoch no.0 train no.16860  loss = 0.36238 avg_loss = 1.15363\n","epoch no.0 train no.16870  loss = 1.48741 avg_loss = 1.16191\n","epoch no.0 train no.16870  loss = 1.48741 avg_loss = 1.16191\n","epoch no.0 train no.16880  loss = 0.93220 avg_loss = 1.15124\n","epoch no.0 train no.16880  loss = 0.93220 avg_loss = 1.15124\n","epoch no.0 train no.16890  loss = 0.26869 avg_loss = 1.14830\n","epoch no.0 train no.16890  loss = 0.26869 avg_loss = 1.14830\n","epoch no.0 train no.16900  loss = 1.48076 avg_loss = 1.15748\n","epoch no.0 train no.16900  loss = 1.48076 avg_loss = 1.15748\n","epoch no.0 train no.16910  loss = 1.27303 avg_loss = 1.14749\n","epoch no.0 train no.16910  loss = 1.27303 avg_loss = 1.14749\n","epoch no.0 train no.16920  loss = 1.46117 avg_loss = 1.14102\n","epoch no.0 train no.16920  loss = 1.46117 avg_loss = 1.14102\n","epoch no.0 train no.16930  loss = 1.20494 avg_loss = 1.13668\n","epoch no.0 train no.16930  loss = 1.20494 avg_loss = 1.13668\n","epoch no.0 train no.16940  loss = 0.62029 avg_loss = 1.13766\n","epoch no.0 train no.16940  loss = 0.62029 avg_loss = 1.13766\n","epoch no.0 train no.16950  loss = 1.43836 avg_loss = 1.13557\n","epoch no.0 train no.16950  loss = 1.43836 avg_loss = 1.13557\n","epoch no.0 train no.16960  loss = 1.92169 avg_loss = 1.14139\n","epoch no.0 train no.16960  loss = 1.92169 avg_loss = 1.14139\n","epoch no.0 train no.16970  loss = 1.09037 avg_loss = 1.13878\n","epoch no.0 train no.16970  loss = 1.09037 avg_loss = 1.13878\n","epoch no.0 train no.16980  loss = 1.65999 avg_loss = 1.13204\n","epoch no.0 train no.16980  loss = 1.65999 avg_loss = 1.13204\n","epoch no.0 train no.16990  loss = 1.33119 avg_loss = 1.13375\n","epoch no.0 train no.16990  loss = 1.33119 avg_loss = 1.13375\n","epoch no.0 train no.17000  loss = 0.68567 avg_loss = 1.12020\n","epoch no.0 train no.17000  loss = 0.68567 avg_loss = 1.12020\n","to_tokens: ['▁', '해요', '▁수', '▁있게', '▁']\n","사랑할 수 있나요\n","to_tokens: ['▁', '해요', '▁수', '▁있게', '▁']\n","사랑할 수 있나요\n","epoch no.0 train no.17010  loss = 0.81776 avg_loss = 1.11883\n","epoch no.0 train no.17010  loss = 0.81776 avg_loss = 1.11883\n","epoch no.0 train no.17020  loss = 1.01999 avg_loss = 1.11017\n","epoch no.0 train no.17020  loss = 1.01999 avg_loss = 1.11017\n","epoch no.0 train no.17030  loss = 1.38802 avg_loss = 1.11953\n","epoch no.0 train no.17030  loss = 1.38802 avg_loss = 1.11953\n","epoch no.0 train no.17040  loss = 1.16727 avg_loss = 1.11565\n","epoch no.0 train no.17040  loss = 1.16727 avg_loss = 1.11565\n","epoch no.0 train no.17050  loss = 0.93814 avg_loss = 1.11308\n","epoch no.0 train no.17050  loss = 0.93814 avg_loss = 1.11308\n","epoch no.0 train no.17060  loss = 1.64018 avg_loss = 1.12088\n","epoch no.0 train no.17060  loss = 1.64018 avg_loss = 1.12088\n","epoch no.0 train no.17070  loss = 1.15347 avg_loss = 1.12077\n","epoch no.0 train no.17070  loss = 1.15347 avg_loss = 1.12077\n","epoch no.0 train no.17080  loss = 1.00939 avg_loss = 1.10645\n","epoch no.0 train no.17080  loss = 1.00939 avg_loss = 1.10645\n","epoch no.0 train no.17090  loss = 1.15555 avg_loss = 1.10182\n","epoch no.0 train no.17090  loss = 1.15555 avg_loss = 1.10182\n","epoch no.0 train no.17100  loss = 1.20986 avg_loss = 1.10312\n","epoch no.0 train no.17100  loss = 1.20986 avg_loss = 1.10312\n","epoch no.0 train no.17110  loss = 1.22529 avg_loss = 1.10846\n","epoch no.0 train no.17110  loss = 1.22529 avg_loss = 1.10846\n","epoch no.0 train no.17120  loss = 1.30994 avg_loss = 1.11112\n","epoch no.0 train no.17120  loss = 1.30994 avg_loss = 1.11112\n","epoch no.0 train no.17130  loss = 0.75178 avg_loss = 1.10889\n","epoch no.0 train no.17130  loss = 0.75178 avg_loss = 1.10889\n","epoch no.0 train no.17140  loss = 1.27069 avg_loss = 1.11989\n","epoch no.0 train no.17140  loss = 1.27069 avg_loss = 1.11989\n","epoch no.0 train no.17150  loss = 1.15417 avg_loss = 1.13922\n","epoch no.0 train no.17150  loss = 1.15417 avg_loss = 1.13922\n","epoch no.0 train no.17160  loss = 0.85345 avg_loss = 1.12656\n","epoch no.0 train no.17160  loss = 0.85345 avg_loss = 1.12656\n","epoch no.0 train no.17170  loss = 0.43559 avg_loss = 1.12111\n","epoch no.0 train no.17170  loss = 0.43559 avg_loss = 1.12111\n","epoch no.0 train no.17180  loss = 1.59997 avg_loss = 1.12701\n","epoch no.0 train no.17180  loss = 1.59997 avg_loss = 1.12701\n","epoch no.0 train no.17190  loss = 1.64719 avg_loss = 1.12279\n","epoch no.0 train no.17190  loss = 1.64719 avg_loss = 1.12279\n","epoch no.0 train no.17200  loss = 1.57808 avg_loss = 1.12711\n","epoch no.0 train no.17200  loss = 1.57808 avg_loss = 1.12711\n","epoch no.0 train no.17210  loss = 1.01121 avg_loss = 1.11755\n","epoch no.0 train no.17210  loss = 1.01121 avg_loss = 1.11755\n","epoch no.0 train no.17220  loss = 0.47595 avg_loss = 1.13602\n","epoch no.0 train no.17220  loss = 0.47595 avg_loss = 1.13602\n","epoch no.0 train no.17230  loss = 1.55388 avg_loss = 1.13098\n","epoch no.0 train no.17230  loss = 1.55388 avg_loss = 1.13098\n","epoch no.0 train no.17240  loss = 1.19211 avg_loss = 1.13189\n","epoch no.0 train no.17240  loss = 1.19211 avg_loss = 1.13189\n","epoch no.0 train no.17250  loss = 1.16869 avg_loss = 1.14129\n","epoch no.0 train no.17250  loss = 1.16869 avg_loss = 1.14129\n","epoch no.0 train no.17260  loss = 1.20349 avg_loss = 1.14196\n","epoch no.0 train no.17260  loss = 1.20349 avg_loss = 1.14196\n","epoch no.0 train no.17270  loss = 1.17164 avg_loss = 1.13498\n","epoch no.0 train no.17270  loss = 1.17164 avg_loss = 1.13498\n","epoch no.0 train no.17280  loss = 1.14832 avg_loss = 1.14102\n","epoch no.0 train no.17280  loss = 1.14832 avg_loss = 1.14102\n","epoch no.0 train no.17290  loss = 1.13736 avg_loss = 1.13632\n","epoch no.0 train no.17290  loss = 1.13736 avg_loss = 1.13632\n","epoch no.0 train no.17300  loss = 1.57694 avg_loss = 1.11770\n","epoch no.0 train no.17300  loss = 1.57694 avg_loss = 1.11770\n","epoch no.0 train no.17310  loss = 1.15790 avg_loss = 1.12413\n","epoch no.0 train no.17310  loss = 1.15790 avg_loss = 1.12413\n","epoch no.0 train no.17320  loss = 1.49753 avg_loss = 1.13084\n","epoch no.0 train no.17320  loss = 1.49753 avg_loss = 1.13084\n","epoch no.0 train no.17330  loss = 0.86220 avg_loss = 1.12687\n","epoch no.0 train no.17330  loss = 0.86220 avg_loss = 1.12687\n","epoch no.0 train no.17340  loss = 0.89181 avg_loss = 1.12391\n","epoch no.0 train no.17340  loss = 0.89181 avg_loss = 1.12391\n","epoch no.0 train no.17350  loss = 0.72560 avg_loss = 1.10845\n","epoch no.0 train no.17350  loss = 0.72560 avg_loss = 1.10845\n","epoch no.0 train no.17360  loss = 0.94900 avg_loss = 1.11513\n","epoch no.0 train no.17360  loss = 0.94900 avg_loss = 1.11513\n","epoch no.0 train no.17370  loss = 1.36100 avg_loss = 1.10365\n","epoch no.0 train no.17370  loss = 1.36100 avg_loss = 1.10365\n","epoch no.0 train no.17380  loss = 0.77000 avg_loss = 1.09933\n","epoch no.0 train no.17380  loss = 0.77000 avg_loss = 1.09933\n","epoch no.0 train no.17390  loss = 0.83978 avg_loss = 1.09433\n","epoch no.0 train no.17390  loss = 0.83978 avg_loss = 1.09433\n","epoch no.0 train no.17400  loss = 0.96889 avg_loss = 1.10183\n","epoch no.0 train no.17400  loss = 0.96889 avg_loss = 1.10183\n","epoch no.0 train no.17410  loss = 0.99928 avg_loss = 1.10240\n","epoch no.0 train no.17410  loss = 0.99928 avg_loss = 1.10240\n","epoch no.0 train no.17420  loss = 1.39473 avg_loss = 1.10891\n","epoch no.0 train no.17420  loss = 1.39473 avg_loss = 1.10891\n","epoch no.0 train no.17430  loss = 1.79256 avg_loss = 1.09966\n","epoch no.0 train no.17430  loss = 1.79256 avg_loss = 1.09966\n","epoch no.0 train no.17440  loss = 1.23815 avg_loss = 1.08590\n","epoch no.0 train no.17440  loss = 1.23815 avg_loss = 1.08590\n","epoch no.0 train no.17450  loss = 1.62845 avg_loss = 1.09666\n","epoch no.0 train no.17450  loss = 1.62845 avg_loss = 1.09666\n","epoch no.0 train no.17460  loss = 0.96900 avg_loss = 1.09375\n","epoch no.0 train no.17460  loss = 0.96900 avg_loss = 1.09375\n","epoch no.0 train no.17470  loss = 1.49487 avg_loss = 1.10695\n","epoch no.0 train no.17470  loss = 1.49487 avg_loss = 1.10695\n","epoch no.0 train no.17480  loss = 0.81609 avg_loss = 1.09496\n","epoch no.0 train no.17480  loss = 0.81609 avg_loss = 1.09496\n","epoch no.0 train no.17490  loss = 1.37131 avg_loss = 1.10199\n","epoch no.0 train no.17490  loss = 1.37131 avg_loss = 1.10199\n","epoch no.0 train no.17500  loss = 0.76062 avg_loss = 1.11298\n","epoch no.0 train no.17500  loss = 0.76062 avg_loss = 1.11298\n","epoch no.0 train no.17510  loss = 1.20468 avg_loss = 1.12435\n","epoch no.0 train no.17510  loss = 1.20468 avg_loss = 1.12435\n","epoch no.0 train no.17520  loss = 0.85007 avg_loss = 1.12315\n","epoch no.0 train no.17520  loss = 0.85007 avg_loss = 1.12315\n","epoch no.0 train no.17530  loss = 0.88850 avg_loss = 1.12180\n","epoch no.0 train no.17530  loss = 0.88850 avg_loss = 1.12180\n","epoch no.0 train no.17540  loss = 1.62193 avg_loss = 1.12527\n","epoch no.0 train no.17540  loss = 1.62193 avg_loss = 1.12527\n","epoch no.0 train no.17550  loss = 0.83046 avg_loss = 1.11581\n","epoch no.0 train no.17550  loss = 0.83046 avg_loss = 1.11581\n","epoch no.0 train no.17560  loss = 1.04386 avg_loss = 1.12616\n","epoch no.0 train no.17560  loss = 1.04386 avg_loss = 1.12616\n","epoch no.0 train no.17570  loss = 1.26262 avg_loss = 1.13664\n","epoch no.0 train no.17570  loss = 1.26262 avg_loss = 1.13664\n","epoch no.0 train no.17580  loss = 1.36971 avg_loss = 1.12702\n","epoch no.0 train no.17580  loss = 1.36971 avg_loss = 1.12702\n","epoch no.0 train no.17590  loss = 1.58016 avg_loss = 1.14175\n","epoch no.0 train no.17590  loss = 1.58016 avg_loss = 1.14175\n","epoch no.0 train no.17600  loss = 1.67386 avg_loss = 1.13437\n","epoch no.0 train no.17600  loss = 1.67386 avg_loss = 1.13437\n","epoch no.0 train no.17610  loss = 1.15307 avg_loss = 1.13890\n","epoch no.0 train no.17610  loss = 1.15307 avg_loss = 1.13890\n","epoch no.0 train no.17620  loss = 1.49557 avg_loss = 1.14278\n","epoch no.0 train no.17620  loss = 1.49557 avg_loss = 1.14278\n","epoch no.0 train no.17630  loss = 1.03928 avg_loss = 1.13509\n","epoch no.0 train no.17630  loss = 1.03928 avg_loss = 1.13509\n","epoch no.0 train no.17640  loss = 0.86279 avg_loss = 1.11430\n","epoch no.0 train no.17640  loss = 0.86279 avg_loss = 1.11430\n","epoch no.0 train no.17650  loss = 0.75894 avg_loss = 1.09153\n","epoch no.0 train no.17650  loss = 0.75894 avg_loss = 1.09153\n","epoch no.0 train no.17660  loss = 1.44986 avg_loss = 1.08058\n","epoch no.0 train no.17660  loss = 1.44986 avg_loss = 1.08058\n","epoch no.0 train no.17670  loss = 1.29239 avg_loss = 1.08187\n","epoch no.0 train no.17670  loss = 1.29239 avg_loss = 1.08187\n","epoch no.0 train no.17680  loss = 1.25782 avg_loss = 1.08026\n","epoch no.0 train no.17680  loss = 1.25782 avg_loss = 1.08026\n","epoch no.0 train no.17690  loss = 0.68140 avg_loss = 1.08030\n","epoch no.0 train no.17690  loss = 0.68140 avg_loss = 1.08030\n","epoch no.0 train no.17700  loss = 1.12429 avg_loss = 1.06665\n","epoch no.0 train no.17700  loss = 1.12429 avg_loss = 1.06665\n","epoch no.0 train no.17710  loss = 0.93065 avg_loss = 1.05347\n","epoch no.0 train no.17710  loss = 0.93065 avg_loss = 1.05347\n","epoch no.0 train no.17720  loss = 0.84828 avg_loss = 1.05141\n","epoch no.0 train no.17720  loss = 0.84828 avg_loss = 1.05141\n","epoch no.0 train no.17730  loss = 0.58869 avg_loss = 1.07548\n","epoch no.0 train no.17730  loss = 0.58869 avg_loss = 1.07548\n","epoch no.0 train no.17740  loss = 1.28536 avg_loss = 1.08385\n","epoch no.0 train no.17740  loss = 1.28536 avg_loss = 1.08385\n","epoch no.0 train no.17750  loss = 1.62117 avg_loss = 1.08675\n","epoch no.0 train no.17750  loss = 1.62117 avg_loss = 1.08675\n","epoch no.0 train no.17760  loss = 1.18787 avg_loss = 1.10482\n","epoch no.0 train no.17760  loss = 1.18787 avg_loss = 1.10482\n","epoch no.0 train no.17770  loss = 1.21480 avg_loss = 1.10204\n","epoch no.0 train no.17770  loss = 1.21480 avg_loss = 1.10204\n","epoch no.0 train no.17780  loss = 0.95396 avg_loss = 1.10608\n","epoch no.0 train no.17780  loss = 0.95396 avg_loss = 1.10608\n","epoch no.0 train no.17790  loss = 1.11068 avg_loss = 1.11825\n","epoch no.0 train no.17790  loss = 1.11068 avg_loss = 1.11825\n","epoch no.0 train no.17800  loss = 0.62763 avg_loss = 1.10672\n","epoch no.0 train no.17800  loss = 0.62763 avg_loss = 1.10672\n","epoch no.0 train no.17810  loss = 0.81710 avg_loss = 1.07776\n","epoch no.0 train no.17810  loss = 0.81710 avg_loss = 1.07776\n","epoch no.0 train no.17820  loss = 0.58192 avg_loss = 1.07388\n","epoch no.0 train no.17820  loss = 0.58192 avg_loss = 1.07388\n","epoch no.0 train no.17830  loss = 0.84423 avg_loss = 1.08054\n","epoch no.0 train no.17830  loss = 0.84423 avg_loss = 1.08054\n","epoch no.0 train no.17840  loss = 1.36635 avg_loss = 1.08840\n","epoch no.0 train no.17840  loss = 1.36635 avg_loss = 1.08840\n","epoch no.0 train no.17850  loss = 1.40873 avg_loss = 1.10007\n","epoch no.0 train no.17850  loss = 1.40873 avg_loss = 1.10007\n","epoch no.0 train no.17860  loss = 1.29435 avg_loss = 1.08889\n","epoch no.0 train no.17860  loss = 1.29435 avg_loss = 1.08889\n","epoch no.0 train no.17870  loss = 1.29058 avg_loss = 1.11163\n","epoch no.0 train no.17870  loss = 1.29058 avg_loss = 1.11163\n","epoch no.0 train no.17880  loss = 1.58877 avg_loss = 1.12196\n","epoch no.0 train no.17880  loss = 1.58877 avg_loss = 1.12196\n","epoch no.0 train no.17890  loss = 1.22221 avg_loss = 1.12470\n","epoch no.0 train no.17890  loss = 1.22221 avg_loss = 1.12470\n","epoch no.0 train no.17900  loss = 1.25649 avg_loss = 1.12210\n","epoch no.0 train no.17900  loss = 1.25649 avg_loss = 1.12210\n","epoch no.0 train no.17910  loss = 0.90719 avg_loss = 1.12401\n","epoch no.0 train no.17910  loss = 0.90719 avg_loss = 1.12401\n","epoch no.0 train no.17920  loss = 1.01061 avg_loss = 1.13272\n","epoch no.0 train no.17920  loss = 1.01061 avg_loss = 1.13272\n","epoch no.0 train no.17930  loss = 1.92549 avg_loss = 1.12620\n","epoch no.0 train no.17930  loss = 1.92549 avg_loss = 1.12620\n","epoch no.0 train no.17940  loss = 0.75593 avg_loss = 1.12906\n","epoch no.0 train no.17940  loss = 0.75593 avg_loss = 1.12906\n","epoch no.0 train no.17950  loss = 0.61960 avg_loss = 1.12397\n","epoch no.0 train no.17950  loss = 0.61960 avg_loss = 1.12397\n","epoch no.0 train no.17960  loss = 0.96307 avg_loss = 1.12887\n","epoch no.0 train no.17960  loss = 0.96307 avg_loss = 1.12887\n","epoch no.0 train no.17970  loss = 0.98066 avg_loss = 1.12050\n","epoch no.0 train no.17970  loss = 0.98066 avg_loss = 1.12050\n","epoch no.0 train no.17980  loss = 1.02401 avg_loss = 1.10608\n","epoch no.0 train no.17980  loss = 1.02401 avg_loss = 1.10608\n","epoch no.0 train no.17990  loss = 1.18571 avg_loss = 1.10281\n","epoch no.0 train no.17990  loss = 1.18571 avg_loss = 1.10281\n","epoch no.0 train no.18000  loss = 1.10235 avg_loss = 1.11961\n","epoch no.0 train no.18000  loss = 1.10235 avg_loss = 1.11961\n","to_tokens: ['▁', '해요', '▁', '▁I', '해요', '▁', '해요', '▁', '해요', '▁', '해요', '▁', '해요', '▁', '해요', '▁', '▁', 'om', 'an', 'ization', '▁', '▁su', 's', '▁with', 'id', 'u', 'a', 'el', 'un', '▁', 'e', 'ja', '▁', 'aram', 'ang', '▁', '▁', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', 'do', 'os', 'do', '▁h', 'al', '▁s', 'ul', 'og', '▁j', 'in', 'h', 'e', 'ane', 'un', '▁', 'ar', 'ang', 'i', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁O', 'h', '▁', 'h', '▁', 'h', '▁O', 'h']\n","사랑해요\n","\n",", 희망해요\n","\n","사랑해요\n","\n","사랑해요\n","\n","사랑해요 사랑해요\n","\n","사랑해요\n","\n","그대\n","\n","Romanization\n","\n","Always Geudaeneun  Gajin  Sarangi Oh Oh  Oh Oh\n","\n","Amugeosdo\n","\n","halsulog\n","\n","jalnaganeun\n","\n","Sarangi\n","\n","Oh Oh Oh Oh Oh Oh Oh\n","\n","Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh\n","\n","Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh  Oh Oh Oh Oh Oh Oh Oh Oh  Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh  Oh Oh  Oh Oh Oh Oh Oh\n","\n","Oh  Oh Oh Oh Oh Oh Oh Oh Oh\n","to_tokens: ['▁', '해요', '▁', '▁I', '해요', '▁', '해요', '▁', '해요', '▁', '해요', '▁', '해요', '▁', '해요', '▁', '▁', 'om', 'an', 'ization', '▁', '▁su', 's', '▁with', 'id', 'u', 'a', 'el', 'un', '▁', 'e', 'ja', '▁', 'aram', 'ang', '▁', '▁', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', 'do', 'os', 'do', '▁h', 'al', '▁s', 'ul', 'og', '▁j', 'in', 'h', 'e', 'ane', 'un', '▁', 'ar', 'ang', 'i', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁O', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁O', 'h', '▁', 'h', '▁', 'h', '▁O', 'h']\n","사랑해요\n","\n",", 희망해요\n","\n","사랑해요\n","\n","사랑해요\n","\n","사랑해요 사랑해요\n","\n","사랑해요\n","\n","그대\n","\n","Romanization\n","\n","Always Geudaeneun  Gajin  Sarangi Oh Oh  Oh Oh\n","\n","Amugeosdo\n","\n","halsulog\n","\n","jalnaganeun\n","\n","Sarangi\n","\n","Oh Oh Oh Oh Oh Oh Oh\n","\n","Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh\n","\n","Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh  Oh Oh Oh Oh Oh Oh Oh Oh  Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh Oh  Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh Oh  Oh Oh Oh Oh Oh Oh  Oh Oh  Oh Oh Oh Oh Oh\n","\n","Oh  Oh Oh Oh Oh Oh Oh Oh Oh\n","epoch no.0 train no.18010  loss = 1.33658 avg_loss = 1.11958\n","epoch no.0 train no.18010  loss = 1.33658 avg_loss = 1.11958\n","epoch no.0 train no.18020  loss = 1.30231 avg_loss = 1.13361\n","epoch no.0 train no.18020  loss = 1.30231 avg_loss = 1.13361\n","epoch no.0 train no.18030  loss = 1.20251 avg_loss = 1.12253\n","epoch no.0 train no.18030  loss = 1.20251 avg_loss = 1.12253\n","epoch no.0 train no.18040  loss = 0.79823 avg_loss = 1.12506\n","epoch no.0 train no.18040  loss = 0.79823 avg_loss = 1.12506\n","epoch no.0 train no.18050  loss = 1.09742 avg_loss = 1.13083\n","epoch no.0 train no.18050  loss = 1.09742 avg_loss = 1.13083\n","epoch no.0 train no.18060  loss = 1.24923 avg_loss = 1.12496\n","epoch no.0 train no.18060  loss = 1.24923 avg_loss = 1.12496\n","epoch no.0 train no.18070  loss = 0.87250 avg_loss = 1.12633\n","epoch no.0 train no.18070  loss = 0.87250 avg_loss = 1.12633\n","epoch no.0 train no.18080  loss = 1.48217 avg_loss = 1.10588\n","epoch no.0 train no.18080  loss = 1.48217 avg_loss = 1.10588\n","epoch no.0 train no.18090  loss = 1.13100 avg_loss = 1.10214\n","epoch no.0 train no.18090  loss = 1.13100 avg_loss = 1.10214\n","epoch no.0 train no.18100  loss = 1.16630 avg_loss = 1.09936\n","epoch no.0 train no.18100  loss = 1.16630 avg_loss = 1.09936\n","epoch no.0 train no.18110  loss = 1.89454 avg_loss = 1.09732\n","epoch no.0 train no.18110  loss = 1.89454 avg_loss = 1.09732\n","epoch no.0 train no.18120  loss = 0.98100 avg_loss = 1.09502\n","epoch no.0 train no.18120  loss = 0.98100 avg_loss = 1.09502\n","epoch no.0 train no.18130  loss = 0.90273 avg_loss = 1.10247\n","epoch no.0 train no.18130  loss = 0.90273 avg_loss = 1.10247\n","epoch no.0 train no.18140  loss = 0.76803 avg_loss = 1.09365\n","epoch no.0 train no.18140  loss = 0.76803 avg_loss = 1.09365\n","epoch no.0 train no.18150  loss = 1.60718 avg_loss = 1.09512\n","epoch no.0 train no.18150  loss = 1.60718 avg_loss = 1.09512\n","epoch no.0 train no.18160  loss = 2.09188 avg_loss = 1.09251\n","epoch no.0 train no.18160  loss = 2.09188 avg_loss = 1.09251\n","epoch no.0 train no.18170  loss = 1.10063 avg_loss = 1.10547\n","epoch no.0 train no.18170  loss = 1.10063 avg_loss = 1.10547\n","epoch no.0 train no.18180  loss = 1.38690 avg_loss = 1.09242\n","epoch no.0 train no.18180  loss = 1.38690 avg_loss = 1.09242\n","epoch no.0 train no.18190  loss = 0.60868 avg_loss = 1.09418\n","epoch no.0 train no.18190  loss = 0.60868 avg_loss = 1.09418\n","epoch no.0 train no.18200  loss = 1.31287 avg_loss = 1.09793\n","epoch no.0 train no.18200  loss = 1.31287 avg_loss = 1.09793\n","epoch no.0 train no.18210  loss = 1.60594 avg_loss = 1.10551\n","epoch no.0 train no.18210  loss = 1.60594 avg_loss = 1.10551\n","epoch no.0 train no.18220  loss = 1.37730 avg_loss = 1.10546\n","epoch no.0 train no.18220  loss = 1.37730 avg_loss = 1.10546\n","epoch no.0 train no.18230  loss = 0.44856 avg_loss = 1.10753\n","epoch no.0 train no.18230  loss = 0.44856 avg_loss = 1.10753\n","epoch no.0 train no.18240  loss = 1.28881 avg_loss = 1.09396\n","epoch no.0 train no.18240  loss = 1.28881 avg_loss = 1.09396\n","epoch no.0 train no.18250  loss = 1.16350 avg_loss = 1.09202\n","epoch no.0 train no.18250  loss = 1.16350 avg_loss = 1.09202\n","epoch no.0 train no.18260  loss = 1.12596 avg_loss = 1.11144\n","epoch no.0 train no.18260  loss = 1.12596 avg_loss = 1.11144\n","epoch no.0 train no.18270  loss = 1.07464 avg_loss = 1.11257\n","epoch no.0 train no.18270  loss = 1.07464 avg_loss = 1.11257\n","epoch no.0 train no.18280  loss = 1.55296 avg_loss = 1.12268\n","epoch no.0 train no.18280  loss = 1.55296 avg_loss = 1.12268\n","epoch no.0 train no.18290  loss = 1.10174 avg_loss = 1.10911\n","epoch no.0 train no.18290  loss = 1.10174 avg_loss = 1.10911\n","epoch no.0 train no.18300  loss = 1.49484 avg_loss = 1.11783\n","epoch no.0 train no.18300  loss = 1.49484 avg_loss = 1.11783\n","epoch no.0 train no.18310  loss = 1.03201 avg_loss = 1.13420\n","epoch no.0 train no.18310  loss = 1.03201 avg_loss = 1.13420\n","epoch no.0 train no.18320  loss = 0.95144 avg_loss = 1.12934\n","epoch no.0 train no.18320  loss = 0.95144 avg_loss = 1.12934\n","epoch no.0 train no.18330  loss = 1.30929 avg_loss = 1.12512\n","epoch no.0 train no.18330  loss = 1.30929 avg_loss = 1.12512\n","epoch no.0 train no.18340  loss = 1.16059 avg_loss = 1.10683\n","epoch no.0 train no.18340  loss = 1.16059 avg_loss = 1.10683\n","epoch no.0 train no.18350  loss = 1.00444 avg_loss = 1.10477\n","epoch no.0 train no.18350  loss = 1.00444 avg_loss = 1.10477\n","epoch no.0 train no.18360  loss = 0.71158 avg_loss = 1.11139\n","epoch no.0 train no.18360  loss = 0.71158 avg_loss = 1.11139\n","epoch no.0 train no.18370  loss = 1.12603 avg_loss = 1.11867\n","epoch no.0 train no.18370  loss = 1.12603 avg_loss = 1.11867\n","epoch no.0 train no.18380  loss = 0.85885 avg_loss = 1.11375\n","epoch no.0 train no.18380  loss = 0.85885 avg_loss = 1.11375\n","epoch no.0 train no.18390  loss = 1.00210 avg_loss = 1.11819\n","epoch no.0 train no.18390  loss = 1.00210 avg_loss = 1.11819\n","epoch no.0 train no.18400  loss = 0.68724 avg_loss = 1.10945\n","epoch no.0 train no.18400  loss = 0.68724 avg_loss = 1.10945\n","epoch no.0 train no.18410  loss = 0.87630 avg_loss = 1.10169\n","epoch no.0 train no.18410  loss = 0.87630 avg_loss = 1.10169\n","epoch no.0 train no.18420  loss = 1.47451 avg_loss = 1.10964\n","epoch no.0 train no.18420  loss = 1.47451 avg_loss = 1.10964\n","epoch no.0 train no.18430  loss = 1.07615 avg_loss = 1.10647\n","epoch no.0 train no.18430  loss = 1.07615 avg_loss = 1.10647\n","epoch no.0 train no.18440  loss = 0.80907 avg_loss = 1.11824\n","epoch no.0 train no.18440  loss = 0.80907 avg_loss = 1.11824\n","epoch no.0 train no.18450  loss = 1.40791 avg_loss = 1.11677\n","epoch no.0 train no.18450  loss = 1.40791 avg_loss = 1.11677\n","epoch no.0 train no.18460  loss = 0.69272 avg_loss = 1.11434\n","epoch no.0 train no.18460  loss = 0.69272 avg_loss = 1.11434\n","epoch no.0 train no.18470  loss = 1.06765 avg_loss = 1.13557\n","epoch no.0 train no.18470  loss = 1.06765 avg_loss = 1.13557\n","epoch no.0 train no.18480  loss = 1.26874 avg_loss = 1.12774\n","epoch no.0 train no.18480  loss = 1.26874 avg_loss = 1.12774\n","epoch no.0 train no.18490  loss = 1.23379 avg_loss = 1.12648\n","epoch no.0 train no.18490  loss = 1.23379 avg_loss = 1.12648\n","epoch no.0 train no.18500  loss = 1.09833 avg_loss = 1.11475\n","epoch no.0 train no.18500  loss = 1.09833 avg_loss = 1.11475\n","epoch no.0 train no.18510  loss = 0.87214 avg_loss = 1.10225\n","epoch no.0 train no.18510  loss = 0.87214 avg_loss = 1.10225\n","epoch no.0 train no.18520  loss = 1.34560 avg_loss = 1.10568\n","epoch no.0 train no.18520  loss = 1.34560 avg_loss = 1.10568\n","epoch no.0 train no.18530  loss = 0.76358 avg_loss = 1.09765\n","epoch no.0 train no.18530  loss = 0.76358 avg_loss = 1.09765\n","epoch no.0 train no.18540  loss = 1.07541 avg_loss = 1.10424\n","epoch no.0 train no.18540  loss = 1.07541 avg_loss = 1.10424\n","epoch no.0 train no.18550  loss = 0.61909 avg_loss = 1.09044\n","epoch no.0 train no.18550  loss = 0.61909 avg_loss = 1.09044\n","epoch no.0 train no.18560  loss = 1.59673 avg_loss = 1.09912\n","epoch no.0 train no.18560  loss = 1.59673 avg_loss = 1.09912\n","epoch no.0 train no.18570  loss = 1.15884 avg_loss = 1.11201\n","epoch no.0 train no.18570  loss = 1.15884 avg_loss = 1.11201\n","epoch no.0 train no.18580  loss = 1.31760 avg_loss = 1.12400\n","epoch no.0 train no.18580  loss = 1.31760 avg_loss = 1.12400\n","epoch no.0 train no.18590  loss = 1.15388 avg_loss = 1.11795\n","epoch no.0 train no.18590  loss = 1.15388 avg_loss = 1.11795\n","epoch no.0 train no.18600  loss = 1.52903 avg_loss = 1.11365\n","epoch no.0 train no.18600  loss = 1.52903 avg_loss = 1.11365\n","epoch no.0 train no.18610  loss = 0.70485 avg_loss = 1.10635\n","epoch no.0 train no.18610  loss = 0.70485 avg_loss = 1.10635\n","epoch no.0 train no.18620  loss = 1.17097 avg_loss = 1.10459\n","epoch no.0 train no.18620  loss = 1.17097 avg_loss = 1.10459\n","epoch no.0 train no.18630  loss = 0.96386 avg_loss = 1.10884\n","epoch no.0 train no.18630  loss = 0.96386 avg_loss = 1.10884\n","epoch no.0 train no.18640  loss = 0.78402 avg_loss = 1.10378\n","epoch no.0 train no.18640  loss = 0.78402 avg_loss = 1.10378\n","epoch no.0 train no.18650  loss = 1.15494 avg_loss = 1.11213\n","epoch no.0 train no.18650  loss = 1.15494 avg_loss = 1.11213\n","epoch no.0 train no.18660  loss = 1.65363 avg_loss = 1.12224\n","epoch no.0 train no.18660  loss = 1.65363 avg_loss = 1.12224\n","epoch no.0 train no.18670  loss = 0.94967 avg_loss = 1.13175\n","epoch no.0 train no.18670  loss = 0.94967 avg_loss = 1.13175\n","epoch no.0 train no.18680  loss = 1.91326 avg_loss = 1.14223\n","epoch no.0 train no.18680  loss = 1.91326 avg_loss = 1.14223\n","epoch no.0 train no.18690  loss = 1.02781 avg_loss = 1.14340\n","epoch no.0 train no.18690  loss = 1.02781 avg_loss = 1.14340\n","epoch no.0 train no.18700  loss = 1.02417 avg_loss = 1.14678\n","epoch no.0 train no.18700  loss = 1.02417 avg_loss = 1.14678\n","epoch no.0 train no.18710  loss = 1.23324 avg_loss = 1.15205\n","epoch no.0 train no.18710  loss = 1.23324 avg_loss = 1.15205\n","epoch no.0 train no.18720  loss = 1.36701 avg_loss = 1.15508\n","epoch no.0 train no.18720  loss = 1.36701 avg_loss = 1.15508\n","epoch no.0 train no.18730  loss = 1.09490 avg_loss = 1.15611\n","epoch no.0 train no.18730  loss = 1.09490 avg_loss = 1.15611\n","epoch no.0 train no.18740  loss = 1.04346 avg_loss = 1.14371\n","epoch no.0 train no.18740  loss = 1.04346 avg_loss = 1.14371\n","epoch no.0 train no.18750  loss = 1.08478 avg_loss = 1.15164\n","epoch no.0 train no.18750  loss = 1.08478 avg_loss = 1.15164\n","epoch no.0 train no.18760  loss = 0.76442 avg_loss = 1.14150\n","epoch no.0 train no.18760  loss = 0.76442 avg_loss = 1.14150\n","epoch no.0 train no.18770  loss = 1.21088 avg_loss = 1.12857\n","epoch no.0 train no.18770  loss = 1.21088 avg_loss = 1.12857\n","epoch no.0 train no.18780  loss = 1.44851 avg_loss = 1.14444\n","epoch no.0 train no.18780  loss = 1.44851 avg_loss = 1.14444\n","epoch no.0 train no.18790  loss = 1.18588 avg_loss = 1.15466\n","epoch no.0 train no.18790  loss = 1.18588 avg_loss = 1.15466\n","epoch no.0 train no.18800  loss = 0.88231 avg_loss = 1.15226\n","epoch no.0 train no.18800  loss = 0.88231 avg_loss = 1.15226\n","epoch no.0 train no.18810  loss = 0.92176 avg_loss = 1.14272\n","epoch no.0 train no.18810  loss = 0.92176 avg_loss = 1.14272\n","epoch no.0 train no.18820  loss = 1.16045 avg_loss = 1.13795\n","epoch no.0 train no.18820  loss = 1.16045 avg_loss = 1.13795\n","epoch no.0 train no.18830  loss = 1.40183 avg_loss = 1.14789\n","epoch no.0 train no.18830  loss = 1.40183 avg_loss = 1.14789\n","epoch no.0 train no.18840  loss = 1.20654 avg_loss = 1.14083\n","epoch no.0 train no.18840  loss = 1.20654 avg_loss = 1.14083\n","epoch no.0 train no.18850  loss = 1.46642 avg_loss = 1.13482\n","epoch no.0 train no.18850  loss = 1.46642 avg_loss = 1.13482\n","epoch no.0 train no.18860  loss = 1.74335 avg_loss = 1.14606\n","epoch no.0 train no.18860  loss = 1.74335 avg_loss = 1.14606\n","epoch no.0 train no.18870  loss = 0.91392 avg_loss = 1.13188\n","epoch no.0 train no.18870  loss = 0.91392 avg_loss = 1.13188\n","epoch no.0 train no.18880  loss = 1.21201 avg_loss = 1.12443\n","epoch no.0 train no.18880  loss = 1.21201 avg_loss = 1.12443\n","epoch no.0 train no.18890  loss = 1.33152 avg_loss = 1.11802\n","epoch no.0 train no.18890  loss = 1.33152 avg_loss = 1.11802\n","epoch no.0 train no.18900  loss = 0.64960 avg_loss = 1.11125\n","epoch no.0 train no.18900  loss = 0.64960 avg_loss = 1.11125\n","epoch no.0 train no.18910  loss = 1.01480 avg_loss = 1.12463\n","epoch no.0 train no.18910  loss = 1.01480 avg_loss = 1.12463\n","epoch no.0 train no.18920  loss = 0.74123 avg_loss = 1.13548\n","epoch no.0 train no.18920  loss = 0.74123 avg_loss = 1.13548\n","epoch no.0 train no.18930  loss = 1.60311 avg_loss = 1.12964\n","epoch no.0 train no.18930  loss = 1.60311 avg_loss = 1.12964\n","epoch no.0 train no.18940  loss = 1.34495 avg_loss = 1.13396\n","epoch no.0 train no.18940  loss = 1.34495 avg_loss = 1.13396\n","epoch no.0 train no.18950  loss = 1.00856 avg_loss = 1.14513\n","epoch no.0 train no.18950  loss = 1.00856 avg_loss = 1.14513\n","epoch no.0 train no.18960  loss = 1.41767 avg_loss = 1.14466\n","epoch no.0 train no.18960  loss = 1.41767 avg_loss = 1.14466\n","epoch no.0 train no.18970  loss = 1.14767 avg_loss = 1.13167\n","epoch no.0 train no.18970  loss = 1.14767 avg_loss = 1.13167\n","epoch no.0 train no.18980  loss = 1.48868 avg_loss = 1.14283\n","epoch no.0 train no.18980  loss = 1.48868 avg_loss = 1.14283\n","epoch no.0 train no.18990  loss = 1.10138 avg_loss = 1.13230\n","epoch no.0 train no.18990  loss = 1.10138 avg_loss = 1.13230\n","epoch no.0 train no.19000  loss = 1.54107 avg_loss = 1.12371\n","epoch no.0 train no.19000  loss = 1.54107 avg_loss = 1.12371\n","to_tokens: ['▁', '해요', '▁', '▁', '▁모습', '▁그대로', '▁', '의', '▁모든', '▁눈물', '▁닦아', '주고', '▁싶어', '▁', '서', '▁와', '요', '▁그대', '▁같이', '▁걸', '어가', '▁', '▁가요', '하게', '▁안', '어', '▁줄', '던', '▁그', '▁미소', '들', '▁그', '▁미소', '▁', '▁', '의', '▁마음', '▁그', '▁가득', '▁갚', '아가는', '게', '▁']\n","사랑해요\n","\n","그대\n","\n","있는 모습 그대로\n","\n","너의 모든 눈물 닦아주고 싶어\n","\n","어서 와요 그대 같이 걸어가 다정하게 웃어주던  그 날의 그 미소도\n","\n","너의 사랑으로 다 갚을게\n","\n","\n","to_tokens: ['▁', '해요', '▁', '▁', '▁모습', '▁그대로', '▁', '의', '▁모든', '▁눈물', '▁닦아', '주고', '▁싶어', '▁', '서', '▁와', '요', '▁그대', '▁같이', '▁걸', '어가', '▁', '▁가요', '하게', '▁안', '어', '▁줄', '던', '▁그', '▁미소', '들', '▁그', '▁미소', '▁', '▁', '의', '▁마음', '▁그', '▁가득', '▁갚', '아가는', '게', '▁']\n","사랑해요\n","\n","그대\n","\n","있는 모습 그대로\n","\n","너의 모든 눈물 닦아주고 싶어\n","\n","어서 와요 그대 같이 걸어가 다정하게 웃어주던  그 날의 그 미소도\n","\n","너의 사랑으로 다 갚을게\n","\n","\n","epoch no.0 train no.19010  loss = 0.88148 avg_loss = 1.12908\n","epoch no.0 train no.19010  loss = 0.88148 avg_loss = 1.12908\n","epoch no.0 train no.19020  loss = 1.64477 avg_loss = 1.15653\n","epoch no.0 train no.19020  loss = 1.64477 avg_loss = 1.15653\n","epoch no.0 train no.19030  loss = 1.10378 avg_loss = 1.15007\n","epoch no.0 train no.19030  loss = 1.10378 avg_loss = 1.15007\n","epoch no.0 train no.19040  loss = 1.08003 avg_loss = 1.12097\n","epoch no.0 train no.19040  loss = 1.08003 avg_loss = 1.12097\n","epoch no.0 train no.19050  loss = 0.87943 avg_loss = 1.10557\n","epoch no.0 train no.19050  loss = 0.87943 avg_loss = 1.10557\n","epoch no.0 train no.19060  loss = 1.08383 avg_loss = 1.10080\n","epoch no.0 train no.19060  loss = 1.08383 avg_loss = 1.10080\n","epoch no.0 train no.19070  loss = 1.36783 avg_loss = 1.11956\n","epoch no.0 train no.19070  loss = 1.36783 avg_loss = 1.11956\n","epoch no.0 train no.19080  loss = 1.04881 avg_loss = 1.12309\n","epoch no.0 train no.19080  loss = 1.04881 avg_loss = 1.12309\n","epoch no.0 train no.19090  loss = 1.13893 avg_loss = 1.13335\n","epoch no.0 train no.19090  loss = 1.13893 avg_loss = 1.13335\n","epoch no.0 train no.19100  loss = 0.73551 avg_loss = 1.11766\n","epoch no.0 train no.19100  loss = 0.73551 avg_loss = 1.11766\n","epoch no.0 train no.19110  loss = 0.84808 avg_loss = 1.11519\n","epoch no.0 train no.19110  loss = 0.84808 avg_loss = 1.11519\n","epoch no.0 train no.19120  loss = 1.80145 avg_loss = 1.12225\n","epoch no.0 train no.19120  loss = 1.80145 avg_loss = 1.12225\n","epoch no.0 train no.19130  loss = 1.22697 avg_loss = 1.11471\n","epoch no.0 train no.19130  loss = 1.22697 avg_loss = 1.11471\n","epoch no.0 train no.19140  loss = 0.95967 avg_loss = 1.11902\n","epoch no.0 train no.19140  loss = 0.95967 avg_loss = 1.11902\n","epoch no.0 train no.19150  loss = 1.49675 avg_loss = 1.11864\n","epoch no.0 train no.19150  loss = 1.49675 avg_loss = 1.11864\n","epoch no.0 train no.19160  loss = 0.86883 avg_loss = 1.12479\n","epoch no.0 train no.19160  loss = 0.86883 avg_loss = 1.12479\n","epoch no.0 train no.19170  loss = 1.25098 avg_loss = 1.13242\n","epoch no.0 train no.19170  loss = 1.25098 avg_loss = 1.13242\n","epoch no.0 train no.19180  loss = 0.75255 avg_loss = 1.13169\n","epoch no.0 train no.19180  loss = 0.75255 avg_loss = 1.13169\n","epoch no.0 train no.19190  loss = 1.23662 avg_loss = 1.13762\n","epoch no.0 train no.19190  loss = 1.23662 avg_loss = 1.13762\n","epoch no.0 train no.19200  loss = 0.58530 avg_loss = 1.13732\n","epoch no.0 train no.19200  loss = 0.58530 avg_loss = 1.13732\n","epoch no.0 train no.19210  loss = 1.31763 avg_loss = 1.13589\n","epoch no.0 train no.19210  loss = 1.31763 avg_loss = 1.13589\n","epoch no.0 train no.19220  loss = 1.17840 avg_loss = 1.10376\n","epoch no.0 train no.19220  loss = 1.17840 avg_loss = 1.10376\n","epoch no.0 train no.19230  loss = 1.03790 avg_loss = 1.11862\n","epoch no.0 train no.19230  loss = 1.03790 avg_loss = 1.11862\n","epoch no.0 train no.19240  loss = 0.82381 avg_loss = 1.12730\n","epoch no.0 train no.19240  loss = 0.82381 avg_loss = 1.12730\n","epoch no.0 train no.19250  loss = 1.61312 avg_loss = 1.12959\n","epoch no.0 train no.19250  loss = 1.61312 avg_loss = 1.12959\n","epoch no.0 train no.19260  loss = 1.34399 avg_loss = 1.12228\n","epoch no.0 train no.19260  loss = 1.34399 avg_loss = 1.12228\n","epoch no.0 train no.19270  loss = 1.43840 avg_loss = 1.12366\n","epoch no.0 train no.19270  loss = 1.43840 avg_loss = 1.12366\n","epoch no.0 train no.19280  loss = 0.47309 avg_loss = 1.11614\n","epoch no.0 train no.19280  loss = 0.47309 avg_loss = 1.11614\n","epoch no.0 train no.19290  loss = 1.07380 avg_loss = 1.11487\n","epoch no.0 train no.19290  loss = 1.07380 avg_loss = 1.11487\n","epoch no.0 train no.19300  loss = 1.00331 avg_loss = 1.12419\n","epoch no.0 train no.19300  loss = 1.00331 avg_loss = 1.12419\n","epoch no.0 train no.19310  loss = 0.73881 avg_loss = 1.13129\n","epoch no.0 train no.19310  loss = 0.73881 avg_loss = 1.13129\n","epoch no.0 train no.19320  loss = 1.10189 avg_loss = 1.11698\n","epoch no.0 train no.19320  loss = 1.10189 avg_loss = 1.11698\n","epoch no.0 train no.19330  loss = 1.01451 avg_loss = 1.10754\n","epoch no.0 train no.19330  loss = 1.01451 avg_loss = 1.10754\n","epoch no.0 train no.19340  loss = 1.47986 avg_loss = 1.11923\n","epoch no.0 train no.19340  loss = 1.47986 avg_loss = 1.11923\n","epoch no.0 train no.19350  loss = 0.97320 avg_loss = 1.11828\n","epoch no.0 train no.19350  loss = 0.97320 avg_loss = 1.11828\n","epoch no.0 train no.19360  loss = 1.66099 avg_loss = 1.12995\n","epoch no.0 train no.19360  loss = 1.66099 avg_loss = 1.12995\n","epoch no.0 train no.19370  loss = 1.25299 avg_loss = 1.13232\n","epoch no.0 train no.19370  loss = 1.25299 avg_loss = 1.13232\n","epoch no.0 train no.19380  loss = 1.27380 avg_loss = 1.13295\n","epoch no.0 train no.19380  loss = 1.27380 avg_loss = 1.13295\n","epoch no.0 train no.19390  loss = 1.04980 avg_loss = 1.12555\n","epoch no.0 train no.19390  loss = 1.04980 avg_loss = 1.12555\n","epoch no.0 train no.19400  loss = 0.97201 avg_loss = 1.12531\n","epoch no.0 train no.19400  loss = 0.97201 avg_loss = 1.12531\n","epoch no.0 train no.19410  loss = 0.86549 avg_loss = 1.10348\n","epoch no.0 train no.19410  loss = 0.86549 avg_loss = 1.10348\n","epoch no.0 train no.19420  loss = 0.63145 avg_loss = 1.09498\n","epoch no.0 train no.19420  loss = 0.63145 avg_loss = 1.09498\n","epoch no.0 train no.19430  loss = 1.47924 avg_loss = 1.09920\n","epoch no.0 train no.19430  loss = 1.47924 avg_loss = 1.09920\n","epoch no.0 train no.19440  loss = 0.65489 avg_loss = 1.11079\n","epoch no.0 train no.19440  loss = 0.65489 avg_loss = 1.11079\n","epoch no.0 train no.19450  loss = 0.95038 avg_loss = 1.12567\n","epoch no.0 train no.19450  loss = 0.95038 avg_loss = 1.12567\n","epoch no.0 train no.19460  loss = 1.22770 avg_loss = 1.12060\n","epoch no.0 train no.19460  loss = 1.22770 avg_loss = 1.12060\n","epoch no.0 train no.19470  loss = 0.73501 avg_loss = 1.11654\n","epoch no.0 train no.19470  loss = 0.73501 avg_loss = 1.11654\n","epoch no.0 train no.19480  loss = 1.04685 avg_loss = 1.12735\n","epoch no.0 train no.19480  loss = 1.04685 avg_loss = 1.12735\n","epoch no.0 train no.19490  loss = 0.42876 avg_loss = 1.11375\n","epoch no.0 train no.19490  loss = 0.42876 avg_loss = 1.11375\n","epoch no.0 train no.19500  loss = 0.98920 avg_loss = 1.12115\n","epoch no.0 train no.19500  loss = 0.98920 avg_loss = 1.12115\n","epoch no.0 train no.19510  loss = 1.17257 avg_loss = 1.12631\n","epoch no.0 train no.19510  loss = 1.17257 avg_loss = 1.12631\n","epoch no.0 train no.19520  loss = 1.50709 avg_loss = 1.13567\n","epoch no.0 train no.19520  loss = 1.50709 avg_loss = 1.13567\n","epoch no.0 train no.19530  loss = 0.63385 avg_loss = 1.11836\n","epoch no.0 train no.19530  loss = 0.63385 avg_loss = 1.11836\n","epoch no.0 train no.19540  loss = 0.45610 avg_loss = 1.11133\n","epoch no.0 train no.19540  loss = 0.45610 avg_loss = 1.11133\n","epoch no.0 train no.19550  loss = 1.18089 avg_loss = 1.10919\n","epoch no.0 train no.19550  loss = 1.18089 avg_loss = 1.10919\n","epoch no.0 train no.19560  loss = 0.89453 avg_loss = 1.09623\n","epoch no.0 train no.19560  loss = 0.89453 avg_loss = 1.09623\n","epoch no.0 train no.19570  loss = 1.46561 avg_loss = 1.10638\n","epoch no.0 train no.19570  loss = 1.46561 avg_loss = 1.10638\n","epoch no.0 train no.19580  loss = 1.00266 avg_loss = 1.09631\n","epoch no.0 train no.19580  loss = 1.00266 avg_loss = 1.09631\n","epoch no.0 train no.19590  loss = 0.74364 avg_loss = 1.09166\n","epoch no.0 train no.19590  loss = 0.74364 avg_loss = 1.09166\n","epoch no.0 train no.19600  loss = 0.38681 avg_loss = 1.09588\n","epoch no.0 train no.19600  loss = 0.38681 avg_loss = 1.09588\n","epoch no.0 train no.19610  loss = 1.19865 avg_loss = 1.10219\n","epoch no.0 train no.19610  loss = 1.19865 avg_loss = 1.10219\n","epoch no.0 train no.19620  loss = 0.73137 avg_loss = 1.12520\n","epoch no.0 train no.19620  loss = 0.73137 avg_loss = 1.12520\n","epoch no.0 train no.19630  loss = 1.83900 avg_loss = 1.12639\n","epoch no.0 train no.19630  loss = 1.83900 avg_loss = 1.12639\n","epoch no.0 train no.19640  loss = 1.33434 avg_loss = 1.12711\n","epoch no.0 train no.19640  loss = 1.33434 avg_loss = 1.12711\n","epoch no.0 train no.19650  loss = 1.34719 avg_loss = 1.12692\n","epoch no.0 train no.19650  loss = 1.34719 avg_loss = 1.12692\n","epoch no.0 train no.19660  loss = 1.64626 avg_loss = 1.13559\n","epoch no.0 train no.19660  loss = 1.64626 avg_loss = 1.13559\n","epoch no.0 train no.19670  loss = 0.98828 avg_loss = 1.15402\n","epoch no.0 train no.19670  loss = 0.98828 avg_loss = 1.15402\n","epoch no.0 train no.19680  loss = 0.99380 avg_loss = 1.14292\n","epoch no.0 train no.19680  loss = 0.99380 avg_loss = 1.14292\n","epoch no.0 train no.19690  loss = 1.00626 avg_loss = 1.14324\n","epoch no.0 train no.19690  loss = 1.00626 avg_loss = 1.14324\n","epoch no.0 train no.19700  loss = 1.04373 avg_loss = 1.14506\n","epoch no.0 train no.19700  loss = 1.04373 avg_loss = 1.14506\n","epoch no.0 train no.19710  loss = 0.83641 avg_loss = 1.14039\n","epoch no.0 train no.19710  loss = 0.83641 avg_loss = 1.14039\n","epoch no.0 train no.19720  loss = 0.07507 avg_loss = 1.12664\n","epoch no.0 train no.19720  loss = 0.07507 avg_loss = 1.12664\n","epoch no.0 train no.19730  loss = 0.96580 avg_loss = 1.11839\n","epoch no.0 train no.19730  loss = 0.96580 avg_loss = 1.11839\n","epoch no.0 train no.19740  loss = 1.19558 avg_loss = 1.11996\n","epoch no.0 train no.19740  loss = 1.19558 avg_loss = 1.11996\n","epoch no.0 train no.19750  loss = 0.96248 avg_loss = 1.11119\n","epoch no.0 train no.19750  loss = 0.96248 avg_loss = 1.11119\n","epoch no.0 train no.19760  loss = 1.01484 avg_loss = 1.09106\n","epoch no.0 train no.19760  loss = 1.01484 avg_loss = 1.09106\n","epoch no.0 train no.19770  loss = 1.08039 avg_loss = 1.07770\n","epoch no.0 train no.19770  loss = 1.08039 avg_loss = 1.07770\n","epoch no.0 train no.19780  loss = 1.18595 avg_loss = 1.09234\n","epoch no.0 train no.19780  loss = 1.18595 avg_loss = 1.09234\n","epoch no.0 train no.19790  loss = 0.85087 avg_loss = 1.08999\n","epoch no.0 train no.19790  loss = 0.85087 avg_loss = 1.08999\n","epoch no.0 train no.19800  loss = 0.86042 avg_loss = 1.09119\n","epoch no.0 train no.19800  loss = 0.86042 avg_loss = 1.09119\n","epoch no.0 train no.19810  loss = 1.11641 avg_loss = 1.08549\n","epoch no.0 train no.19810  loss = 1.11641 avg_loss = 1.08549\n","epoch no.0 train no.19820  loss = 1.23185 avg_loss = 1.10281\n","epoch no.0 train no.19820  loss = 1.23185 avg_loss = 1.10281\n","epoch no.0 train no.19830  loss = 1.53684 avg_loss = 1.08759\n","epoch no.0 train no.19830  loss = 1.53684 avg_loss = 1.08759\n","epoch no.0 train no.19840  loss = 0.67973 avg_loss = 1.07891\n","epoch no.0 train no.19840  loss = 0.67973 avg_loss = 1.07891\n","epoch no.0 train no.19850  loss = 1.03689 avg_loss = 1.06469\n","epoch no.0 train no.19850  loss = 1.03689 avg_loss = 1.06469\n","epoch no.0 train no.19860  loss = 1.29981 avg_loss = 1.06137\n","epoch no.0 train no.19860  loss = 1.29981 avg_loss = 1.06137\n","epoch no.0 train no.19870  loss = 1.03442 avg_loss = 1.06005\n","epoch no.0 train no.19870  loss = 1.03442 avg_loss = 1.06005\n","epoch no.0 train no.19880  loss = 1.27080 avg_loss = 1.04526\n","epoch no.0 train no.19880  loss = 1.27080 avg_loss = 1.04526\n","epoch no.0 train no.19890  loss = 1.41385 avg_loss = 1.06018\n","epoch no.0 train no.19890  loss = 1.41385 avg_loss = 1.06018\n","epoch no.0 train no.19900  loss = 1.51370 avg_loss = 1.06929\n","epoch no.0 train no.19900  loss = 1.51370 avg_loss = 1.06929\n","epoch no.0 train no.19910  loss = 1.31776 avg_loss = 1.08333\n","epoch no.0 train no.19910  loss = 1.31776 avg_loss = 1.08333\n","epoch no.0 train no.19920  loss = 0.76767 avg_loss = 1.05874\n","epoch no.0 train no.19920  loss = 0.76767 avg_loss = 1.05874\n","epoch no.0 train no.19930  loss = 0.79793 avg_loss = 1.07734\n","epoch no.0 train no.19930  loss = 0.79793 avg_loss = 1.07734\n","epoch no.0 train no.19940  loss = 0.90309 avg_loss = 1.08488\n","epoch no.0 train no.19940  loss = 0.90309 avg_loss = 1.08488\n","epoch no.0 train no.19950  loss = 1.30220 avg_loss = 1.08531\n","epoch no.0 train no.19950  loss = 1.30220 avg_loss = 1.08531\n","epoch no.0 train no.19960  loss = 1.70300 avg_loss = 1.10444\n","epoch no.0 train no.19960  loss = 1.70300 avg_loss = 1.10444\n","epoch no.0 train no.19970  loss = 0.83902 avg_loss = 1.10406\n","epoch no.0 train no.19970  loss = 0.83902 avg_loss = 1.10406\n","epoch no.0 train no.19980  loss = 1.20301 avg_loss = 1.07901\n","epoch no.0 train no.19980  loss = 1.20301 avg_loss = 1.07901\n","epoch no.0 train no.19990  loss = 1.28313 avg_loss = 1.10016\n","epoch no.0 train no.19990  loss = 1.28313 avg_loss = 1.10016\n","epoch no.0 train no.20000  loss = 0.89936 avg_loss = 1.10376\n","epoch no.0 train no.20000  loss = 0.89936 avg_loss = 1.10376\n","to_tokens: ['▁', '해요', '▁건', '▁뭐', '할', '수', '▁없는', '걸', '▁봐', '▁', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁']\n","사랑이란게 영원할 수 없는 건가 봐\n","\n","요 uh\n","\n","uh\n","\n","uh\n","\n","uh  uh uh uh uh uh  uh uh uh uh\n","\n","uh uh uh\n","\n","uh\n","\n","uh  uh  uh\n","\n","uh  uh uh uh  uh uh\n","\n","uh uh uh  uh uh  uh uh uh  uh  uh uh  uh uh uh uh uh uh uh uh uh uh uh  uh uh uh  uh  uh uh uh  uh  uh uh uh uh uh uh  uh uh uh uh\n","\n","uh uh uh uh uh  uh uh uh  uh  uh uh uh uh  uh uh  uh uh  uh\n","\n","uh uh uh uh uh  uh uh uh  uh uh\n","\n","uh uh uh uh\n","\n","uh uh\n","\n","uh uh uh  uh\n","\n","uh uh uh uh uh uh\n","\n","uh\n","\n","uh  uh uh uh\n","\n","uh uh uh  uh  uh\n","\n","uh uh uh uh  uh\n","\n","uh  uh  uh  uh uh\n","\n","uh\n","\n","uh uh uh  uh uh\n","\n","uh\n","\n","uh  uh uh uh  uh  uh uh  uh uh uh\n","\n","uh uh uh uh\n","\n","uh  uh uh  uh uh\n","\n","uh  uh  uh uh uh uh uh  uh  uh  uh uh uh\n","\n","uh uh uh\n","\n","uh uh uh uh uh uh\n","\n","uh uh  uh uh uh  uh\n","\n","uh uh uh uh uh  uh uh\n","\n","uh uh\n","\n","uh uh\n","\n","uh uh\n","\n","uh uh uh uh  uh  uh  uh uh uh uh uh\n","\n","uh\n","\n","uh uh uh uh uh  uh uh uh uh  uh uh uh\n","\n","uh uh uh\n","\n","uh uh  uh\n","\n","uh  uh uh\n","\n","uh uh  uh uh  uh uh uh uh uh uh  uh uh  uh\n","\n","uh uh  uh  uh  uh uh  uh uh uh uh  uh\n","\n","uh uh  uh\n","\n","uh  uh\n","\n","uh uh uh\n","\n","uh uh\n","\n","uh uh uh uh uh  uh uh uh uh uh  uh\n","\n","uh uh\n","\n","uh  uh  uh uh  uh  \n","to_tokens: ['▁', '해요', '▁건', '▁뭐', '할', '수', '▁없는', '걸', '▁봐', '▁', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁u', 'h', '▁u', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁', 'h', '▁', 'h', '▁u', 'h', '▁']\n","사랑이란게 영원할 수 없는 건가 봐\n","\n","요 uh\n","\n","uh\n","\n","uh\n","\n","uh  uh uh uh uh uh  uh uh uh uh\n","\n","uh uh uh\n","\n","uh\n","\n","uh  uh  uh\n","\n","uh  uh uh uh  uh uh\n","\n","uh uh uh  uh uh  uh uh uh  uh  uh uh  uh uh uh uh uh uh uh uh uh uh uh  uh uh uh  uh  uh uh uh  uh  uh uh uh uh uh uh  uh uh uh uh\n","\n","uh uh uh uh uh  uh uh uh  uh  uh uh uh uh  uh uh  uh uh  uh\n","\n","uh uh uh uh uh  uh uh uh  uh uh\n","\n","uh uh uh uh\n","\n","uh uh\n","\n","uh uh uh  uh\n","\n","uh uh uh uh uh uh\n","\n","uh\n","\n","uh  uh uh uh\n","\n","uh uh uh  uh  uh\n","\n","uh uh uh uh  uh\n","\n","uh  uh  uh  uh uh\n","\n","uh\n","\n","uh uh uh  uh uh\n","\n","uh\n","\n","uh  uh uh uh  uh  uh uh  uh uh uh\n","\n","uh uh uh uh\n","\n","uh  uh uh  uh uh\n","\n","uh  uh  uh uh uh uh uh  uh  uh  uh uh uh\n","\n","uh uh uh\n","\n","uh uh uh uh uh uh\n","\n","uh uh  uh uh uh  uh\n","\n","uh uh uh uh uh  uh uh\n","\n","uh uh\n","\n","uh uh\n","\n","uh uh\n","\n","uh uh uh uh  uh  uh  uh uh uh uh uh\n","\n","uh\n","\n","uh uh uh uh uh  uh uh uh uh  uh uh uh\n","\n","uh uh uh\n","\n","uh uh  uh\n","\n","uh  uh uh\n","\n","uh uh  uh uh  uh uh uh uh uh uh  uh uh  uh\n","\n","uh uh  uh  uh  uh uh  uh uh uh uh  uh\n","\n","uh uh  uh\n","\n","uh  uh\n","\n","uh uh uh\n","\n","uh uh\n","\n","uh uh uh uh uh  uh uh uh uh uh  uh\n","\n","uh uh\n","\n","uh  uh  uh uh  uh  \n","epoch no.0 train no.20010  loss = 1.57506 avg_loss = 1.11640\n","epoch no.0 train no.20010  loss = 1.57506 avg_loss = 1.11640\n","epoch no.0 train no.20020  loss = 1.01659 avg_loss = 1.11448\n","epoch no.0 train no.20020  loss = 1.01659 avg_loss = 1.11448\n","epoch no.0 train no.20030  loss = 0.92168 avg_loss = 1.12144\n","epoch no.0 train no.20030  loss = 0.92168 avg_loss = 1.12144\n","epoch no.0 train no.20040  loss = 1.12912 avg_loss = 1.12866\n","epoch no.0 train no.20040  loss = 1.12912 avg_loss = 1.12866\n","epoch no.0 train no.20050  loss = 1.66377 avg_loss = 1.12677\n","epoch no.0 train no.20050  loss = 1.66377 avg_loss = 1.12677\n","epoch no.0 train no.20060  loss = 0.57023 avg_loss = 1.11449\n","epoch no.0 train no.20060  loss = 0.57023 avg_loss = 1.11449\n","epoch no.0 train no.20070  loss = 1.46730 avg_loss = 1.11452\n","epoch no.0 train no.20070  loss = 1.46730 avg_loss = 1.11452\n","epoch no.0 train no.20080  loss = 0.74929 avg_loss = 1.09697\n","epoch no.0 train no.20080  loss = 0.74929 avg_loss = 1.09697\n","epoch no.0 train no.20090  loss = 1.09854 avg_loss = 1.09330\n","epoch no.0 train no.20090  loss = 1.09854 avg_loss = 1.09330\n","epoch no.0 train no.20100  loss = 0.76106 avg_loss = 1.08167\n","epoch no.0 train no.20100  loss = 0.76106 avg_loss = 1.08167\n","epoch no.0 train no.20110  loss = 1.25058 avg_loss = 1.07410\n","epoch no.0 train no.20110  loss = 1.25058 avg_loss = 1.07410\n","epoch no.0 train no.20120  loss = 0.63555 avg_loss = 1.06137\n","epoch no.0 train no.20120  loss = 0.63555 avg_loss = 1.06137\n","epoch no.0 train no.20130  loss = 1.26238 avg_loss = 1.06056\n","epoch no.0 train no.20130  loss = 1.26238 avg_loss = 1.06056\n","epoch no.0 train no.20140  loss = 1.85676 avg_loss = 1.06215\n","epoch no.0 train no.20140  loss = 1.85676 avg_loss = 1.06215\n","epoch no.0 train no.20150  loss = 1.59887 avg_loss = 1.07483\n","epoch no.0 train no.20150  loss = 1.59887 avg_loss = 1.07483\n","epoch no.0 train no.20160  loss = 1.97531 avg_loss = 1.08745\n","epoch no.0 train no.20160  loss = 1.97531 avg_loss = 1.08745\n","epoch no.0 train no.20170  loss = 1.30957 avg_loss = 1.09624\n","epoch no.0 train no.20170  loss = 1.30957 avg_loss = 1.09624\n","epoch no.0 train no.20180  loss = 1.10416 avg_loss = 1.10287\n","epoch no.0 train no.20180  loss = 1.10416 avg_loss = 1.10287\n","epoch no.0 train no.20190  loss = 0.97003 avg_loss = 1.09959\n","epoch no.0 train no.20190  loss = 0.97003 avg_loss = 1.09959\n","epoch no.0 train no.20200  loss = 1.06421 avg_loss = 1.09530\n","epoch no.0 train no.20200  loss = 1.06421 avg_loss = 1.09530\n","epoch no.0 train no.20210  loss = 0.84908 avg_loss = 1.07435\n","epoch no.0 train no.20210  loss = 0.84908 avg_loss = 1.07435\n","epoch no.0 train no.20220  loss = 1.66447 avg_loss = 1.08386\n","epoch no.0 train no.20220  loss = 1.66447 avg_loss = 1.08386\n","epoch no.0 train no.20230  loss = 0.80779 avg_loss = 1.07800\n","epoch no.0 train no.20230  loss = 0.80779 avg_loss = 1.07800\n","epoch no.0 train no.20240  loss = 0.79968 avg_loss = 1.07944\n","epoch no.0 train no.20240  loss = 0.79968 avg_loss = 1.07944\n","epoch no.0 train no.20250  loss = 1.30254 avg_loss = 1.09817\n","epoch no.0 train no.20250  loss = 1.30254 avg_loss = 1.09817\n","epoch no.0 train no.20260  loss = 1.25762 avg_loss = 1.09212\n","epoch no.0 train no.20260  loss = 1.25762 avg_loss = 1.09212\n","epoch no.0 train no.20270  loss = 0.61405 avg_loss = 1.08052\n","epoch no.0 train no.20270  loss = 0.61405 avg_loss = 1.08052\n","epoch no.0 train no.20280  loss = 0.69050 avg_loss = 1.08657\n","epoch no.0 train no.20280  loss = 0.69050 avg_loss = 1.08657\n","epoch no.0 train no.20290  loss = 1.40828 avg_loss = 1.11660\n","epoch no.0 train no.20290  loss = 1.40828 avg_loss = 1.11660\n","epoch no.0 train no.20300  loss = 1.12078 avg_loss = 1.11643\n","epoch no.0 train no.20300  loss = 1.12078 avg_loss = 1.11643\n","epoch no.0 train no.20310  loss = 1.27962 avg_loss = 1.12107\n","epoch no.0 train no.20310  loss = 1.27962 avg_loss = 1.12107\n","epoch no.0 train no.20320  loss = 0.72870 avg_loss = 1.11164\n","epoch no.0 train no.20320  loss = 0.72870 avg_loss = 1.11164\n","epoch no.0 train no.20330  loss = 1.60922 avg_loss = 1.11678\n","epoch no.0 train no.20330  loss = 1.60922 avg_loss = 1.11678\n","epoch no.0 train no.20340  loss = 0.90378 avg_loss = 1.10697\n","epoch no.0 train no.20340  loss = 0.90378 avg_loss = 1.10697\n","epoch no.0 train no.20350  loss = 0.80802 avg_loss = 1.09294\n","epoch no.0 train no.20350  loss = 0.80802 avg_loss = 1.09294\n","epoch no.0 train no.20360  loss = 1.77379 avg_loss = 1.11109\n","epoch no.0 train no.20360  loss = 1.77379 avg_loss = 1.11109\n","epoch no.0 train no.20370  loss = 1.19594 avg_loss = 1.10620\n","epoch no.0 train no.20370  loss = 1.19594 avg_loss = 1.10620\n","epoch no.0 train no.20380  loss = 1.68823 avg_loss = 1.11462\n","epoch no.0 train no.20380  loss = 1.68823 avg_loss = 1.11462\n","epoch no.0 train no.20390  loss = 1.85891 avg_loss = 1.13097\n","epoch no.0 train no.20390  loss = 1.85891 avg_loss = 1.13097\n","epoch no.0 train no.20400  loss = 1.09749 avg_loss = 1.13922\n","epoch no.0 train no.20400  loss = 1.09749 avg_loss = 1.13922\n","epoch no.0 train no.20410  loss = 0.95622 avg_loss = 1.12927\n","epoch no.0 train no.20410  loss = 0.95622 avg_loss = 1.12927\n","epoch no.0 train no.20420  loss = 0.71403 avg_loss = 1.11538\n","epoch no.0 train no.20420  loss = 0.71403 avg_loss = 1.11538\n","epoch no.0 train no.20430  loss = 1.00282 avg_loss = 1.10808\n","epoch no.0 train no.20430  loss = 1.00282 avg_loss = 1.10808\n","epoch no.0 train no.20440  loss = 0.90324 avg_loss = 1.12554\n","epoch no.0 train no.20440  loss = 0.90324 avg_loss = 1.12554\n","epoch no.0 train no.20450  loss = 0.89625 avg_loss = 1.11811\n","epoch no.0 train no.20450  loss = 0.89625 avg_loss = 1.11811\n","epoch no.0 train no.20460  loss = 1.01969 avg_loss = 1.12062\n","epoch no.0 train no.20460  loss = 1.01969 avg_loss = 1.12062\n","epoch no.0 train no.20470  loss = 1.05230 avg_loss = 1.11832\n","epoch no.0 train no.20470  loss = 1.05230 avg_loss = 1.11832\n","epoch no.0 train no.20480  loss = 0.80169 avg_loss = 1.09970\n","epoch no.0 train no.20480  loss = 0.80169 avg_loss = 1.09970\n","epoch no.0 train no.20490  loss = 1.15290 avg_loss = 1.09999\n","epoch no.0 train no.20490  loss = 1.15290 avg_loss = 1.09999\n","epoch no.0 train no.20500  loss = 0.83054 avg_loss = 1.10082\n","epoch no.0 train no.20500  loss = 0.83054 avg_loss = 1.10082\n","epoch no.0 train no.20510  loss = 0.83000 avg_loss = 1.09962\n","epoch no.0 train no.20510  loss = 0.83000 avg_loss = 1.09962\n","epoch no.0 train no.20520  loss = 1.22112 avg_loss = 1.10299\n","epoch no.0 train no.20520  loss = 1.22112 avg_loss = 1.10299\n","epoch no.0 train no.20530  loss = 0.56918 avg_loss = 1.07891\n","epoch no.0 train no.20530  loss = 0.56918 avg_loss = 1.07891\n","epoch no.0 train no.20540  loss = 1.52722 avg_loss = 1.09409\n","epoch no.0 train no.20540  loss = 1.52722 avg_loss = 1.09409\n","epoch no.0 train no.20550  loss = 1.34295 avg_loss = 1.10553\n","epoch no.0 train no.20550  loss = 1.34295 avg_loss = 1.10553\n","epoch no.0 train no.20560  loss = 0.68546 avg_loss = 1.10449\n","epoch no.0 train no.20560  loss = 0.68546 avg_loss = 1.10449\n","epoch no.0 train no.20570  loss = 0.99645 avg_loss = 1.11140\n","epoch no.0 train no.20570  loss = 0.99645 avg_loss = 1.11140\n","epoch no.0 train no.20580  loss = 1.95909 avg_loss = 1.11553\n","epoch no.0 train no.20580  loss = 1.95909 avg_loss = 1.11553\n","epoch no.0 train no.20590  loss = 0.65834 avg_loss = 1.12083\n","epoch no.0 train no.20590  loss = 0.65834 avg_loss = 1.12083\n","epoch no.0 train no.20600  loss = 0.28900 avg_loss = 1.08802\n","epoch no.0 train no.20600  loss = 0.28900 avg_loss = 1.08802\n","epoch no.0 train no.20610  loss = 1.15555 avg_loss = 1.07943\n","epoch no.0 train no.20610  loss = 1.15555 avg_loss = 1.07943\n","epoch no.0 train no.20620  loss = 0.97083 avg_loss = 1.08138\n","epoch no.0 train no.20620  loss = 0.97083 avg_loss = 1.08138\n","epoch no.0 train no.20630  loss = 1.08209 avg_loss = 1.08240\n","epoch no.0 train no.20630  loss = 1.08209 avg_loss = 1.08240\n","epoch no.0 train no.20640  loss = 1.77434 avg_loss = 1.09832\n","epoch no.0 train no.20640  loss = 1.77434 avg_loss = 1.09832\n","epoch no.0 train no.20650  loss = 1.19414 avg_loss = 1.10259\n","epoch no.0 train no.20650  loss = 1.19414 avg_loss = 1.10259\n","epoch no.0 train no.20660  loss = 0.91513 avg_loss = 1.08611\n","epoch no.0 train no.20660  loss = 0.91513 avg_loss = 1.08611\n","epoch no.0 train no.20670  loss = 1.27979 avg_loss = 1.09063\n","epoch no.0 train no.20670  loss = 1.27979 avg_loss = 1.09063\n","epoch no.0 train no.20680  loss = 1.00665 avg_loss = 1.10489\n","epoch no.0 train no.20680  loss = 1.00665 avg_loss = 1.10489\n","epoch no.0 train no.20690  loss = 0.77737 avg_loss = 1.11172\n","epoch no.0 train no.20690  loss = 0.77737 avg_loss = 1.11172\n","epoch no.0 train no.20700  loss = 1.20082 avg_loss = 1.12414\n","epoch no.0 train no.20700  loss = 1.20082 avg_loss = 1.12414\n","epoch no.0 train no.20710  loss = 1.37237 avg_loss = 1.13557\n","epoch no.0 train no.20710  loss = 1.37237 avg_loss = 1.13557\n","epoch no.0 train no.20720  loss = 1.09277 avg_loss = 1.12076\n","epoch no.0 train no.20720  loss = 1.09277 avg_loss = 1.12076\n","epoch no.0 train no.20730  loss = 1.57863 avg_loss = 1.10505\n","epoch no.0 train no.20730  loss = 1.57863 avg_loss = 1.10505\n","epoch no.0 train no.20740  loss = 0.41688 avg_loss = 1.10585\n","epoch no.0 train no.20740  loss = 0.41688 avg_loss = 1.10585\n","epoch no.0 train no.20750  loss = 1.36865 avg_loss = 1.10991\n","epoch no.0 train no.20750  loss = 1.36865 avg_loss = 1.10991\n","epoch no.0 train no.20760  loss = 0.81937 avg_loss = 1.09552\n","epoch no.0 train no.20760  loss = 0.81937 avg_loss = 1.09552\n","epoch no.0 train no.20770  loss = 1.69758 avg_loss = 1.11504\n","epoch no.0 train no.20770  loss = 1.69758 avg_loss = 1.11504\n","epoch no.0 train no.20780  loss = 1.41691 avg_loss = 1.12432\n","epoch no.0 train no.20780  loss = 1.41691 avg_loss = 1.12432\n","epoch no.0 train no.20790  loss = 2.05069 avg_loss = 1.13411\n","epoch no.0 train no.20790  loss = 2.05069 avg_loss = 1.13411\n","epoch no.0 train no.20800  loss = 0.60733 avg_loss = 1.14767\n","epoch no.0 train no.20800  loss = 0.60733 avg_loss = 1.14767\n","epoch no.0 train no.20810  loss = 1.09533 avg_loss = 1.13877\n","epoch no.0 train no.20810  loss = 1.09533 avg_loss = 1.13877\n","epoch no.0 train no.20820  loss = 0.76074 avg_loss = 1.13634\n","epoch no.0 train no.20820  loss = 0.76074 avg_loss = 1.13634\n","epoch no.0 train no.20830  loss = 1.00809 avg_loss = 1.12849\n","epoch no.0 train no.20830  loss = 1.00809 avg_loss = 1.12849\n","epoch no.0 train no.20840  loss = 0.62393 avg_loss = 1.13032\n","epoch no.0 train no.20840  loss = 0.62393 avg_loss = 1.13032\n","epoch no.0 train no.20850  loss = 1.21437 avg_loss = 1.12230\n","epoch no.0 train no.20850  loss = 1.21437 avg_loss = 1.12230\n","epoch no.0 train no.20860  loss = 0.99469 avg_loss = 1.10387\n","epoch no.0 train no.20860  loss = 0.99469 avg_loss = 1.10387\n","epoch no.0 train no.20870  loss = 0.87583 avg_loss = 1.09529\n","epoch no.0 train no.20870  loss = 0.87583 avg_loss = 1.09529\n","epoch no.0 train no.20880  loss = 1.18586 avg_loss = 1.08296\n","epoch no.0 train no.20880  loss = 1.18586 avg_loss = 1.08296\n","epoch no.0 train no.20890  loss = 1.59023 avg_loss = 1.07995\n","epoch no.0 train no.20890  loss = 1.59023 avg_loss = 1.07995\n","epoch no.0 train no.20900  loss = 1.26138 avg_loss = 1.08387\n","epoch no.0 train no.20900  loss = 1.26138 avg_loss = 1.08387\n","epoch no.0 train no.20910  loss = 0.55110 avg_loss = 1.09256\n","epoch no.0 train no.20910  loss = 0.55110 avg_loss = 1.09256\n","epoch no.0 train no.20920  loss = 0.97559 avg_loss = 1.10740\n","epoch no.0 train no.20920  loss = 0.97559 avg_loss = 1.10740\n","epoch no.0 train no.20930  loss = 0.89701 avg_loss = 1.09834\n","epoch no.0 train no.20930  loss = 0.89701 avg_loss = 1.09834\n","epoch no.0 train no.20940  loss = 0.95523 avg_loss = 1.10635\n","epoch no.0 train no.20940  loss = 0.95523 avg_loss = 1.10635\n","epoch no.0 train no.20950  loss = 1.49307 avg_loss = 1.09813\n","epoch no.0 train no.20950  loss = 1.49307 avg_loss = 1.09813\n","epoch no.0 train no.20960  loss = 0.70940 avg_loss = 1.10324\n","epoch no.0 train no.20960  loss = 0.70940 avg_loss = 1.10324\n","epoch no.0 train no.20970  loss = 2.01141 avg_loss = 1.11230\n","epoch no.0 train no.20970  loss = 2.01141 avg_loss = 1.11230\n","epoch no.0 train no.20980  loss = 1.25992 avg_loss = 1.12179\n","epoch no.0 train no.20980  loss = 1.25992 avg_loss = 1.12179\n","epoch no.0 train no.20990  loss = 1.01711 avg_loss = 1.12642\n","epoch no.0 train no.20990  loss = 1.01711 avg_loss = 1.12642\n","epoch no.0 train no.21000  loss = 1.49749 avg_loss = 1.12794\n","epoch no.0 train no.21000  loss = 1.49749 avg_loss = 1.12794\n","to_tokens: ['▁', '해요', '▁', '해요', '▁', '해요', '▁', '▁', '을', '▁사랑', '해요', '▁', '만을', '▁사랑', '만을', '▁사랑', '해요', '▁']\n","사랑해요  사랑해요\n","\n","사랑해요\n","\n","그대\n","\n","만을 사랑해요 그대만을  당신만을\n","\n","사랑해요\n","to_tokens: ['▁', '해요', '▁', '해요', '▁', '해요', '▁', '▁', '을', '▁사랑', '해요', '▁', '만을', '▁사랑', '만을', '▁사랑', '해요', '▁']\n","사랑해요  사랑해요\n","\n","사랑해요\n","\n","그대\n","\n","만을 사랑해요 그대만을  당신만을\n","\n","사랑해요\n","epoch no.0 train no.21010  loss = 0.87267 avg_loss = 1.12699\n","epoch no.0 train no.21010  loss = 0.87267 avg_loss = 1.12699\n","epoch no.0 train no.21020  loss = 0.89956 avg_loss = 1.13003\n","epoch no.0 train no.21020  loss = 0.89956 avg_loss = 1.13003\n","epoch no.0 train no.21030  loss = 0.87899 avg_loss = 1.10827\n","epoch no.0 train no.21030  loss = 0.87899 avg_loss = 1.10827\n","epoch no.0 train no.21040  loss = 1.68852 avg_loss = 1.10524\n","epoch no.0 train no.21040  loss = 1.68852 avg_loss = 1.10524\n","epoch no.0 train no.21050  loss = 1.05399 avg_loss = 1.09999\n","epoch no.0 train no.21050  loss = 1.05399 avg_loss = 1.09999\n","epoch no.0 train no.21060  loss = 1.05904 avg_loss = 1.10793\n","epoch no.0 train no.21060  loss = 1.05904 avg_loss = 1.10793\n","epoch no.0 train no.21070  loss = 0.55831 avg_loss = 1.08876\n","epoch no.0 train no.21070  loss = 0.55831 avg_loss = 1.08876\n","epoch no.0 train no.21080  loss = 1.43870 avg_loss = 1.09315\n","epoch no.0 train no.21080  loss = 1.43870 avg_loss = 1.09315\n","epoch no.0 train no.21090  loss = 0.79943 avg_loss = 1.09924\n","epoch no.0 train no.21090  loss = 0.79943 avg_loss = 1.09924\n","epoch no.0 train no.21100  loss = 1.70877 avg_loss = 1.10983\n","epoch no.0 train no.21100  loss = 1.70877 avg_loss = 1.10983\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bWl5Z0fmbqeU","colab_type":"text"},"source":["# 이제 부터는 관련 연구들"]},{"cell_type":"code","metadata":{"id":"wjlJXSPUaRfd","colab_type":"code","colab":{}},"source":["!pip install kss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ytBhpEJnbO0","colab_type":"code","colab":{}},"source":["text = \"사랑한다 <|endoftext|> [Verse 1: J-Hope] 힙합이란 것은 원래 그런 거지 원래 그런 거지 원래 그런 거지 원래 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐 [Verse 2: Suga] 뭐 어쩌겠어 그딴 게 어<unk> Hangul [Verse 3: RM, Suga] 그래 내가 뭐 틀린 말했어 내가 뭐 거짓말했어 이길 수 있을까 이 기적 아닌 기적을 우리가 만든 걸까 (No) 난 여기 있었고 니가 내게 다가와준 거야 I do believe your galaxy 듣고 싶어 너의 멜로디 너의 은하수의 별들은 너의 하늘을 과연 어떻게 수놓을지 나의 절망 끝에 결국 내가 널 찾았음을 잊지마 넌 절벽 끝에 서 있던 내 마지막 이유야 Live [Pre-Chorus: Jin, RM] 나의 Daydream Daydream Daydream Daydream Daydream Daydream Last Daydream Daydream Daydream Daydream</s>\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V1Iow6H0aRrw","colab_type":"code","colab":{}},"source":["for sent in kss.split_sentences(text):\n","    print(sent)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0vsTa7iHtxlV","colab_type":"code","colab":{}},"source":["s = \"회사 동료 분들과 다녀왔는데 분위기도 좋고 음식도 맛있었어요 다만, 강남 토끼정이 강남 쉑쉑버거 골목길로 쭉 올라가야 하는데 다들 쉑쉑버거의 유혹에 넘어갈 뻔 했답니다 강남역 맛집 토끼정의 외부 모습.\"\n","for sent in kss.split_sentences(s):\n","    print(sent)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwCBhD1bmEJX","colab_type":"code","colab":{}},"source":["!pip install py-hanspell"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sAvL8M8Dpure","colab_type":"code","colab":{}},"source":["test = []\n","for sent in kss.split_sentences(text):\n","    test.append(sent)\n","\n","test = \"\\n\".join(test)\n","\n","print(test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNp-tUu-p3sX","colab_type":"code","colab":{}},"source":["from hanspell import spell_checker\n","result = spell_checker.check(test)\n","result.as_dict()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2nd0AbiqA55","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}